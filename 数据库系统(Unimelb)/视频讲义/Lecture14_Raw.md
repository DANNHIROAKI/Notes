```
Hello, everyone, and welcome back to our 14th lecture on database systems. Today, we'll be concluding our section on query optimization. This will also wrap up the comprehensive module discussing the execution of our queries. I trust that after this, you'll be eager to delve back into the material, as we transition to some lighter topics.
å¤§å®¶å¥½ï¼Œæ¬¢è¿å›åˆ°æ•°æ®åº“ç³»ç»Ÿç¬¬ 14 è®²ã€‚ä»Šå¤©ï¼Œæˆ‘ä»¬å°†ç»“æŸæŸ¥è¯¢ä¼˜åŒ–éƒ¨åˆ†ã€‚è¿™ä¹Ÿå°†ä¸ºæˆ‘ä»¬è®¨è®ºæŸ¥è¯¢æ‰§è¡Œçš„ç»¼åˆæ¨¡å—ç”»ä¸Šå¥å·ã€‚æˆ‘ç›¸ä¿¡ï¼Œåœ¨è¿™ä¹‹åï¼Œä½ ä¼šè¿«ä¸åŠå¾…åœ°é‡æ–°é’»ç ”æ•™æï¼Œå› ä¸ºæˆ‘ä»¬å°†è¿‡æ¸¡åˆ°ä¸€äº›è½»æ¾çš„è¯é¢˜ã€‚
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420152623708.png" alt="image-20240420152623708" style="zoom:50%;" /> 

This is one of several possible architectures; each system has its own slight variations.

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420152702871.png" alt="image-20240420152702871" style="zoom: 50%;" /> 

```
è®©æˆ‘ä»¬å…ˆæ¥ç®€å•å›é¡¾ä¸€ä¸‹ï¼šåœ¨è¿‡å»ä¸¤å‘¨é‡Œï¼Œæˆ‘ä»¬æ²‰æµ¸åœ¨æŸ¥è¯¢å¤„ç†æ¨¡å—ä¸­ã€‚æˆ‘ä»¬æ¢ç´¢äº†æ‰§è¡Œçš„å¤æ‚æ€§ï¼Œæ·±å…¥ç ”ç©¶äº†å„ç§ç®—æ³•ä»¥åŠæ•°æ®åº“å¦‚ä½•æ‰§è¡Œç‰¹å®šæ“ä½œï¼Œå¦‚è¿æ¥å’Œé€‰æ‹©ã€‚ä¸Šæ¬¡ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¼˜åŒ–å™¨å¦‚ä½•ä½¿ç”¨ç¼©å‡å› å­ä¼°ç®—ç»“æœçš„å¤§å°ã€‚ä»Šå¤©ï¼Œæˆ‘ä»¬å°†æœ€ç»ˆæ­å¼€å“ªäº›è®¡åˆ’ä¼šè¢«è€ƒè™‘ä»¥åŠå®ƒä»¬èƒŒåçš„é€‰æ‹©è¿‡ç¨‹ã€‚æœ¬è®²åº§æ˜¯ä¸€ä¸ªé¡¶ç‚¹ï¼Œç»¼åˆäº†æˆ‘ä»¬åœ¨å‰äº”è®²ä¸­è®¨è®ºçš„æ‰€æœ‰å†…å®¹ã€‚å¯ä»¥æŠŠå®ƒçœ‹ä½œæ˜¯ä¸€ä»½ç»¼åˆæŒ‡å—ï¼Œæˆ‘ä»¬å°†æ‰€æœ‰å…³é”®ç‚¹æ•´åˆæˆä¸€ä»½ç»Ÿä¸€çš„æ¦‚è¿°ã€‚
Letâ€™s begin with a brief refresher: over the past two weeks, we've immersed ourselves in the query processing module. We've explored the intricacies of executionâ€”diving into various algorithms and how databases ğŸš—ry out specific operations, like joins and selections. Last time, we examined how the optimizer estimates the size of results using reduction factors. Today, we'll finally uncover which plans are considered and the selection process behind them. This lecture serves as a capstone, synthesizing all the elements we've discussed across the previous five lectures. Think of it as a comprehensive guide, where we consolidate all the key points into a single, cohesive overview.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420152750219.png" alt="image-20240420152750219" style="zoom:50%;" /> 

**1ï¸âƒ£**When enumerating alternative plans, there are two main cases:

1. Single-relation plans
2. Multiple-relation plans (joins)

```
åœ¨è§„åˆ’æŸ¥è¯¢å¤„ç†æ—¶ï¼Œæˆ‘ä»¬é¢å¯¹ä¸¤ç§ä¸»è¦æƒ…å†µã€‚ç¬¬ä¸€ç§æ˜¯å•ä¸€å…³ç³»ï¼ˆå•è¡¨æŸ¥è¯¢ï¼‰æ–¹æ¡ˆï¼Œè¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬ä»…æ¶‰åŠä¸€ä¸ªè¡¨ï¼Œæ²¡æœ‰è¿›è¡Œä»»ä½•è¿æ¥æ“ä½œã€‚å¦ä¸€ç§æƒ…å†µï¼Œä¹Ÿæ˜¯æ›´å¸¸è§çš„æƒ…å†µï¼Œæ¶‰åŠå¤šä¸ªå…³ç³»æˆ–è€…å¤šä¸ªè¡¨çš„è¿æ¥æ“ä½œï¼Œå®é™…ä¸Šï¼Œè¿™ç§æƒ…å†µå æ®äº†å¤§éƒ¨åˆ†æŸ¥è¯¢ã€‚é’ˆå¯¹å•ä¸€å…³ç³»çš„æŸ¥è¯¢æ–¹æ¡ˆï¼Œæˆ‘ä»¬é¦–å…ˆè¦è¯„ä¼°æ¯ä¸€ç§å¯ç”¨çš„æ•°æ®è®¿é—®è·¯å¾„ï¼Œå¹¶é€‰æ‹©æˆæœ¬æœ€ä½çš„ä¸€ç§ã€‚è®°ä½ï¼Œæ— è®ºæ˜¯å¦å­˜åœ¨ç´¢å¼•ï¼Œå †æ‰«æï¼ˆheap scanï¼‰å§‹ç»ˆæ˜¯ä¸€ç§é€‰æ‹©ï¼Œè¿™ç§æ–¹æ³•ä»ç£ç›˜çš„ç¬¬ä¸€é¡µå¼€å§‹ï¼Œç›´è‡³æœ€åä¸€é¡µã€‚æ­¤å¤–ï¼Œæ¯ä¸€ä¸ªç´¢å¼•éƒ½å¯ä»¥æ˜¯å¦ä¸€ç§æ•°æ®è®¿é—®è·¯å¾„ï¼Œç‰¹åˆ«æ˜¯å½“ç´¢å¼•ä¸é€‰æ‹©è°“è¯ç›¸åŒ¹é…æ—¶ï¼Œè¿™å°†æœ‰åŠ©äºå‡å°‘æœç´¢æˆæœ¬ã€‚
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420152750219.png" alt="image-20240420152750219" style="zoom:50%;" /> 

**2ï¸âƒ£**For queries over a **single relation**:

1. Each availableaccess path (file scan / index) is considered, and the one with the **lowest estimated cost** is chosen
   - **Heap scan is always one alternative**
   - **Each index can be another alternative (if matching selection predicates)**
2. Other operations can be performed on top of access paths, but they typically donâ€™t incur additional cost since they are done on the fly(e.g. projections, additional non-matching predicates)

```
å¯¹äºé’ˆå¯¹å•ä¸€å…³ç³»çš„æŸ¥è¯¢ï¼Œæˆ‘ä»¬éœ€è¦è€ƒè™‘æ¯ä¸ªå¯ç”¨çš„è®¿é—®è·¯å¾„ï¼ˆå¦‚æ–‡ä»¶æ‰«ææˆ–ç´¢å¼•ï¼‰ï¼Œå¹¶é€‰æ‹©ä¼°ç®—æˆæœ¬æœ€ä½çš„è·¯å¾„ã€‚è¿™é‡Œéœ€è¦è®°ä½ï¼Œå †æ‰«ææ€»æ˜¯ä¸€ä¸ªå¤‡é€‰æ–¹æ¡ˆï¼Œè€Œæ¯ä¸€ä¸ªç´¢å¼•éƒ½å¯èƒ½æ˜¯å¦ä¸€ä¸ªé€‰æ‹©ï¼Œå°¤å…¶æ˜¯å½“ç´¢å¼•ä¸é€‰æ‹©æ€§è°“è¯ç›¸åŒ¹é…æ—¶ã€‚é™¤äº†è®¿é—®è·¯å¾„ä¹‹å¤–ï¼Œè¿˜å¯ä»¥æ‰§è¡Œå…¶ä»–æ“ä½œï¼Œå¦‚æŠ•å½±æˆ–é¢å¤–çš„ä¸åŒ¹é…è°“è¯ï¼Œä½†è¿™äº›é€šå¸¸ä¸ä¼šå¢åŠ é¢å¤–æˆæœ¬ï¼Œå› ä¸ºå®ƒä»¬æ˜¯å³æ—¶å®Œæˆçš„ã€‚è¿™é‡Œçš„å…³é”®æ˜¯äº†è§£ä¸åŒçš„ç´¢å¼•å’Œè°“è¯ï¼Œä»¥åŠå¦‚ä½•åŒºåˆ†å“ªäº›è°“è¯æ˜¯åŒ¹é…çš„ï¼Œå“ªäº›è°“è¯å°†åœ¨åç»­è¿‡ç¨‹ä¸­å³æ—¶æ£€æŸ¥ã€‚è¿™ä¸ªåŒºåˆ†æ˜¯å­¦ç”Ÿä»¬å¸¸å¸¸æ„Ÿåˆ°å›°éš¾çš„ä¸€ä¸ªæ–¹é¢ã€‚
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420152750219.png" alt="image-20240420152750219" style="zoom:50%;" /> 

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420222557144.png" alt="image-20240420222557144" style="zoom:50%;" /> 

```
è®©æˆ‘ä»¬æ¥å®Œå–„ä¸€ä¸‹è¿™ä¸ªä¾‹å­ï¼Œä½¿å…¶æ›´åŠ æ¸…æ™°ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ª SQL æŸ¥è¯¢ï¼š SELECT * FROM Table A WHERE A=5 AND B>6 AND C<8 AND D=10ã€‚åœ¨è¿™ä¸ªæŸ¥è¯¢ä¸­ï¼Œæˆ‘ä»¬æœ‰å››ä¸ªæ¡ä»¶ï¼ˆè°“è¯ï¼‰æ¥è¿‡æ»¤æ•°æ®ã€‚æ¯ä¸ªæ¡ä»¶ï¼Œå¦‚ A=5ã€B>6ã€C<8 å’Œ D=10ï¼Œéƒ½å¯èƒ½æœ‰è‡ªå·±çš„ç¼©å‡å› å­ï¼Œè¡¨ç¤ºæ¯ä¸ªæ¡ä»¶å¯¹æ•°æ®é›†çš„ç¼©å‡ç¨‹åº¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦è€ƒè™‘è¿™å››ä¸ªç¼©å‡å› å­ï¼Œä»¥äº†è§£æŸ¥è¯¢å¯¹æ£€ç´¢æ•°æ®å¤§å°çš„æ€»ä½“å½±å“ã€‚
Let's refine that example to make it clearer. Imagine we have a SQL query: SELECT * FROM Table A WHERE A=5 AND B>6 AND C<8 AND D=10. In this query, we have four conditions (predicates) that filter the data. Each condition, such as A=5, B>6, C<8, and D=10, might have its own reduction factor, which indicates how much each condition reduces the data set. Thus, we need to consider these four reduction factors to understand the overall impact of the query on the size of the data retrieved.
```


$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420152750219.png" alt="image-20240420152750219" style="zoom:50%;" /> 

 <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420222844906.png" alt="image-20240420222844906" style="zoom:50%;" /> 

```
ä¸ºç®€å•èµ·è§ï¼Œè®©æˆ‘ä»¬è€ƒè™‘ä½¿ç”¨ä¸¤ä¸ªç´¢å¼•è®¡ç®—æ›¿ä»£å“æˆæœ¬çš„è¿‡ç¨‹ã€‚å‡è®¾æˆ‘ä»¬åœ¨ A å’Œ B åˆ—ä¸Šæœ‰ä¸€ä¸ªç´¢å¼•ï¼Œåœ¨ Aã€C å’Œ D åˆ—ä¸Šæœ‰å¦ä¸€ä¸ªç´¢å¼•ã€‚
Let's consider the process of costing alternatives with two indices for simplicity. Assume we have one index on columns A and B, and another on columns A, C, and D. For the sake of this example, let's also assume that both indices are clustered.
```


$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420152750219.png" alt="image-20240420152750219" style="zoom:50%;" /> 

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420222844906.png" alt="image-20240420222844906" style="zoom:50%;" /> 

```
æˆ‘ä»¬è¦è®¨è®ºçš„ç¬¬ä¸€ç§æˆæœ¬è®¡ç®—ç­–ç•¥æ˜¯å †æ‰«æã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨ä½¿ç”¨å †æ‰«ææ—¶ï¼Œæˆ‘ä»¬å¯èƒ½ä½¿ç”¨çš„è°“è¯å¹¶ä¸é‡è¦ã€‚è¿™æ˜¯å› ä¸ºå †æ‰«æä¸­çš„æ•°æ®æ˜¯æœªåˆ†ç±»çš„ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦æ‰«ææ‰€æœ‰æ•°æ®ï¼Œè€Œä¸è€ƒè™‘ä»»ä½•ç‰¹å®šæ¡ä»¶ã€‚
Now, focus on a single table, which we'll refer to as relation A. The first costing strategy we will discuss is a heap scan. The cost associated with a heap scan is essentially the number of pages in relation A. It's important to note that when employing a heap scan, the predicates we might have are irrelevant. This is because the data in a heap scan is unsorted, and therefore, we need to scan through all the data regardless of any specific conditions.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$



<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420152750219.png" alt="image-20240420152750219" style="zoom:50%;" /> 

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420223052665.png" alt="image-20240420223052665" style="zoom:50%;" /> 

```
ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ¥è®¡ç®—ä¸¤ä¸ªç´¢å¼•çš„ç›¸å…³æˆæœ¬ï¼Œæˆ‘ä»¬å°†å…¶åˆ†åˆ«ç§°ä¸ºç´¢å¼• I1 å’Œç´¢å¼• I2ã€‚ç´¢å¼• I1 æ˜¯ä¸€ä¸ªèšç±»ç´¢å¼•ï¼Œä»ç´¢å¼• I1 å¼€å§‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨èšç±»ç´¢å¼•çš„ç‰¹å®šå…¬å¼ã€‚è¯¥å…¬å¼æ˜¯ç´¢å¼•ä¸­çš„é¡µæ•°åŠ ä¸Šå…³ç³»ä¸­çš„é¡µæ•°ä¹‹å’Œï¼Œç„¶åå°†è¯¥å’Œä¹˜ä»¥é€‚ç”¨çš„ç¼©å‡å› å­çš„ä¹˜ç§¯ã€‚è®©æˆ‘ä»¬æ¥ç¡®å®šé€‚ç”¨çš„ç¼©å‡å› å­ã€‚å¯¹äºæ¶µç›– A åˆ—å’Œ B åˆ—çš„ç´¢å¼• I1ï¼Œè¯¥ç´¢å¼•æœ‰åŠ©äºé«˜æ•ˆæŸ¥è¯¢ A ç­‰äº 5ã€B ç­‰äº 6 çš„æ¡ä»¶ã€‚ä¸ C å’Œ D ç›¸å…³çš„è°“è¯ä¸è¯¥ç´¢å¼•æ‰€è¦†ç›–çš„åˆ—ä¸åŒ¹é…ï¼Œè¿™æ„å‘³ç€å®ƒä»¬ä¸ä¼šå½±å“ I1 çš„æˆæœ¬è®¡ç®—ã€‚
Now, let's calculate the costs associated with two indices, which we'll refer to as index I1 and index I2. Starting with index I1, which is a clustered index, we'll use the specific formula for clustered indices. This formula is the sum of the number of pages in the index plus the number of pages in the relation, and then we multiply this sum by the product of the applicable reduction factors. Let's identify which reduction factors apply. For index I1, which covers columns A and B, this index aids in efficiently querying for conditions where A equals five and B equals six. Consequently, the reduction factor for A significantly impacts the cost calculation, and this is then multiplied by the reduction factor for B. The predicates related to C and D do not match the columns covered by this index, meaning they do not influence the cost calculation for I1.
```


$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420152750219.png" alt="image-20240420152750219" style="zoom:50%;" /> 

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420223231656.png" alt="image-20240420223231656" style="zoom: 50%;" /> 

```
ç°åœ¨æˆ‘ä»¬æ¥è€ƒè™‘ç¬¬äºŒä¸ªç´¢å¼•çš„æˆæœ¬è®¡ç®—ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºç´¢å¼• I2ã€‚ä½¿ç”¨çš„å…¬å¼ä¸ I1 ç›¸åŒï¼šç´¢å¼• I2 ä¸­çš„é¡µæ•°åŠ ä¸Šç›¸å…³å…³ç³»ä¸­çš„é¡µæ•°ï¼Œå†ä¹˜ä»¥é€‚ç”¨çš„ç¼©å‡ç³»æ•°çš„ä¹˜ç§¯ã€‚å¯¹äºç´¢å¼• I2ï¼ˆç´¢å¼•åˆ— Aã€C å’Œ Dï¼‰ï¼Œé€‚ç”¨çš„ç¼©å‡å› å­æ˜¯ Aã€C å’Œ D çš„ç¼©å‡å› å­ã€‚è¿™äº›å› å­è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒä»¬å†³å®šäº†ç´¢å¼•æ ¹æ®æŒ‡å®šæ¡ä»¶ç¼©å°ç»“æœèŒƒå›´çš„æ•ˆæœã€‚å› æ­¤ï¼Œè¿™äº›å› ç´ ä¼šå¢åŠ ä½¿ç”¨æŒ‡æ•° I2 çš„æˆæœ¬ã€‚å¦ä¸€æ–¹é¢ï¼Œè¯¥ç´¢å¼•ä¸åŒ…æ‹¬åˆ— B ä¸Šçš„ä»»ä½•æ¡ä»¶ã€‚å› æ­¤ï¼ŒB ä¸Šçš„æ¡ä»¶å°†åœ¨æŸ¥è¯¢æ‰§è¡Œè¿‡ç¨‹ä¸­ "å³æ—¶ "è¯„ä¼°ï¼Œä¸ä¼šå½±å“ä½¿ç”¨ç´¢å¼• I2 çš„åˆå§‹æˆæœ¬ä¼°ç®—ã€‚
Let's now consider the cost calculation for the second index, which we'll call index I2. The formula used is identical to that for I1: the number of pages in index I2 plus the number of pages in the associated relation, all multiplied by the product of the applicable reduction factors. For index I2, which indexes columns A, C, and D, the applicable reduction factors are those for A, C, and D. These factors are crucial as they determine how effectively the index narrows down the results based on the specified conditions. Therefore, these are the factors that will contribute to the cost of using index I2. On the other hand, any condition on column B is not covered by this index. Consequently, the condition on B will be evaluated 'on the fly' during query execution and does not affect the initial cost estimation for using index I2.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$
<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420153112133.png" alt="image-20240420153112133" style="zoom:50%;" /> 

**1ï¸âƒ£**Sequential (heap) scan of data file:

- **Cost = NPages(R)**

**é¡ºåºï¼ˆå †ï¼‰æ‰«ææ•°æ®æ–‡ä»¶**ï¼š
   é¡ºåºæ‰«ææˆ–å…¨è¡¨æ‰«ææ˜¯ä¸€ç§ç®€å•ä½†é€šå¸¸æˆæœ¬è¾ƒé«˜çš„æ•°æ®æ£€ç´¢æ–¹æ³•ï¼Œæ•´ä¸ªè¡¨ä¼šä¸€é¡µé¡µåœ°è¢«è¯»å–ã€‚é¡ºåºæ‰«æçš„æˆæœ¬ç­‰äºå…³ç³» $R$ çš„é¡µæ•°ï¼Œè¡¨ç¤ºä¸º $ \text{Cost} = \text{NPages}(R) $ã€‚å½“æ²¡æœ‰åˆé€‚çš„ç´¢å¼•å¯ç”¨äºæŸ¥è¯¢ä¼˜åŒ–æ—¶ï¼Œä¼šä½¿ç”¨æ­¤æ–¹æ³•ã€‚

 A sequential or full table scan is a straightforward yet often costly data retrieval method where the entire table is read disk-page by disk-page. The cost of a sequential scan is equal to the number of pages of the relation $R$, denoted as $\text{Cost} = \text{NPages}(R) $. This method is used when no suitable indexes are available for query optimization.
$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$
<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420153112133.png" alt="image-20240420153112133" style="zoom:50%;" /> 

**2ï¸âƒ£**Index selection over a primary key (just a single tuple):

- **Cost(B+Tree)=Height(I)+1**, Height is the index height
- **Cost(HashIndex)= ProbeCost(I)+1**, ProbeCost(I)~1.2

**åŸºäºä¸»é”®çš„ç´¢å¼•é€‰æ‹©**ï¼š
   åŸºäºä¸»é”®æŸ¥è¯¢æ—¶ï¼Œæˆ‘ä»¬éœ€è¦è€ƒè™‘ä¸¤ç§ç±»å‹çš„ç´¢å¼•ï¼šB+æ ‘å’Œå“ˆå¸Œç´¢å¼•ã€‚é€šè¿‡ B+æ ‘ç´¢å¼•è®¿é—®è®°å½•çš„æˆæœ¬ç”±ç´¢å¼•çš„é«˜åº¦å†³å®šï¼Œè¡¨ç¤ºä¸º $ \text{Cost(B+Tree)} = \text{Height}(I) + 1 $ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¯¹äºå“ˆå¸Œç´¢å¼•ï¼Œæˆæœ¬æ¶‰åŠå¤§çº¦ä¸º1.2çš„æ¢æµ‹æˆæœ¬ï¼Œè¿™æ˜¯å› ä¸ºä¸€ä¸ªæ¡¶å†…å¯èƒ½æœ‰å¤šä¸ªé¡µé¢ï¼Œä»è€Œå¯¼è‡´ $ \text{Cost(HashIndex)} = \text{ProbeCost}(I) + 1 $ã€‚

When querying based on a primary key, we have two types of indexes to consider: B+Tree and HashIndex. The cost of accessing a record via a B+Tree index is determined by the height of the index, denoted as $\text{Cost(B+Tree)} = \text{Height}(I) + 1 $. In contrast, for a HashIndex, the cost involves a probe cost of approximately 1.2 due to potentially multiple pages within a bucket, leading to $\text{Cost(HashIndex)} = \text{ProbeCost}(I) + 1 $.
$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$
<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420153112133.png" alt="image-20240420153112133" style="zoom:50%;" /> 

**3ï¸âƒ£**Clustered index matching one or more predicates:

- **Cost(B+Tree)=(NPages(I) + ==NPages(R)==)***$\prod\limits_{i=1 . . n} R F_i$
- **Cost(HashIndex)= NPages(R)***$\prod\limits_{i=1 . . n} R F_i * 2.2$â€‹

**èšé›†ç´¢å¼•åŒ¹é…ä¸€ä¸ªæˆ–å¤šä¸ªè°“è¯**ï¼š
   å¯¹äºæ•°æ®ç‰©ç†å­˜å‚¨é¡ºåºä¸ç´¢å¼•é¡ºåºä¸€è‡´çš„èšé›†ç´¢å¼•ï¼Œæˆæœ¬è®¡ç®—æ¶‰åŠç´¢å¼•é¡µé¢æ•°å’ŒåŒ¹é…è°“è¯çš„æ•°æ®é¡µé¢æ•°ã€‚è¿™è¡¨ç¤ºä¸º $ \text{Cost(B+Tree)} = (\text{NPages}(I) + \text{NPages}(R)) \times \prod_{i=1}^{n} RF_i $ï¼Œå…¶ä¸­ $ RF_i $ æ˜¯åŸºäºè°“è¯çš„é™ä½å› å­ã€‚åœ¨ç±»ä¼¼æƒ…å†µä¸‹ï¼Œå“ˆå¸Œç´¢å¼•çš„æˆæœ¬ç•¥æœ‰å¢åŠ ï¼Œä»¥è€ƒè™‘é¢å¤–çš„è¯»å–æ“ä½œï¼Œå¯¼è‡´ $ \text{Cost(HashIndex)} = \text{NPages}(R) \times \prod_{i=1}^{n} RF_i \times 2.2 $ã€‚

For a clustered index, where the data is physically stored in the order of the index, the cost calculation involves both the number of index pages and the number of data pages that match the predicate. This is expressed as $\text{Cost(B+Tree)} = (\text{NPages}(I) + \text{NPages}(R)) \times \prod_{i=1}^{n} RF_i $, where $RF_i $ are the reduction factors based on the predicates. The cost for a HashIndex in a similar scenario increases slightly to account for additional reads, leading to $\text{Cost(HashIndex)} = \text{NPages}(R) \times \prod_{i=1}^{n} RF_i \times 2.2 $.
$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$
<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420153112133.png" alt="image-20240420153112133" style="zoom:50%;" /> 

**4ï¸âƒ£**Non-clustered index matching one or more predicates:

- **Cost(B+Tree)=(NPages(I) + ==NPages(R)==)***$\prod\limits_{i=1 . . n} R F_i$
- **Cost(HashIndex)= NTuples(R)***$\prod\limits_{i=1 . . n} R F_i * 2.2$â€‹

**éèšé›†ç´¢å¼•åŒ¹é…ä¸€ä¸ªæˆ–å¤šä¸ªè°“è¯**ï¼š
   å¯¹äºéèšé›†ç´¢å¼•ï¼Œæ•°æ®å¹¶éæŒ‰ç´¢å¼•é¡ºåºé¡ºåºå­˜å‚¨ï¼Œå› æ­¤åœ¨æ£€ç´¢åŒ¹é…è°“è¯çš„æ•°æ®æ—¶ä¼šäº§ç”Ÿé¢å¤–æˆæœ¬ã€‚B+æ ‘çš„å…¬å¼ä¸èšé›†ç´¢å¼•ç›¸ä¼¼ï¼Œä½†æ¶‰åŠæ•´ä¸ªå…³ç³»çš„æ•°æ®æ£€ç´¢ï¼š$ \text{Cost(B+Tree)} = (\text{NPages}(I) + \text{NPages}(R)) \times \prod_{i=1}^{n} RF_i $ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå“ˆå¸Œç´¢å¼•çš„æˆæœ¬æ˜¯åŸºäºå…ƒç»„æ•°é‡è€Œéé¡µé¢è®¡ç®—çš„ï¼Œå› æ­¤ä¸º $ \text{Cost(HashIndex)} = \text{NTuples}(R) \times \prod_{i=1}^{n} RF_i \times 2.2 $ã€‚

 With non-clustered indexes, data isn't stored sequentially as per the index, so additional costs are incurred when retrieving data matching the predicates. The formula for B+Tree remains similar to that of a clustered index but involves the entire relation for data retrieval: $\text{Cost(B+Tree)} = (\text{NPages}(I) + \text{NPages}(R)) \times \prod_{i=1}^{n} RF_i $. In contrast, the cost for a HashIndex is calculated based on the number of tuples rather than pages, hence $\text{Cost(HashIndex)} = \text{NTuples}(R) \times \prod_{i=1}^{n} RF_i \times 2.2 $.
$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420155536745.png" alt="image-20240420155536745" style="zoom:50%;" /> 

**0ï¸âƒ£**Letâ€™s say that Sailors(S) has 500 pages, 40000 tuples, NKeys(rating) = 10

```sql
SELECT S.sidFROM Sailors S WHERE S.rating=8
```

$\text{Result size = (1/NKeys(rating)) * NTuples(S) = (1/10)*40000 =4000 tuples}$

**1ï¸âƒ£**If we have **I(rating)**, NPages(I) = 50:

1. Clustered index:

   $\text{Cost = (1/NKeys(rating))*(NPages(I)+NPages(S))=(1/10)*(50+500) = 55 I/O}$

2. Unclustered index:

   $\text{Cost = (1/NKeys(rating))*(NPages(I)+NTuples(S))=(1/10)*(50+40000) = 4005 I/O}$

```
åœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬è¦æ£€æŸ¥çš„æ˜¯ä» "æ°´æ‰‹ "è¡¨ä¸­é€‰å–çš„å†…å®¹ï¼Œå…¶ä¸­ "ç­‰çº§ "ç­‰äº 8ã€‚è¯¥è¡¨ç”± 500 é¡µä¸­çš„ 40,000 è¡Œç»„æˆï¼Œå…¶ä¸­ "ç­‰çº§ "æœ‰ 10 ä¸ªä¸åŒçš„å€¼ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ ¹æ®è¿™äº›å€¼è®¡ç®—é¢„æœŸç»“æœå¤§å°ï¼Œä¼°è®¡çº¦ä¸º 4000 è¡Œã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬è€ƒè™‘åœ¨è¯¥è¡¨ä¸­å»ºç«‹ä¸¤ä¸ªå‡è®¾ç´¢å¼•ï¼šä¸€ä¸ªæ˜¯ "rating "ç´¢å¼•ï¼Œå¦ä¸€ä¸ªæ˜¯ "sid "ç´¢å¼•ã€‚åœ¨è®¨è®ºä¸­ï¼Œæˆ‘ä»¬å‡å®šä¸æ¸…æ¥šè¿™äº›ç´¢å¼•æ˜¯èšç±»çš„è¿˜æ˜¯éèšç±»çš„ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œåœ¨å®é™…æ“ä½œä¸­ï¼Œä½ ä¼šçŸ¥é“æ¯ä¸ªç´¢å¼•çš„ç±»å‹ï¼Œå¹¶ç›¸åº”åœ°ä½¿ç”¨é€‚å½“çš„è®¡ç®—æ–¹æ³•ã€‚å¦‚æœ "è¯„çº§ "ç´¢å¼•æ˜¯èšç°‡ç´¢å¼•ï¼Œå¹¶ä¸”è·¨è¶Š 50 ä¸ªé¡µé¢ï¼Œé‚£ä¹ˆè®¡ç®—å¾—å‡ºçš„ I/O æ“ä½œæ•°å°†å¤§å¤§å°‘äºéèšç°‡ç´¢å¼•ï¼Œåè€…ä¼šå› ä¸ºå¿…é¡»è®¿é—®æ›´å¤šæ•°æ®é¡µé¢è€Œå¯¼è‡´ I/O æ•°å¤§å¤§å¢åŠ ã€‚äº†è§£è¿™äº›åŒºåˆ«è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒä»¬ä¼šæå¤§åœ°å½±å“æ•°æ®åº“æ“ä½œçš„æ€§èƒ½ã€‚
In this example, we're examining a selection from the 'sailors' table where 'rating' equals 8. The table consists of 40,000 rows across 500 pages, with 'rating' having 10 distinct values. Initially, we calculate the expected result size based on these values, estimating around 4,000 rows.Next, let's consider two hypothetical indexes on this table: one on 'rating' and another on 'sid'. For this discussion, we'll assume that it's unclear whether these indexes are clustered or unclustered. Typically, in practice, you would know the type of each index and use the appropriate calculation method accordingly. However, for illustrative purposes, I'll demonstrate both calculations.If the 'rating' index is clustered and spans 50 pages, the calculation would yield a significantly reduced number of I/O operations compared to an unclustered index, which would result in a much higher I/O count due to the necessity of accessing more data pages. Understanding these distinctions is crucial as they greatly influence the performance of database operations.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420155536745.png" alt="image-20240420155536745" style="zoom:50%;" /> 

**2ï¸âƒ£**If we have an $\text{I(sid), NPages(I)= 50}$â€‹

1. Cost = ?, Result size = ?
2. Would have to retrieve all tuples/pages. With a **clustered** index, the **cost is 50+500**, with **unclustered** index, **50+40000**

**3ï¸âƒ£**Doing a file scan:

- $\text{Cost = NPages(S) = 500}$â€‹

```
è®©æˆ‘ä»¬è€ƒè™‘è¿™æ ·ä¸€ä¸ªåœºæ™¯ï¼šæˆ‘ä»¬æœ‰ä¸€ä¸ªæ¶µç›– 50 é¡µçš„â€œæ°´æ‰‹ IDâ€ç´¢å¼•ã€‚ æˆ‘ä»¬å¯èƒ½ä¼šé—®çš„ç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯å¦‚ä½•è®¡ç®—ä½¿ç”¨è¯¥æŒ‡æ•°çš„æˆæœ¬ã€‚ å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨è¿™ç§ç‰¹å®šæƒ…å†µä¸‹ï¼Œæ²¡æœ‰é™ä½å› å­æ¥é™ä½è®¿é—®ç´¢å¼•çš„æˆæœ¬ã€‚ è¿™æ„å‘³ç€ç¼©å‡å› å­å®é™…ä¸Šæ˜¯1ï¼Œè¡¨æ˜ä½¿ç”¨è¯¥ç´¢å¼•ä¸ä¼šå‡å°‘æ“ä½œçš„æˆæœ¬ã€‚å¦‚æœè¿™æ˜¯ä¸€ä¸ªèšé›†ç´¢å¼•ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€å®šçš„å…¬å¼è®¡ç®—æˆæœ¬ã€‚ ç„¶è€Œï¼Œå³ä½¿ç´¢å¼•æ˜¯èšé›†çš„ï¼Œä¸å…¶ä»–é€‰é¡¹ç›¸æ¯”ï¼Œæˆæœ¬ä¹Ÿå¯èƒ½ç›¸å¯¹è¾ƒé«˜ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰ä»æ•°æ®è®¿é—®çš„ä»»ä½•å‡å°‘ä¸­å—ç›Šã€‚å°½ç®¡æˆæœ¬å¾ˆé«˜ï¼Œä½†æ­¤ç±»ç´¢å¼•åœ¨ç°å®åœºæ™¯ä¸­ä»ç„¶æœ‰ç”¨ã€‚ ä¾‹å¦‚ï¼Œå¦‚æœè¿™æ˜¯ä¸€ä¸ªB+æ ‘ç´¢å¼•ï¼Œæˆ‘ä»¬åªéœ€éå†å¶å­èŠ‚ç‚¹ï¼Œå°±å¯ä»¥æŒ‰æ’åºé¡ºåºæ£€ç´¢æ•°æ®ã€‚ å³ä½¿æ²¡æœ‰è°“è¯æ¥è¿‡æ»¤è®°å½•ï¼Œè¿™ä¹Ÿæ˜¯æœ‰åˆ©çš„ã€‚ æœ‰æ—¶ï¼Œæ•°æ®åº“å¯èƒ½ä¼šå‡ºäºæ­¤åŸå› é€‰æ‹©ä½¿ç”¨æ­¤ç´¢å¼•ï¼Œå³ä½¿å®ƒä¸æ˜¯å‡å°‘ I/O æ“ä½œçš„æœ€å…·æˆæœ¬æ•ˆç›Šçš„é€‰é¡¹ã€‚ æœ€åï¼Œä¸è¦å¿½è§†å †æ‰«æé€‰é¡¹ï¼Œè¿™ä¸€ç‚¹å¾ˆé‡è¦ã€‚ å­¦ç”Ÿç»å¸¸å¿˜è®°è¿™æ˜¯ä¸€ç§å¯è¡Œçš„é€‰æ‹©ï¼Œä½†å®ƒå§‹ç»ˆå­˜åœ¨ï¼Œæœ‰æ—¶ç”šè‡³æ˜¯æœ€ä¾¿å®œçš„é€‰æ‹©ã€‚ æœ€ç»ˆï¼Œæ•°æ®åº“ä¼˜åŒ–å™¨å°†åœ¨æ‰§è¡ŒæŸ¥è¯¢çš„å¯ç”¨ç­–ç•¥ä¸­é€‰æ‹©æœ€å…·æˆæœ¬æ•ˆç›Šçš„æ–¹æ³•ã€‚ æ­¤ç¤ºä¾‹è¯´æ˜å•å…³ç³»æŸ¥è¯¢è®¡åˆ’ä¸­çš„å†³ç­–å¦‚ä½•æ ¹æ®æ“ä½œçš„ç‰¹å®šè¦æ±‚å’Œçº¦æŸè€Œå˜åŒ–ã€‚
Let's consider a scenario where we have an index on 'sailor ID' that covers 50 pages. The first question we might ask is how to calculate the cost of using this index. It's important to note that in this specific case, there are no reduction factors to lower the cost of accessing the index. This means the reduction factor is effectively one, indicating that using this index will not reduce the cost of the operation.If this were a clustered index, we could calculate the cost using a certain formula. However, even if the index is clustered, the cost is likely to be relatively high compared to other options because we are not benefiting from any reduction in data access.Despite the high cost, such indices can still be useful in real-life scenarios. For example, if this is a B+ tree index and we simply traverse through the leaf nodes, we can retrieve data in a sorted order. This can be advantageous even if there are no predicates to filter the records. Sometimes, databases might choose to use this index for that reason, even if it's not the most cost-effective option for reducing I/O operations. Finally, it's essential not to overlook the option of a heap scan. This is often forgotten by students as a viable alternative, but it's always present and sometimes it turns out to be the cheapest option. Ultimately, the database optimizer will select the most cost-effective approach among the available strategies for executing a query. This example illustrates how decision-making in single-relation query plans can vary depending on the specific requirements and constraints of the operation.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420160803961.png" alt="image-20240420160803961" style="zoom:50%;" /> 

**1ï¸âƒ£**Steps:

1. Select orderof relations
   - E.g. SxRxB, or SxBxRor RxSxBâ€¦
   - maximum possible orderings = N!
2. For each join, select **join algorithm**
   - E.g. Hash join, Sort-Merge Joinâ€¦
3. For each input relation, select access method
   - Heap Scan, or various index alternatives

```
åœ¨å¤„ç†å¤šç›¸å…³è®¡åˆ’ï¼Œå°¤å…¶æ˜¯æ¶‰åŠè¿æ¥çš„è®¡åˆ’æ—¶ï¼Œè¿‡ç¨‹ä¼šå˜å¾—å¤æ‚å¾—å¤šã€‚ä»¥ä¸‹æ˜¯ä¼˜åŒ–å™¨å¤„ç†è¿™äº›æŸ¥è¯¢çš„ä¸‰ä¸ªæ­¥éª¤ï¼š

é¦–å…ˆï¼Œä¼˜åŒ–å™¨ç¡®å®šå…³ç³»çš„é¡ºåºã€‚å®ƒä¼šç³»ç»Ÿåœ°æšä¸¾æ‰€æœ‰å¯èƒ½çš„å…³ç³»åºåˆ—ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬è€ƒè™‘æ¶‰åŠè¡¨ Srã€B å’Œ R çš„ä¸‰è¡¨è¿æ¥ï¼Œä¼˜åŒ–å™¨ä¼šæ¢ç´¢è¿™äº›è¡¨çš„æ¯ä¸€ç§æ’åˆ—ï¼Œå¦‚ Sr-B-Rã€S-B-Rã€R-S-B ç­‰ã€‚è¯·è®°ä½ï¼Œå¯èƒ½çš„æ’åºæ€»æ•°æ˜¯ä»¥è¡¨çš„æ•°é‡ä¸ºåŸºç¡€çš„é˜¶ä¹˜æ€§è´¨--ä¸è¿‡è®°ä½è¿™ä¸ªæ•°å­—å¹¶ä¸æ˜¯å¿…è¦çš„ï¼›å®ƒåªæ˜¯ä¸ºäº†æä¾›ä¸€ä¸ªè§„æ¨¡æ¦‚å¿µã€‚

æ¥ä¸‹æ¥ï¼Œå¯¹äºç¡®å®šçš„æ¯ä¸ªæœ‰åºåºåˆ—ï¼Œä¼˜åŒ–å™¨å¿…é¡»ä¸ºæ¯ä¸ªè¿æ¥æ“ä½œé€‰æ‹©åˆé€‚çš„è¿æ¥å®ç°ã€‚é€‰é¡¹å¯èƒ½åŒ…æ‹¬æ•£åˆ—è¿æ¥ã€æ’åºåˆå¹¶è¿æ¥æˆ–åµŒå¥—å¾ªç¯è¿æ¥ç­‰ã€‚

æœ€åï¼Œä¼˜åŒ–å™¨ä¼šå†³å®šè®¡åˆ’åº•éƒ¨æ¯ä¸ªè¾“å…¥å…³ç³»çš„è®¿é—®æ–¹æ³•ã€‚å¯ä¾›é€‰æ‹©çš„æ–¹æ³•å¤šç§å¤šæ ·ï¼Œä»ç®€å•çš„å †æ‰«æåˆ°æ›´å¤æ‚çš„ç´¢å¼•æ‰«æï¼ŒåŒ…æ‹¬èšç±»ç´¢å¼•æˆ–å¸¦æœ‰è¾…åŠ©ç´¢å¼•çš„å †æ‰«æã€‚æ¯ç§æ–¹æ³•å¯¹æ€§èƒ½å’Œæ•ˆç‡éƒ½æœ‰å„è‡ªçš„å½±å“ã€‚

When dealing with multi-relation plans, particularly those involving joins, the process becomes significantly more complex. Here's how the optimizer handles these queries in three steps:

First, the optimizer determines the order of the relations. It systematically enumerates all possible sequences of the relations. For instance, if we consider a three-table join involving tables Sr, B, and R, the optimizer explores every permutation of these tables, such as Sr-B-R, S-B-R, R-S-B, and so forth. Remember, the total number of possible orderings is factorial in nature based on the number of tablesâ€”though it's not essential to memorize this number; it's just to give an idea of the scale.

Next, for each ordered sequence identified, the optimizer must choose the appropriate join implementation for each join operation. The options might include hash joins, sort-merge joins, or nested loops joins, among others.

Finally, the optimizer decides on the access method for each input relation at the bottom of the plan. The choices vary from simple heap scans to more complex indexed scans, including clustered indexes or heap scans with auxiliary indexes. Each method has its own implications for performance and efficiency.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420160803961.png" alt="image-20240420160803961" style="zoom:50%;" /> 

**2ï¸âƒ£**Q: How many plans are there for a query over N relations? Back-of-envelope calculation:

1. With 3 join algorithms, I indexes per relation: 

   $\text { \# plans } \approx[N !]^*\left[3^{(N-1)}\right]{}^*\left[(\mathrm{I}+1)^{\mathrm{N}}\right]$

2. Suppose N = 3, I = 2: 

   $\text { plans } \approx 3 ! * 3^2 * 3^3 = 1458\text{ plans}$

3. This is just for illustration â€“you donâ€™t need to remember this

``` 
åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬æ¥æ¢è®¨æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–å™¨çš„ä»»åŠ¡å¤æ‚æ€§ï¼Œé€šè¿‡æŸ¥çœ‹å®ƒåœ¨å¤„ç†å¤šä¸ªå…³ç³»çš„æŸ¥è¯¢æ—¶ä¼šè€ƒè™‘å¤šå°‘ç§è®¡åˆ’ã€‚è¯·è®°ä½ï¼Œæˆ‘å³å°†åˆ†äº«çš„å…¬å¼å¾ˆå¤æ‚ï¼Œä½†ä½ ä¸éœ€è¦è®°ä½å®ƒã€‚æˆ‘çš„ç›®çš„ä»…ä»…æ˜¯ä¸ºäº†çªå‡ºè¿™ä¸ªæ•°å­—çš„åºå¤§è§„æ¨¡ã€‚åœ¨å¤„ç†Nä¸ªå…³ç³»æ—¶ï¼Œä¼˜åŒ–å™¨é¦–å…ˆè€ƒè™‘è¿™äº›å…³ç³»çš„æ‰€æœ‰å¯èƒ½æ’åºï¼Œè¿™ç­‰äºNçš„é˜¶ä¹˜ï¼ˆN!ï¼‰ï¼Œä»£è¡¨æ‰€æœ‰æ’åˆ—ã€‚æ¥ä¸‹æ¥ï¼Œå¯¹äºNä¸ªè¡¨ï¼Œæœ‰N-1ä¸ªè¿æ¥ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæœ‰ä¸‰ä¸ªè¡¨ï¼Œå°±æœ‰ä¸¤ä¸ªè¿æ¥ã€‚å‡è®¾æœ‰ä¸‰ç§å¯èƒ½çš„è¿æ¥ç®—æ³•â€”â€”å †æ‰«æï¼ˆhash scanï¼‰ã€åµŒå¥—å¾ªç¯ï¼ˆnested loopsï¼‰å’Œæ•£åˆ—è¿æ¥ï¼ˆhash joinï¼‰ï¼Œä¸åŒè¿æ¥è®¡åˆ’çš„æ•°é‡æ˜¯3çš„N-1æ¬¡å¹‚ã€‚æ­¤å¤–ï¼Œå¦‚æœæœ‰Iä¸ªç´¢å¼•ï¼Œä¼˜åŒ–å™¨ä¼šä¸ºæ¯ä¸ªè¡¨è€ƒè™‘I+1ä¸ªå¯èƒ½çš„è®¿é—®è·¯å¾„ï¼ˆé¢å¤–çš„è·¯å¾„è€ƒè™‘äº†å †æ‰«æï¼‰ï¼Œæ€»å…±æ˜¯(I+1)^Nä¸ªå¯èƒ½çš„è®¿é—®è·¯å¾„ã€‚ç»¼åˆèµ·æ¥ï¼Œå³ä½¿åœ¨åªæœ‰ä¸‰ä¸ªè¡¨çš„ç®€å•æƒ…å†µä¸‹ï¼Œä¼˜åŒ–å™¨è¯„ä¼°çš„æ½œåœ¨æ‰§è¡Œè®¡åˆ’æ•°é‡ä¹Ÿæ˜¯æƒŠäººçš„ï¼Œå¤§çº¦æœ‰1458ç§ä¸åŒçš„è®¡åˆ’ã€‚å†æ¬¡å¼ºè°ƒï¼Œè¿™é‡Œçš„é‡ç‚¹ä¸æ˜¯è®°ä½å…¬å¼ï¼Œè€Œæ˜¯ç†è§£æ•°æ®åº“ä¼˜åŒ–å™¨å¸¸è§„å¤„ç†çš„è§„æ¨¡ä¹‹å¤§ã€‚
In this example, let's explore the complexity of the database query optimizer's task by looking at the number of plans it will consider for a query over multiple relations. Remember, the formula I'm about to share is complex, but you don't need to memorize it. My aim is simply to highlight the sheer scale of the number. When dealing with N relations, the optimizer first considers all possible orders of these relations, which amounts to N! (N factorial), representing all permutations. Next, for N tables, there are N-1 joins. For instance, with three tables, there are two joins. Assuming there are three possible join algorithmsâ€”hash scan, nested loops, and hash joinâ€”the number of different join plans is 3^(N-1). Additionally, if there are I indices available, the optimizer considers I+1 possible access paths for each table (the additional path accounts for a heap scan), leading to (I+1)^N possible access paths overall. Putting it all together, even for a simple scenario with just three tables, the number of potential execution plans the optimizer evaluates is staggering, amounting to approximately 1458 different plans. Again, the focus here isn't on memorizing the formula but on understanding the magnitude of what database optimizers handle routinely
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420162505917.png" alt="image-20240420162505917" style="zoom:50%;" /> 

**1ï¸âƒ£**As number of joins increases, number of alternative plans grows rapidly 

â†’==need to restrict search space==

**2ï¸âƒ£**Fundamental decision in System R (first DBMS): **only left-deep join trees** are considered

- Left-deep trees allow us to generate all **fully pipelined** plans

  - Intermediate results are not written to temporary files

    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420224518730.png" alt="image-20240420224518730" style="zoom: 67%;" /> 
    
     

```
æ­£å¦‚æˆ‘ä»¬æ‰€è§‚å¯Ÿåˆ°çš„ï¼Œä¼˜åŒ–å™¨çš„ä»»åŠ¡ç›¸å½“å¤æ‚ï¼Œéšç€å…³ç³»æ•°é‡çš„å¢åŠ ï¼Œå¤‡é€‰æ–¹æ¡ˆçš„æ•°é‡ä¹Ÿå‘ˆæŒ‡æ•°çº§å¢é•¿ã€‚å› æ­¤ï¼Œä¼˜åŒ–å™¨éœ€è¦é™åˆ¶æœç´¢ç©ºé—´ã€‚IBM å¼€å‘çš„ç¬¬ä¸€ä¸ªå…³ç³» DBMS System R åšå‡ºäº†ä¸€ä¸ªå¼€åˆ›æ€§çš„å†³å®šï¼Œå³åªè€ƒè™‘å·¦æ·±è¿æ¥æ ‘ã€‚è¿™ä¸€å†³å®šåæ¥è¢«å•†ä¸šæ•°æ®åº“æ™®éé‡‡ç”¨ï¼Œä¸»è¦æ˜¯å› ä¸ºå®ƒèƒ½æœ‰æ•ˆé™ä½è®¡ç®—æˆæœ¬ã€‚å·¦æ·±è®¡åˆ’å¯ä»¥ç”Ÿæˆå®Œå…¨æµæ°´çº¿å¼çš„è®¡åˆ’ï¼Œåœ¨è¿™ç§è®¡åˆ’ä¸­ä¸å­˜å‚¨ä¸­é—´ç»“æœï¼›ä¸€ä¸ªæ“ä½œçš„è¾“å‡ºç›´æ¥ä½œä¸ºä¸‹ä¸€ä¸ªæ“ä½œçš„è¾“å…¥ã€‚è¿™å°±æ˜¯æµæ°´çº¿æµç¨‹çš„å«ä¹‰--ä¸æ–­å‘ä¸Šæ¸¸æ¨é€æ•°æ®ï¼Œå¦‚å‰é¢çš„ç¤ºä¾‹æ‰€ç¤ºã€‚æŸ¥çœ‹è¿™äº›è®¡åˆ’æ—¶ï¼Œåªæœ‰å·¦ä¾§æ˜¾ç¤ºçš„è®¡åˆ’ç¬¦åˆå·¦æ·±æ ‘çš„æ¡ä»¶ï¼Œå› ä¸ºæ¯ä¸ªæ“ä½œçš„è¾“å‡ºéƒ½æ˜¯åç»­æ“ä½œçš„å·¦è¾“å…¥ã€‚è¿™äº›è®¡åˆ’æ˜¯å®Œå…¨æµæ°´çº¿å¼çš„ï¼›å› æ­¤ï¼Œå½“æˆ‘ä»¬æ‰§è¡Œ "åœ¨ä¸€ä¸ª T ä¸Šè¿æ¥ B "è¿™æ ·çš„è¿æ¥æ“ä½œæ—¶ï¼Œæ•°æ®ä¼šç«‹å³è¢«æ¨é€åˆ°ä¸‹ä¸€ä¸ªè¿æ¥æ“ä½œï¼Œä¾æ­¤ç±»æ¨ï¼Œè€Œä¸ä¼šå­˜å‚¨è¿™äº›æ“ä½œçš„ç»“æœã€‚
As we've observed, the task of the optimizer is quite complex, and as the number of relations increases, the number of alternative plans grows exponentially. Therefore, the optimizer needs to restrict the search space. A seminal decision made by System R, the first relational DBMS developed by IBM, was to consider only left-deep join trees. This decision has since been universally adopted by commercial databases, primarily because it effectively reduces computational costs. Left-deep plans enable the generation of completely pipelined plans where no intermediate results are stored; the output of one operation is directly pushed as the input to the next. This is what a pipelined process entailsâ€”continuously pushing data upstream, as illustrated in a previous example. When reviewing the plans, only the one shown on the left qualifies as a left-deep tree because the output of each operation serves as the left input for the subsequent operation. These plans are fully pipelined; thus, as we execute a join operation like 'join B over one T,' it is immediately pushed further up to the next join, and so on, without ever storing the results of these operations.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

 <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420162505917.png" alt="image-20240420162505917" style="zoom:50%;" />

**1ï¸âƒ£**As number of joins increases, number of alternative plans grows rapidly 

â†’==need to restrict search space==

**2ï¸âƒ£**Fundamental decision in System R (first DBMS): **only left-deep join trees** are considered

- Left-deep trees allow us to generate all **fully pipelined** plans

  - Intermediate results are not written to temporary files

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420162614698.png" alt="image-20240420162614698" style="zoom: 50%;" /> 

```
ä¸ "è¿æ¥ B "è¿™æ ·çš„å·¦æ·±æ ‘ç›¸åï¼Œå›¾ä¸­çš„å…¶ä»–è®¡åˆ’å¹¶ä¸æ˜¯å·¦æ·±è¿æ¥æ ‘ã€‚å³è¾¹çš„è®¡åˆ’è¢«å½’ç±»ä¸ºä¸›ç”Ÿè®¡åˆ’ï¼Œä¸­é—´çš„è®¡åˆ’åˆ™æ˜¯ä¸€ç§æ··åˆè®¡åˆ’ã€‚ä¸ºäº†æœ‰æ•ˆåœ°ç¼©å°æœç´¢ç©ºé—´ï¼Œè¿™ç±»è®¡åˆ’ä»ä¸€å¼€å§‹å°±ä¼šè¢«ä¸¢å¼ƒã€‚
In contrast to the left-deep tree like 'join B,' the other plans depicted are not left-deep join trees. The plan on the right is classified as bushy, and the one in the middle represents a sort of hybrid. To reduce the search space effectively, these types of plans are disğŸš—ded right from the start.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420164003754.png" alt="image-20240420164003754" style="zoom:50%;" /> 

```sql
SELECT S.sname, B.bname, R.day
FROM Sailors S, Reserves R, Boats B
WHERE S.sid = R.sid AND R.bid = B.bid
```

Letâ€™s assume:

**1ï¸âƒ£**Two join algorithms to choose from:

1. Hash-Join
2. NL-Join (page-oriented)

**2ï¸âƒ£**Clustered B+Tree index: $\text { I(R.sid); NPages }(\text{I})=50$

**3ï¸âƒ£**No other indexes

**4ï¸âƒ£**$\text {S: NPages }(S)=500, \text { NTuplesPerPage }(S)=80$

**5ï¸âƒ£**$\text { R: NPages }(R)=1000, \text { NTuplesPerPage }(R)=100$â€‹

**6ï¸âƒ£**$\text {B: NPages }(B)=10$

**7ï¸âƒ£**$100 \mathrm{R} \bowtie \mathrm{S} $â€‹tuples fit on a page

```
ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä¸€èµ·æ¥æšä¸¾è¿™ä¸ªç®€å•ä¾‹å­ä¸­çš„æ‰€æœ‰è®¡åˆ’ï¼Œåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªä¸‰è¡¨è”æ¥ï¼ŒåŸºæœ¬ä¸Šæœ‰ä¸¤ä¸ªè”æ¥è°“è¯ã€‚ä¸ºäº†ç®€å•èµ·è§ï¼Œåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘å‡è®¾åªæœ‰å“ˆå¸Œè¿æ¥å’ŒåµŒå¥—å¾ªç¯è¿æ¥ä¸¤ç§é€‰æ‹©ï¼Œè¿™æ ·å¯ä»¥ç¨å¾®ç¼©å°æœç´¢ç©ºé—´ã€‚ä½†å®é™…ä¸Šï¼Œæ’åºåˆå¹¶è¿æ¥é€šå¸¸ä¹Ÿæ˜¯å¯ç”¨çš„ã€‚æˆ‘è¿˜å‡è®¾åªæœ‰ä¸€ä¸ªå…³äºé¢„ç•™å–å®¶ ID çš„ç´¢å¼•ï¼Œä¸å­˜åœ¨å…¶ä»–ç´¢å¼•ã€‚æ­¤å¤–ï¼Œæˆ‘è¿™é‡Œè¿˜æœ‰å…³äºæˆ‘çš„é¡µé¢çš„ä¿¡æ¯ã€‚
Let's now enumerate all the plans together in this straightforward example, where we have a three-table join with essentially two join predicates. For simplicity, in this example, I'll assume the only alternatives are hash join and nested loops join, just to slightly reduce my search space. Although in reality, sort merge joins are also typically available. I'll also assume there's just one index on a reserved seller ID and no other indices exist. Additionally, I have information about my pages here.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420164536139.png" alt="image-20240420164536139" style="zoom:50%;" /> 

```sql
SELECT S.sname, B.bname, R.day
FROM Sailors S, Reserves R, Boats B
WHERE S.sid = R.sid AND R.bid = B.bid
```

**1ï¸âƒ£**Enumerate **relation orderings**: ==* Prune plans with cross-products immediately!==

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420164613612.png" alt="image-20240420164613612" style="zoom:50%;" /> 

```
è®©æˆ‘ä»¬ä»ç¬¬ä¸€æ­¥å¼€å§‹ï¼šåˆ—ä¸¾å…³ç³»æ’åºã€‚æœ‰ä¸‰ç§å…³ç³»--Sã€R å’Œ Bï¼Œå¯èƒ½çš„æ’åˆ—ç»„åˆæ˜¯ S-R-Bã€S-B-Rã€R-S-Bã€R-B-Sã€B-S-R å’Œ B-R-Sã€‚æœ‰å…­ç§å¯èƒ½çš„æ’åˆ—æ–¹å¼ï¼Œåˆ†åˆ«å¯¹åº”ä¸‰ä¸ªé˜¶ä¹˜æˆ–å…­ä¸ªé˜¶ä¹˜ã€‚å…¶ä¸­ï¼Œä½ ä¼šå‘ç°æœ‰ä¸¤ç§æ–¹æ¡ˆç•¥æœ‰ä¸åŒï¼›ä¾‹å¦‚ï¼Œè¿™é‡Œçš„è¿æ¥ BS å®é™…ä¸Šä»£è¡¨çš„æ˜¯äº¤å‰ç§¯ã€‚ä»æŸ¥è¯¢ä¸­å¯ä»¥çœ‹å‡ºï¼Œå…¶åŸå› åœ¨äº S å’Œ R ä¹‹é—´çš„è¿æ¥æ˜¯è‡ªç„¶è¿æ¥ï¼Œè€Œ R å’Œ B ä¹‹é—´ç¼ºå°‘è¿æ¥è°“è¯æ¥åˆå¹¶å®ƒä»¬ï¼Œå› æ­¤å¿…é¡»è¿›è¡Œäº¤å‰ç§¯ï¼Œè€Œäº¤å‰ç§¯çš„ä»£ä»·éå¸¸æ˜‚è´µã€‚å› æ­¤ï¼Œåœ¨æ•°æ®åº“ç³»ç»Ÿä¸­ï¼Œäº¤å‰ä¹˜ç§¯é€šå¸¸æ˜¯é¿å…çš„ï¼Œè¿™æ ·çš„è®¡åˆ’ä»ä¸€å¼€å§‹å°±ä¼šè¢«å‰ªæ‰ã€‚ä¼˜åŒ–å™¨ç”šè‡³ä¸ä¼šè€ƒè™‘è¿™äº›è®¡åˆ’ï¼Œä»è€Œæœ‰æ•ˆåœ°å°†å®ƒä»¬æ’é™¤åœ¨è€ƒè™‘èŒƒå›´ä¹‹å¤–ã€‚
Let's start with the first step: enumerating relation orderings. With three relationsâ€”S, R, and Bâ€”the possible permutations are S-R-B, S-B-R, R-S-B, R-B-S, B-S-R, and B-R-S. There are six possible orderings, corresponding to three factorial, or six. Among these, you'll notice two plans that differ slightly; for example, a join BS here actually represents a cross product. The reason for this, as you can observe in the query, is that while the join between S and R is a natural join, R and B lack join predicates to merge them, necessitating a cross product, which is, as you might recall, very expensive. Consequently, in database systems, cross products are generally avoided, and such plans will be pruned right from the start. The optimizer won't even consider these plans, effectively eliminating them from consideration.
```


$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420164636947.png" alt="image-20240420164636947" style="zoom:50%;" /> 

```sql
SELECT S.sname, B.bname, R.day
FROM Sailors S, Reserves R, Boats B
WHERE S.sid = R.sid AND R.bid = B.bid
```

**2ï¸âƒ£**Enumerate **join algorithm** choices:

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420164710233.png" alt="image-20240420164710233" style="zoom:50%;" />  + do the same for other plans

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420164726676.png" alt="image-20240420164726676" style="zoom:50%;" /> 

```
æšä¸¾å‡ºè¿™äº›å…³ç³»æ’åºåï¼Œä¸‹ä¸€æ­¥å°±æ˜¯å†³å®šä¸ºæ¯ä¸ªè¿æ¥ä½¿ç”¨å“ªç§è¿æ¥ç®—æ³•ã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘å°†é€‰é¡¹é™åˆ¶ä¸ºåµŒå¥—å¾ªç¯å’Œæ•£åˆ—è¿æ¥ã€‚å¯¹äºä¸€æ£µç‰¹å®šçš„æ ‘ï¼Œå¯èƒ½çš„ç»„åˆåŒ…æ‹¬åµŒå¥—å¾ªç¯ååµŒå¥—å¾ªç¯ã€å“ˆå¸Œè¿æ¥ååµŒå¥—å¾ªç¯ã€åµŒå¥—å¾ªç¯åå“ˆå¸Œè¿æ¥ä»¥åŠå“ˆå¸Œè¿æ¥åå“ˆå¸Œè¿æ¥ã€‚è¿™äº›æ˜¯è¿™æ£µæ ‘çš„é€‰é¡¹ï¼Œå…¶ä»–æ ‘ä¹Ÿéœ€è¦è¯„ä¼°ç±»ä¼¼çš„ç»„åˆã€‚
Once we have enumerated these relation orderings, the next step is to decide which join algorithm to use for each of these joins. In this example, I'm limiting the options to nested loops and hash joins. For one particular tree, the possibilities include combinations like nested loops followed by nested loops, hash join followed by nested loops, nested loop followed by hash join, and hash join followed by hash join. These are the options for this tree, and similar combinations need to be evaluated for all other trees as well.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420164800094.png" alt="image-20240420164800094" style="zoom:50%;" /> 

```sql
SELECT S.sname, B.bname, R.day
FROM Sailors S, Reserves R, Boats B
WHERE S.sid = R.sid AND R.bid = B.bid
```

**3ï¸âƒ£**Enumerate **access method** choices:

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420164828833.png" alt="image-20240420164828833" style="zoom: 50%;" /> + do same for other plans

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420164846871.png" alt="image-20240420164846871" style="zoom:50%;" /> 

```
æœ€åï¼Œåœ¨é€‰æ‹©äº†è¿æ¥ç®—æ³•åï¼Œä¸‹ä¸€æ­¥å°±æ˜¯å†³å®šå¯¹æ¯ä¸ªè¡¨ä½¿ç”¨å“ªç§è®¿é—®è·¯å¾„ã€‚è¿™æ¶‰åŠç¡®å®šæ¯ä¸ªè¡¨çš„è®¿é—®æ–¹å¼ï¼Œæ˜¯å †æ‰«æè¿˜æ˜¯ç´¢å¼•æ‰«æï¼Œä»¥åŠå…¶ä»–æ–¹æ³•ã€‚å› æ­¤ï¼Œå¯èƒ½çš„è®¿é—®è·¯å¾„åŒ…æ‹¬é€šè¿‡å †æ‰«ææˆ–ä½¿ç”¨ R ä¸Šçš„ç´¢å¼•è®¿é—®æ‰€æœ‰è¡¨ï¼Œè€Œå…¶ä»–è¡¨ S å’Œ B åˆ™ç”±äºæ²¡æœ‰å…¶ä»–ç´¢å¼•è€Œç»§ç»­ä½¿ç”¨å †æ‰«æã€‚
Finally, after selecting the join algorithm choices, the next step is to decide which access paths to use for each table. This involves determining how each individual table is accessed, whether by heap scan or index scan, among other methods. In this scenario, my choices are simplified because the only index I have is on relation R. Therefore, the possible access paths include all tables being accessed via heap scan or using the index on R while the other tables, S and B, continue with heap scans due to the absence of other indices.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420165025200.png" alt="image-20240420165025200" style="zoom:50%;" /> 

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420165035271.png" alt="image-20240420165035271" style="zoom:50%;" /> 

**1ï¸âƒ£**Code

```sql
SELECT S.sname, B.bname, R.day
FROM Sailors S, Reserves R, Boats B
WHERE S.sid = R.sid AND R.bid = B.bid
```

$$
\begin{aligned}
& S: \text { NPages }(S)=500, \text { NTuplesPerPage }(S)=80 \\
& R:  \text { NPages }(R)=1000, \text { NTuplesPerPage }(R)=100 \\
& B: \text { NPages }(B)=10
\end{aligned}
$$

$100 R \bowtie S$â€‹â€‹tuples fit on a page, All 3 relations are Heap Scan

```
ä¼˜åŒ–å™¨çš„ç›®æ ‡æ˜¯æšä¸¾æ‰€æœ‰è¿™äº›è®¡åˆ’ï¼Œç„¶åè®¡ç®—æ¯ä¸ªè®¡åˆ’çš„æˆæœ¬ï¼Œå°±åƒæˆ‘ä»¬å°†è¦åšçš„é‚£æ ·ã€‚è®©æˆ‘ä»¬å…³æ³¨å±å¹•ä¸Šæ–¹çš„è®¡åˆ’ï¼Œå®ƒç”±ä¸¤ä¸ªåµŒå¥—å¾ªç¯ç»„æˆï¼Œä½¿ç”¨å †æ‰«æè¿æ¥ã€‚è®¡ç®—æˆæœ¬æ‰€éœ€çš„ä¿¡æ¯æ˜¾ç¤ºåœ¨å±å¹•å³ä¾§ã€‚è¯·è®°ä½ï¼Œæˆæœ¬æ˜¯ä»¥ä»ç£ç›˜è¯»å–çš„é¡µæ•°æ¥è¡¡é‡çš„ã€‚é€šè¿‡æŸ¥çœ‹è®¡åˆ’ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸¤ä¸ªåµŒå¥—å¾ªç¯è¿æ¥ã€‚ä¸ºäº†è®¡ç®—å®ƒä»¬çš„æˆæœ¬ï¼Œæˆ‘ä»¬ä»è®¡åˆ’çš„åº•éƒ¨å¼€å§‹è®¡ç®—--è¿™é‡Œæ˜¯æ‰§è¡Œçš„èµ·ç‚¹ã€‚å¦‚æœä½ è¿˜è®°å¾—æˆ‘å‰é¢æåˆ°çš„ï¼Œåœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œå›¾å…ƒä¼šè¢«æ¨å‘ä¸Šæ¸¸ï¼›è¿™å°±æ˜¯æˆ‘ä»¬å¤„ç†æŸ¥è¯¢çš„æ–¹å¼ã€‚
The optimizer's goal is to enumerate all these plans and then calculate the cost of each one, just as we're about to do. Letâ€™s focus on the plan at the top of the screen, which consists of two nested loops joins using heap scans. The information required for cost calculation is displayed on the right-hand side of the screen. Remember, the cost is measured in terms of the number of pages read from disk. Examining the plan, we see two nested loops joins. To calculate their costs, we start from the bottom of the planâ€”this is where execution begins. If you recall what I mentioned earlier, tuples are pushed upstream during execution; this is how our queries are processed.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420165025200.png" alt="image-20240420165025200" style="zoom:50%;" /> 

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420165035271.png" alt="image-20240420165035271" style="zoom:50%;" /> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420170912351.png" alt="image-20240420170912351" style="zoom:50%;" /> 

**2ï¸âƒ£****Calculating cost:**

1. **SxR**: $\text{Cost (SxR) = 500 + 500*1000 = 500500}$

```
ç¬¬ä¸€æ­¥æ˜¯ä½¿ç”¨é€‚å½“çš„æˆæœ¬å…¬å¼è®¡ç®—åµŒå¥—å¾ªç¯è¿æ¥çš„æˆæœ¬ï¼Œå‡å®šå®ƒæ˜¯é¢å‘é¡µé¢çš„åµŒå¥—å¾ªç¯è¿æ¥ã€‚å¯¹äºè¿™ç§ç±»å‹çš„è¿æ¥ï¼Œæˆæœ¬å…¬å¼æ˜¯å¤–å±‚è¡¨çš„é¡µæ•°åŠ ä¸Šå¤–å±‚è¡¨çš„é¡µæ•°ä¹˜ä»¥å†…å±‚è¡¨çš„é¡µæ•°ã€‚è¿™é‡Œï¼Œè¡¨ S çš„é¡µæ•°ä¸º 500 é¡µï¼Œè¡¨ R çš„é¡µæ•°ä¸º 1000 é¡µï¼Œå› æ­¤ I/O æ“ä½œçš„æˆæœ¬è¶…è¿‡ 500,000 æ¬¡ã€‚è¿™ä¸ªè®¡ç®—ç»“æœæ˜¯ç¬¬ä¸€æ¬¡æ“ä½œçš„æˆæœ¬ã€‚ç°åœ¨çš„æŒ‘æˆ˜æ˜¯ç¡®å®šåç»­åµŒå¥—å¾ªç¯è¿æ¥çš„æˆæœ¬ã€‚ç„¶è€Œï¼Œå¤æ‚æ€§åœ¨äºä¸çŸ¥é“å·¦ä¾§è¾“å…¥--è¯¥è¾“å…¥å®é™…ä¸Šæ˜¯å‰ä¸€ä¸ªæ“ä½œçš„ç»“æœã€‚è¦ç»§ç»­æ“ä½œï¼Œæˆ‘å¿…é¡»è®¡ç®—è¿™æ¬¡æ“ä½œçš„ç»“æœå¤§å°ï¼Œä»¥äº†è§£å°†è¢«æ¨ä¸Šçš„å›¾å…ƒçš„æ•°é‡ï¼Œè¿™å¯¹äºç¡®å®šä¸‹ä¸€æ¬¡è¿æ¥çš„è¾“å…¥è‡³å…³é‡è¦ã€‚è¿™ç§è®¡ç®—æ–¹æ³•å‡¸æ˜¾äº†å·¦æ·±è®¡åˆ’åœ¨æŸ¥è¯¢æ‰§è¡Œè¿‡ç¨‹ä¸­çš„é‡è¦æ€§ã€‚
The first step is to calculate the cost of this nested loops join using the appropriate cost formula, assuming it's a page-oriented nested loops join. For this type of join, the cost formula is the number of pages of the outer table plus the number of pages of the outer table multiplied by the number of pages of the inner table. Here, the number of pages is 500 for table S and 1000 for table R, resulting in a cost of over 500,000 I/O operations. This calculation represents the cost of the first operation. The challenge now is to determine the cost of the subsequent nested loops join. However, the complexity lies in not knowing the left inputâ€”this input is actually the result of the previous operation. To proceed, I must calculate the result size of this operation to understand the number of tuples that will be pushed up, which is critical for determining the input for the next join. This calculation highlights the importance of left-deep plans in the query execution process.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420165025200.png" alt="image-20240420165025200" style="zoom:50%;" /> 

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420165035271.png" alt="image-20240420165035271" style="zoom:50%;" /> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420170912351.png" alt="image-20240420170912351" style="zoom:50%;" /> 

**2ï¸âƒ£****Calculating cost:**

1. **SxR**: $\text{Cost (SxR) = 500 + 500*1000 = 500500}$
2. **(SxR)xB**: 
   - $\text{Result size (SxR) = 40000*100000 *1/40000= 100000 tuples = 1000 pages}$â€‹


```
è®©æˆ‘ä»¬ç”¨ä»¥å‰å­¦è¿‡çš„å…¬å¼è®¡ç®—è¡¨ S å’Œè¡¨ R ä¹‹é—´è¿æ¥çš„ç»“æœå¤§å°ã€‚è®¡ç®—æ¶‰åŠå·¦å³è¾“å…¥çš„é¡µæ•°ï¼Œå†ä¹˜ä»¥è¿æ¥çš„ä¸¤åˆ—ä¸­æœ€å¤§ä¸åŒé”®æ•°çš„å€’æ•°ã€‚ä¸ºäº†ç¡®å®šç›¸å…³åˆ—ï¼Œæˆ‘ä»¬æŸ¥çœ‹ S å’Œ R ä¹‹é—´çš„è”åˆæ¡ä»¶ï¼Œå…¶ä¸­æ¶‰åŠåˆ— SI å’Œ RSIã€‚SI æ˜¯ "æ°´æ‰‹ "è¡¨çš„ä¸»é”®ï¼Œå³æ°´æ‰‹ IDã€‚ç”±äºæ°´æ‰‹ ID æ˜¯ä¸»é”®ï¼Œæ‰€ä»¥æœ€å¤§çš„ä¸åŒé”®æ•°ä¸æ°´æ‰‹ ID ä¸­çš„è®°å½•æ•°ç›¸å¯¹åº”ã€‚è™½ç„¶ "å‚¨å¤‡ "è¡¨å¯èƒ½åŒ…å«å¤šè¾¾ 100,000 æ¡è®°å½•ï¼Œä½†ä½œä¸ºå¤–é”®çš„æ°´æ‰‹ ID æœ€å¤šåªèƒ½æœ‰ 40,000 ä¸ªä¸åŒçš„å€¼ï¼Œå› ä¸ºä¸€åæ°´æ‰‹å¯ä»¥é¢„è®¢å¤šä¸ªå‚¨å¤‡ï¼Œä½†ä»èƒ½é€šè¿‡å”¯ä¸€çš„æ°´æ‰‹ ID è¿›è¡Œè¯†åˆ«ã€‚è¿™ç§åŒºåˆ†å¯¹äºè®¡ç®—è”åˆæ€§èƒ½è‡³å…³é‡è¦ï¼Œé€šå¸¸éœ€è¦å¯†åˆ‡å…³æ³¨ã€‚
Let's calculate the result size of the join between tables S and R using the formula we've previously learned. This calculation involves the number of pages of the left and right inputs, multiplied by the reciprocal of the maximum number of distinct keys in either of the two columns being joined. To identify the relevant columns, we look at the joint condition between S and R, which involves columns SI and RSI. SI is the primary key of the 'sailors' table, known as sailor ID. The maximum number of distinct keys corresponds to the number of records in sailor ID, given it's the primary key. While the 'reserves' table may contain up to 100,000 records, the sailor ID as a foreign key can only have a maximum of 40,000 distinct values since a sailor can book multiple reserves but still be identified by a unique sailor ID. This distinction is vital for calculating the joint performance and often requires close attention.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420165025200.png" alt="image-20240420165025200" style="zoom:50%;" /> 

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420165035271.png" alt="image-20240420165035271" style="zoom:50%;" /> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420170912351.png" alt="image-20240420170912351" style="zoom:50%;" /> 

**2ï¸âƒ£****Calculating cost:**

1. **SxR**: $\text{Cost (SxR) = 500 + 500*1000 = 500500}$
2. **(SxR)xB**: 
   - $\text{Result size (SxR) = 40000*100000 *1/40000= 100000 tuples = 1000 pages}$â€‹

```
é€šè¿‡ä¸Šè¿°è®¡ç®—å’Œè€ƒè™‘ï¼Œæˆ‘ä»¬å‘ç°è¿æ¥ä¼šäº§ç”Ÿ 100,000 ä¸ªå›¾å…ƒã€‚æ ¹æ®æ¯é¡µå¯å®¹çº³ 100 ä¸ªæ•°æ®å…ƒç»„çš„è§„å®šï¼Œè¿™ä¸€æ“ä½œå°†äº§ç”Ÿ 1,000 é¡µã€‚è¿™ 1000 ä¸ªé¡µé¢å°†è¢«æ¨é€ï¼Œä½œä¸ºä¸‹ä¸€ä¸ªåµŒå¥—å¾ªç¯è¿æ¥çš„è¾“å…¥ã€‚è¿™ä¸€æ­¥å¯¹äºæ¨è¿›æŸ¥è¯¢æ‰§è¡Œè®¡åˆ’çš„å„ä¸ªé˜¶æ®µè‡³å…³é‡è¦ï¼Œå®ƒå±•ç¤ºäº†æ•°æ®åº“æ“ä½œä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œä»¥åŠåœ¨è®¡ç®—æ•°æ®åº“æŸ¥è¯¢çš„æˆæœ¬å’Œæ•ˆç‡æ—¶äº†è§£åº•å±‚æ•°æ®ç»“æ„çš„é‡è¦æ€§ã€‚
Given these calculations and considerations, we find that the join results in 100,000 tuples. With the specification that each page can hold 100 tuples, this equates to 1,000 pages that result from this operation. These 1,000 pages will be pushed up to serve as the input for the next nested loops join. This step is crucial in advancing through the stages of the query execution plan, demonstrating the interaction between database operations and the significance of understanding the underlying data structure when calculating the cost and efficiency of database queries.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420165025200.png" alt="image-20240420165025200" style="zoom:50%;" /> 

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420165035271.png" alt="image-20240420165035271" style="zoom:50%;" /> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420170912351.png" alt="image-20240420170912351" style="zoom:50%;" /> 

**2ï¸âƒ£****Calculating cost:**

1. **SxR**: $\text{Cost (SxR) = 500 + 500*1000 = 500500}$

2. **(SxR)xB**: 
   - $\text{Result size (SxR) = 40000*100000 *1/40000= 100000 tuples = 1000 pages}$
   - $\text{Cost(xB) = 1000 + 1000*10 = 10000}$


```
è¦è®¡ç®—ç¬¬äºŒä¸ªåµŒå¥—å¾ªç¯è¿æ¥çš„æˆæœ¬ï¼Œæˆ‘ä»¬å¯ä»¥è€ƒè™‘ç›´æ¥åº”ç”¨å…¬å¼ã€‚æˆ‘ä»¬å°†ç¬¬ä¸€æ¬¡è¿æ¥è¾“å‡ºçš„ 1,000 é¡µä¹˜ä»¥å…³ç³» B çš„ 1,010 é¡µï¼Œå¾—å‡ºä¸€ä¸ªå¯è§‚çš„æ•°å­—ã€‚ä¸è¿‡ï¼Œéœ€è¦æ³¨æ„çš„ä¸€ä¸ªé‡è¦æ–¹é¢æ˜¯ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯å·¦æ·±è®¡åˆ’ã€‚å·¦æ·±è®¡åˆ’çš„æœ¬è´¨æ˜¯å°†ä¸€ä¸ªæ“ä½œçš„è¾“å‡ºç›´æ¥ç”¨ä½œåç»­æ“ä½œçš„è¾“å…¥ï¼Œè€Œä¸å­˜å‚¨ä¸­é—´ç»“æœã€‚ç”±äºé‡‡ç”¨äº†æµæ°´çº¿æ–¹å¼ï¼Œè¿™ç§æ–¹æ³•ä»æ ¹æœ¬ä¸Šå…è®¸æˆ‘ä»¬æ”¾å¼ƒä¸é¦–æ¬¡è¯»å–ä¸‹ä¸€ä¸ªæ“ä½œçš„å·¦è¾“å…¥ç›¸å…³çš„æˆæœ¬ã€‚è¿™ç§é«˜æ•ˆçš„æ•°æ®æµå¤„ç†æ–¹å¼æ˜¯å·¦æ·±è¿æ¥è®¡åˆ’çš„ä¸»è¦ä¼˜åŠ¿ä¹‹ä¸€ï¼Œå®ƒå¯ä»¥ä¼˜åŒ–å¤„ç†è¿‡ç¨‹ï¼Œæœ€å¤§é™åº¦åœ°å‡å°‘ä¸å¿…è¦çš„æ•°æ®å¤„ç†æˆæœ¬ã€‚
To calculate the cost for the second nested loop join, let's consider applying the formula straightforwardly. We would take the 1,000 pages from the output of the first join and multiply this by the 1,010 pages of relation B, resulting in a substantial number. However, an important aspect to keep in mind is that we are utilizing left-deep plans. The essence of left-deep plans is that the output from one operation is directly used as the input for the subsequent operation, without storing intermediate results. This approach essentially allows us to disğŸš—d the cost associated with the first read of the left input for the next operation, thanks to pipelining. This efficient handling of data flows is one of the key advantages of left-deep join plans, optimizing processing and minimizing unnecessary data handling costs.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420165025200.png" alt="image-20240420165025200" style="zoom:50%;" /> 

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420165035271.png" alt="image-20240420165035271" style="zoom:50%;" /> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420170912351.png" alt="image-20240420170912351" style="zoom:50%;" /> 

**2ï¸âƒ£****Calculating cost:**

1. **SxR**: $\text{Cost (SxR) = 500 + 500*1000 = 500500}$

2. **(SxR)xB**: 

   - $\text{Result size (SxR) = 40000*100000 *1/40000= 100000 tuples = 1000 pages}$
   - $\text{Cost(xB) = 1000 + 1000*10 = 10000}$

   <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420165450197.png" alt="image-20240420165450197" style="zoom:50%;" /> 

   ==Already read â€“left deep plans apply pipelining==

3. $\text{Total Cost = 500 + 500*1000 + 1000 * 10 = 510500 I/O}$â€‹

```
åœ¨æˆ‘ä»¬çš„æ–¹æ³•ä¸­ï¼Œç”±äºå·¦æ·±è®¡åˆ’å›ºæœ‰çš„æµæ°´çº¿ç‰¹æ€§ï¼Œæˆ‘ä»¬æ€»æ˜¯èˆå¼ƒç¬¬ä¸€æ¬¡è¯»å–çš„æˆæœ¬ã€‚å› æ­¤ï¼Œç¬¬äºŒä¸ªåµŒå¥—å¾ªç¯è¿æ¥çš„æˆæœ¬åªå æˆ‘ä»¬è®¡ç®—çš„ç¬¬äºŒéƒ¨åˆ†ã€‚å› æ­¤ï¼Œè¿™æ¬¡è¿æ¥çš„æ€»é¡µæ•°ä¸º 10,000 é¡µã€‚ç„¶åï¼Œæ•´ä¸ªè®¡åˆ’çš„æ€»æˆæœ¬å°±æ˜¯è¿™äº›å•ç‹¬æ“ä½œçš„æ€»å’Œï¼Œæœ€åå¾—å‡ºæˆæœ¬æ€»è®¡ï¼Œæœ‰æ•ˆè¯´æ˜äº†æˆ‘ä»¬çš„æŸ¥è¯¢ä¼˜åŒ–ç­–ç•¥çš„æ•ˆç‡ã€‚
In our approach, we always disğŸš—d the cost of the first read due to the pipelining inherent in left-deep plans. Thus, the cost for the second nested loops join solely comprises the second part of our calculations. This results in a total of 10,000 pages for this join. The overall cost of the entire plan is then simply the sum of these individual operations, culminating in a final tally of costs, effectively illustrating the efficiency of our query optimization strategy.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420165025200.png" alt="image-20240420165025200" style="zoom:50%;" /> 

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420165035271.png" alt="image-20240420165035271" style="zoom:50%;" /> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420170912351.png" style="zoom:50%;" /> 

**2ï¸âƒ£****Calculating cost:**

1. **SxR**: $\text{Cost (SxR) = 500 + 500*1000 = 500500}$

2. **(SxR)xB**: 

   - $\text{Result size (SxR) = 100000*40000 *1/40000= 100000 tuples = 1000 pages}$

```
è®©æˆ‘ä»¬æ·±å…¥æ¢è®¨è¿™ä¸€å®è·µï¼Œå› ä¸ºå®ƒåŒ…å«äº†æˆ‘ä»¬åœ¨è¿‡å»äº”æ¬¡è®²åº§ä¸­è®¨è®ºè¿‡çš„æ¦‚å¿µï¼Œå°¤å…¶æ˜¯å…³äºæˆæœ¬è®¡ç®—çš„æ¦‚å¿µã€‚è¿™ä¸€æ¬¡ï¼Œæˆ‘ä»¬è¦è¯„ä¼°çš„æ˜¯ä¸€ä¸ªä¸åŒçš„è®¡åˆ’ï¼Œå®ƒçœ‹èµ·æ¥ä¸ä¹‹å‰çš„è®¡åˆ’éå¸¸ç›¸ä¼¼ï¼Œä¸»è¦çš„å˜åŒ–æ˜¯åœ¨ä¸Šé¢æ·»åŠ äº†ä¸€ä¸ªå“ˆå¸Œè¿æ¥ã€‚ä¸ä¹‹å‰çš„æ–¹æ³•ç›¸åŒï¼Œæˆ‘ä»¬ä½¿ç”¨ç›¸åŒçš„å…¬å¼ä»åµŒå¥—å¾ªç¯è¿æ¥å¼€å§‹ã€‚æ­¤å¤–ï¼Œç¡®å®šåç»­å“ˆå¸Œè¿æ¥çš„è¾“å…¥ä¹Ÿè‡³å…³é‡è¦ã€‚é€šè¿‡å‚è€ƒä¹‹å‰æ“ä½œçš„ç»“æœå¤§å°ï¼ˆæˆ‘ä»¬ä¹‹å‰è®¡ç®—è¿‡ï¼‰ï¼Œæˆ‘ä»¬çŸ¥é“è¾“å…¥é‡ä¸º 1,000 é¡µã€‚è¿™ä¸€åŸºç¡€æ­¥éª¤å¯¹äºç†è§£ä¸åŒçš„è¿æ¥æ–¹æ³•åŠå…¶ç›¸å…³æˆæœ¬å¦‚ä½•å½±å“æ•´ä¸ªæŸ¥è¯¢è®¡åˆ’è‡³å…³é‡è¦ã€‚
Letâ€™s dive deeper into this practice as it encapsulates the concepts we've discussed in the last five lectures, particularly around costing. This time, we're evaluating a different plan that looks quite similar to the previous one, with the key change being a hash join on top. Starting off in the same manner as before, we begin with a nested loops join using the same formula. Additionally, it's crucial to determine the input for the subsequent hash join. By referencing the result size from the previous operation, which we calculated earlier, we know this input amounts to 1,000 pages. This foundational step is critical for understanding how the different join methods and their associated costs influence the overall query plan.
```


$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420165025200.png" alt="image-20240420165025200" style="zoom:50%;" /> 

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420165035271.png" alt="image-20240420165035271" style="zoom:50%;" /> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420170912351.png" style="zoom:50%;" /> 

**2ï¸âƒ£****Calculating cost:**

1. **SxR**: $\text{Cost (SxR) = 500 + 500*1000 = 500500}$

2. **(SxR)xB**: 

   - $\text{Result size (SxR) = 100000*40000 *1/40000= 100000 tuples = 1000 pages}$
   - $\text{Cost(xB) = 3*1000 + 3*10 = 2*1000 + 3*10 = 2030}$

   <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420170823369.png" alt="image-20240420170823369" style="zoom:50%;" /> 

   ==Already read once â€“left deep plans apply pipelining==

3. $\text{Total Cost = 500 + 500*1000 + 2*1000+ 3*10 = 502530 I/O}$

```
åœ¨åº”ç”¨æ•£åˆ—è¿æ¥å…¬å¼æ—¶ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šå°†å…¶è®¡ç®—ä¸ºå·¦è¾¹è¾“å…¥çš„ä¸‰å€åŠ ä¸Šå³è¾¹è¾“å…¥çš„ä¸‰å€ã€‚ç„¶è€Œï¼Œç”±äºæœ¬è®¡åˆ’ä½¿ç”¨çš„æ˜¯å·¦æ·±æ ‘ç»“æ„ï¼Œæµæ°´çº¿çš„æ•ˆæœå…è®¸æˆ‘ä»¬æ”¾å¼ƒç¬¬ä¸€æ¬¡è¯»å–çš„æˆæœ¬ã€‚å› æ­¤ï¼Œå…¬å¼è°ƒæ•´ä¸ºå·¦è¾“å…¥çš„ 2 å€åŠ å³è¾“å…¥çš„ 3 å€ã€‚è¿™ä¸€ä¿®æ­£è§£å†³äº†å­¦ç”Ÿä¸­å¸¸è§çš„è¯¯è§£ï¼Œä»–ä»¬å¯èƒ½ä¼šè®¤ä¸ºç”±äºæµæ°´çº¿çš„ä½œç”¨ï¼Œå·¦è¾“å…¥çš„æˆæœ¬å¯ä»¥å®Œå…¨èˆå¼ƒã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¿…é¡»æ˜ç™½ï¼Œåªæœ‰ç¬¬ä¸€æ¬¡è¯»å–çš„ä»£ä»·æ‰ä¼šè¢«æ¶ˆé™¤ã€‚åœ¨æ•£åˆ—è¿æ¥çš„æƒ…å†µä¸‹ï¼Œè¿‡ç¨‹æ¶‰åŠåˆ›å»ºæ•°æ®åˆ†åŒºï¼Œå¹¶å°†å…¶æš‚æ—¶å­˜å‚¨å›ç£ç›˜ï¼Œç„¶åå†æ¬¡è¯»å–è¿™äº›åˆ†åŒºä¸å³è¾“å…¥åˆå¹¶ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸èƒ½å®Œå…¨ä¸¢å¼ƒå·¦ä¾§è¾“å…¥ï¼Œåªèƒ½è·³è¿‡ç¬¬ä¸€æ¬¡è¯»å–ã€‚æŒ‰ç…§è¿™ç§ç†è§£ï¼Œæ€»æˆæœ¬æ˜¯ä»¥è¿™äº›è°ƒæ•´åçš„æ“ä½œä¹‹å’Œæ¥è®¡ç®—çš„ï¼Œè¿™åæ˜ äº†æˆ‘ä»¬å¯¹æµæ°´çº¿æ¡ä»¶ä¸‹æ•£åˆ—è¿æ¥æ‰€æ¶‰åŠçš„æˆæœ¬æœ‰äº†æ›´ç»†è‡´çš„äº†è§£ã€‚
When applying the hash join formula, typically we would calculate it as three times the left input plus three times the right input. However, since this plan uses a left-deep tree structure, the effect of pipelining allows us to disğŸš—d the cost of the first read. Consequently, the formula adjusts to two times the left input plus three times the right input. This correction addresses a common misunderstanding among students, who might think that the left input cost can be entirely disğŸš—ded due to pipelining. However, it's essential to understand that only the cost of the first read is eliminated. In the case of a hash join, the process involves creating partitions of the data, which are temporarily stored back on disk, and then those partitions are read again to merge with the right input. Therefore, we cannot fully disğŸš—d the left input; only the first read is skipped. Following this understanding, the total cost is calculated as the sum of these adjusted operations, reflecting a more nuanced appreciation of the costs involved in hash joins under pipelining conditions.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420171027821.png" alt="image-20240420171027821" style="zoom:50%;" />  

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420171055179.png" alt="image-20240420171055179" style="zoom:50%;" /> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420171102975.png" alt="image-20240420171102975" style="zoom:50%;" /> 

$\begin{aligned}
& S: \text { NPages }(S)=500, \text { NTuplesPerPage }(S)=80 \\
& R:  \text { NPages }(R)=1000, \text { NTuplesPerPage }(R)=100 \\
& B: \text { NPages }(B)=10
\end{aligned}$â€‹

Calculating cost: ==Cost (P3) = ? Cost (P4) = ?==

```
ç°åœ¨ï¼Œæˆ‘æœ‰ä¸€é¡¹ä»»åŠ¡äº¤ç»™ä½ ä»¬ã€‚è¯·èŠ±ä¸€ç‚¹æ—¶é—´æš‚åœä¸€ä¸‹ï¼Œè®¡ç®—ä¸€ä¸‹è¿™ä¸¤ä¸ªè®¡åˆ’çš„æˆæœ¬ã€‚è™½ç„¶æˆ‘çŸ¥é“è¿™çœ‹èµ·æ¥å¾ˆæœ‰æŒ‘æˆ˜æ€§ï¼Œä½†å‚ä¸è¿™é¡¹ç»ƒä¹ å¯¹äºå·©å›ºä½ ä»¬çš„å­¦ä¹ æˆæœè‡³å…³é‡è¦ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡åº”ç”¨æˆ‘ä»¬è®¨è®ºè¿‡çš„æ¦‚å¿µï¼Œå°†ä½ è‚¤æµ…çš„ç†è§£è½¬åŒ–ä¸ºæ·±åˆ»çš„æ´å¯ŸåŠ›ã€‚å› æ­¤ï¼Œè¯·ç»§ç»­æŒ‰ä¸‹æš‚åœé”®ï¼Œæ·±å…¥è®¡ç®—ï¼ŒçœŸæ­£æŒæ¡è¿™äº›è¿ç®—çš„æœºæ¢°åŸç†ã€‚è¿™ä¸€ç»ƒä¹ å°†æœ‰åŠ©äºå·©å›ºæˆ‘ä»¬è¿„ä»Šä¸ºæ­¢æ‰€æ¶‰åŠçš„çŸ¥è¯†ï¼ŒåŠ æ·±å¯¹æŸ¥è¯¢ä¼˜åŒ–çš„ç†è§£ã€‚
Now, I have a task for you. Please take a moment to pause here and calculate the cost of these two plans. While I understand it might seem challenging, engaging with this exercise is crucial for reinforcing your learning. We aim to transform your superficial understanding into profound insight by applying the concepts we've discussed. So, go ahead and press pause, delve into the calculations, and really try to grasp the mechanics of these operations. This exercise will help cement the knowledge we've covered so far and deepen your understanding of query optimization.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420171027821.png" alt="image-20240420171027821" style="zoom:50%;" /> 

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420235957086.png" alt="image-20240420235957086" style="zoom:50%;" /> 

```
è®©æˆ‘ä»¬ä¸€èµ·æ¥å®Œæˆ Petri è®¡åˆ’ã€‚å¯¹äºè¿™ä¸ªå“ˆå¸Œè¿æ¥ï¼Œæˆ‘ä»¬è®¡ç®—çš„æˆæœ¬æ˜¯å·¦è¾¹è¾“å…¥çš„ä¸‰å€åŠ ä¸Šå³è¾¹è¾“å…¥çš„ä¸‰å€ã€‚å·¦è¾¹çš„è¾“å…¥æ˜¯å…³ç³» Sï¼Œç›¸å½“äº 500 çš„ä¸‰å€ã€‚å³ä¾§è¾“å…¥ä¸ºå…³ç³» Rï¼Œåˆ™ä¸º 1000 çš„ä¸‰å€ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨è®¡ç®—å™¨è®¡ç®—ç²¾ç¡®å€¼ï¼Œä½†ä¸ºäº†ä¿æŒè®¨è®ºçš„æµç•…æ€§ï¼Œå¹¶å°†é‡ç‚¹æ”¾åœ¨è¿‡ç¨‹è€Œä¸æ˜¯å…·ä½“æ•°å­—ä¸Šï¼Œæˆ‘ä»¬æš‚æ—¶ä¸è¿›è¡Œè¿™ä¸€æ­¥éª¤ã€‚è¿™ç§æ–¹æ³•æœ‰åŠ©äºæˆ‘ä»¬ç†è§£æ•£åˆ—è¿æ¥æ–¹æ¡ˆä¸­æ¶‰åŠçš„åŸºç¡€æˆæœ¬ã€‚
Let's walk through the Petri plan together. The first calculation involves the cost of the hash join between S and R. For this hash join, we calculate the cost as three times the left input plus three times the right input. With the left input being relation S, this equates to three times 500. The right input, relation R, is three times 1000. Typically, we would calculate the precise value using a calculator, but to keep our discussion flowing and focus on the process rather than specific numbers, we'll proceed without that step for now. This approach helps us understand the foundational costs involved in this hash join scenario.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420171027821.png" alt="image-20240420171027821" style="zoom:50%;" /> 

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240421000203229.png" alt="image-20240421000203229" style="zoom:50%;" /> 

```
è¿™åŒ…æ‹¬è®¡ç®— S ä¸­çš„æ•°æ®å…ƒç»„æ•°ä¹˜ä»¥ R ä¸­çš„æ•°æ®å…ƒç»„æ•°ï¼Œå†æ ¹æ® C ID åˆ—ä¸­ä¸åŒé”®æ•°çš„å€’æ•°è¿›è¡Œè°ƒæ•´ã€‚è¿™ä¸€æ­¥è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒç¡®å®šäº†åµŒå¥—å¾ªç¯è¿æ¥éœ€è¦å¤„ç†çš„æ•°æ®é‡ï¼Œè®©æˆ‘ä»¬æ›´æ¸…æ¥šåœ°äº†è§£è¯¥è¿æ¥å¯¹ç³»ç»Ÿçš„è®¡ç®—éœ€æ±‚ã€‚
Next, we move on to the nested loops join, but first, we need to determine the result size of the join between S and R. This involves calculating the number of tuples in S multiplied by the number of tuples in R, adjusted by the reciprocal of the number of distinct keys in the C ID column, a number we've previously computed. This step is crucial as it establishes the volume of data that the nested loops join will need to process, providing us with a clearer picture of the computational demand this join will place on the system.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420171027821.png" alt="image-20240420171027821" style="zoom:50%;" /> 

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240421000438160.png" alt="image-20240421000438160" style="zoom:50%;" /> 

```
å¥½äº†ï¼Œè®©æˆ‘ä»¬æ¥åˆ†è§£ä¸€ä¸‹è¿™é‡Œçš„è®¡ç®—ã€‚æˆ‘ä»¬ç”¨ 40,000 ä¹˜ä»¥ 100,000 å†é™¤ä»¥ 40,000ã€‚è¿™æ ·å¾—å‡ºçš„ç¼©å‡ç³»æ•°å°±æ˜¯ 100,000 ä¸ªå›¾å…ƒã€‚æ ¹æ®è®¡ç®—ç»“æœï¼Œä¸€ä¸ªé¡µé¢å¯ä»¥å®¹çº³æ•°ç™¾ä¸ªè¿™æ ·çš„å›¾å…ƒã€‚å› æ­¤ï¼Œè¿™ç›¸å½“äºå¤§çº¦ 1,000 é¡µã€‚
OK, let's break down the calculation we have here. We multiply 40,000 by 100,000 and then divide by 40,000. This results in a reduction factor that yields 100,000 tuples. According to the results, hundreds of such tuples can fit on a single page. Therefore, this equates to approximately 1,000 pages.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420171027821.png" alt="image-20240420171027821" style="zoom:50%;" /> 

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240421000606824.png" alt="image-20240421000606824" style="zoom:50%;" /> 

```
è¯¥è®¡ç®—ä¸åç»­åµŒå¥—å¾ªç¯è¿æ¥çš„å·¦è¾“å…¥æœ‰å…³ã€‚åœ¨æ­£å¸¸æƒ…å†µä¸‹ï¼Œä¸ B åµŒå¥—å¾ªç¯è¿æ¥çš„è´¹ç”¨æ˜¯ 1,000 åŠ  1,000 ä¹˜ä»¥ 10ã€‚ä½†æ˜¯ï¼Œç”±äºè¿™æ˜¯ä¸€ä¸ªå·¦æ·±æ ‘ç»“æ„ï¼Œåªè€ƒè™‘äº†åˆå§‹éƒ¨åˆ†ã€‚å› æ­¤ï¼Œè¯¥æ“ä½œçš„æ€»è´¹ç”¨ä¸º 10,000ã€‚
This calculation pertains to the left input for the subsequent nested loops join. Under normal circumstances, the cost of a nested loops join with B would be 1,000 plus 1,000 multiplied by 10. However, due to this being a left deep tree structure, only the initial part is considered. Consequently, the total cost of this operation amounts to 10,000.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420171027821.png" alt="image-20240420171027821" style="zoom:50%;" /> 

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240421000703912.png" alt="image-20240421000703912" style="zoom: 50%;" /> 

```
æœ€åï¼Œæ•´ä¸ªå·¥å‚çš„æˆæœ¬ P3 æ˜¯æŒ‡ç¬¬ä¸€éƒ¨åˆ†ï¼Œå³è¿™ä¸ªçš„ä¸‰å€åŠ ä¸Š 1000 çš„ä¸‰å€ï¼Œå†åŠ ä¸Šè¿™ä¸ªï¼Œå†åŠ ä¸Š 1000ã€‚
And then finally, cost of entire plant P three is uh the first part which is three times this plus three times 1000 then plus this, then 1000. 
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240421000852622.png" alt="image-20240421000852622" style="zoom:50%;" /> 

```
ç°åœ¨è®©æˆ‘ä»¬æ¥çœ‹çœ‹ P4 è®¡åˆ’ï¼Œæˆ‘å°†ç®€è¦ä»‹ç»ä¸€ä¸‹è¯¥è®¡åˆ’ã€‚åº•å±‚ä¿æŒä¸å˜ã€‚æˆ‘ä»¬çš„æˆæœ¬æ˜¯ä¿¡å™ªæ¯”ï¼Œè¿™ä¸ªæ•°å­—æˆ‘ä»¬å·²ç»é‡åˆ°è¿‡ä¸‰æ¬¡äº†ã€‚ç»“æœå¤§å°ä¹Ÿä¸æˆ‘ä»¬ä¹‹å‰è®¡ç®—çš„ä¸€è‡´ã€‚ä¸ºäº†æé«˜æ•ˆç‡ï¼Œæˆ‘å°†é‡å¤ä½¿ç”¨è¿™ä¸ªæ•°å­—ã€‚æœ€åï¼Œæˆ‘ä»¬è¿›è¡Œç¬¬äºŒæ¬¡å“ˆå¸Œè¿æ¥ã€‚
Let's now move on to plan P4, which I'll outline quickly. The bottom layer remains unchanged. We have the cost of SNR, a figure we've encountered three times already. The result size is also consistent with what we've previously calculated. For the sake of efficiency, I will reuse this number. Finally, we then proceed to the second hash join.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240421001056367.png" alt="image-20240421001056367" style="zoom:50%;" /> 

```
å¥½äº†ï¼Œåœ¨ç¬¬äºŒä¸ªå“ˆå¸Œè¿æ¥ï¼ˆæ¶‰åŠæµæ°´çº¿å¤„ç†ï¼‰ä¸­ï¼Œä¸ B ç›¸å…³çš„æˆæœ¬è®¡ç®—ä¸ºå·¦è¾¹è¾“å…¥å¤§å°çš„ 2 å€åŠ ä¸Šå³è¾¹è¾“å…¥å¤§å°çš„ 3 å€ã€‚å…·ä½“æ¥è¯´ï¼Œå°±æ˜¯ 1,000 çš„ 2 å€åŠ ä¸Š 10 çš„ 3 å€ã€‚å°†è¿™äº›ç›¸åŠ ï¼Œå°±æ˜¯è®¡åˆ’ P4 çš„æœ€ç»ˆæˆæœ¬ã€‚åªéœ€å¿«é€Ÿè®¡ç®—å³å¯ï¼Œè™½ç„¶æ•°å­¦å¾ˆç®€å•ï¼Œä½†å¯¹è®¡ç®—è¿‡ç¨‹è‡³å…³é‡è¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è®¡ç®—å‡º 2 ä¹˜ä»¥ 1,000 å†åŠ ä¸Š 3 ä¹˜ä»¥ 10ï¼Œå°±å¾—å‡ºäº†æ•´ä¸ªè®¡åˆ’çš„æˆæœ¬ã€‚
OK, the cost associated with B in the second hash join, which involves pipeline processing, is calculated as two times the size of the left input, plus three times the size of the right input. Specifically, it's two times 1,000 plus three times 10. Summing these up gives us the final cost for plan P4. Just to quickly compute, itâ€™s simple math but crucial to the process. Thus, we calculate two times 1,000 plus three times 10, which totals the cost of the entire plan.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420171206720.png" alt="image-20240420171206720" style="zoom:50%;" /> 

$\begin{aligned}
& S: \text { NPages }(S)=500, \text { NTuplesPerPage }(S)=80 \\
& R:  \text { NPages }(R)=1000, \text { NTuplesPerPage }(R)=100 \\
& B: \text { NPages }(B)=10, \text{NTuplesPerPage(B) = 10}\\
& SMJ : \text{2 passes, RxB: 10 tuplesPerPage}\\
& I(S.sid); NPages(I) = 50\\
\end{aligned}$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420171355685.png" alt="image-20240420171355685" style="zoom:50%;" /> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420171402248.png" alt="image-20240420171402248" style="zoom:50%;" /> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420171409373.png" alt="image-20240420171409373" style="zoom:50%;" /> 

```
å¥½äº†ï¼Œç°åœ¨è®©æˆ‘ä»¬æ¥çœ‹çœ‹è¿™ä¸‰ä¸ªæœ‰è¶£çš„è®¡åˆ’ã€‚æˆ‘å¼ºçƒˆå»ºè®®ä½ æ·±å…¥ç»ƒä¹ è¿™äº›è®¡ç®—ã€‚ä¸€æ—¦ä½ å¼€å§‹å­¦ä¹ ä¸€äº›è®¡åˆ’ï¼Œæµæ°´çº¿å¤„ç†å’Œä¸¢å¼ƒç­‰æ¦‚å¿µå°±ä¼šæˆä¸ºä½ çš„ç¬¬äºŒå¤©æ€§ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘å°†ä»‹ç»ä¸‰ä¸ªæœ‰è¶£çš„è®¡åˆ’ä¾›å¤§å®¶æ¢ç´¢ã€‚å‡­å€Ÿç›®å‰æ‰€å­¦åˆ°çš„çŸ¥è¯†ï¼Œä½ å®Œå…¨æœ‰èƒ½åŠ›æ‰§è¡Œè¿™äº›æ“ä½œã€‚åœ¨ä½ å°è¯•è¿‡è¿™äº›ç¤ºä¾‹åï¼Œæˆ‘ç¨åä¼šä¸ä½ åˆ†äº«è§£å†³æ–¹æ¡ˆã€‚è¯·ç‰¹åˆ«æ³¨æ„æˆ‘ä»¬åœ¨ä½¿ç”¨ç´¢å¼•æ‰«æåè¿›è¡ŒçŸ­åˆå¹¶è¿æ¥çš„æƒ…å†µã€‚åœ¨æœ¬è®²åº§çš„å‰åŠéƒ¨åˆ†ï¼Œæˆ‘ä»¬å·²ç»æš—ç¤ºäº†ä½¿ç”¨ç´¢å¼•çš„å¥½å¤„ï¼Œæœ‰æ—¶åŸºæœ¬ä¸Šå¯ä»¥å…è´¹è·å¾—ç´¢å¼•ã€‚æˆ‘å¸Œæœ›ä½ èƒ½å‘ç°è¿™ä¸ªç»ƒä¹ çš„å¸å¼•åŠ›ï¼Œæˆ‘æœŸå¾…ç€ç¨åä¸ä½ åˆ†äº«ç»“æœã€‚
OK, let's now look at these three interesting plans. I highly encourage you to delve deeper into practicing these calculations. Once you start working through a few plans, concepts like pipeline processing and disğŸš—ding will become second nature to you. Here, Iâ€™m presenting three intriguing plans for you to explore. With the knowledge you've gained so far, you're well-equipped to perform these operations. I'll share solutions later, after youâ€™ve attempted these examples. Pay particular attention to the scenario where we use an index scan followed by a short merge join. The benefits of using indexes, which can sometimes be obtained essentially for free, were hinted at earlier in this lecture. I hope you find this exercise engaging, and I look forward to sharing the results with you later.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240420171431581.png" alt="image-20240420171431581" style="zoom:50%;" /> 

**1ï¸âƒ£**Understand plan enumeration and cost various plans
**2ï¸âƒ£**Important for Assignment 3 as well

```
æœ€åï¼Œæˆ‘ä»¬å·²ç»äº†è§£äº†æŸ¥è¯¢è®¡åˆ’æ˜¯å¦‚ä½•æšä¸¾å’Œè®¡ç®—æˆæœ¬çš„ï¼Œä»è€Œå®Œæˆäº†å¯¹æŸ¥è¯¢ä¼˜åŒ–å™¨ä½œç”¨çš„è®¨è®ºã€‚æˆ‘ä»¬å·²ç»å½»åº•äº†è§£äº†æŸ¥è¯¢ä¼˜åŒ–å™¨çš„ä½œç”¨ä»¥åŠå®ƒæ˜¯å¦‚ä½•å®Œæˆä»»åŠ¡çš„ã€‚è¿™äº›çŸ¥è¯†åœ¨ä½ çš„èŒä¸šç”Ÿæ¶¯ä¸­å°†ä¼šç‰¹åˆ«æœ‰ç”¨ï¼Œå°¤å…¶æ˜¯å½“ä½ é‡åˆ°è¿è¡Œæ—¶é—´è¿‡é•¿æˆ–è¡Œä¸ºä¸å¯é¢„æµ‹çš„é—®é¢˜æŸ¥è¯¢æ—¶ã€‚åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬å°†æ•™ä½ å¦‚ä½•æ£€æŸ¥æŸ¥è¯¢è®¡åˆ’ï¼Œæ‰§è¡Œä¸€äº›æˆæœ¬åˆ†æï¼Œå¹¶è€ƒè™‘å…¶ä»–æ–¹æ³•æ¥å‘ç°å’Œè§£å†³é—®é¢˜ã€‚æ‚¨å¯èƒ½ä¼šå‘ç°ï¼Œå»ºè®®ä½¿ç”¨ä¸åŒçš„ç´¢å¼•å¯ä»¥å¤§å¤§æé«˜æŸ¥è¯¢çš„æ€§èƒ½ï¼Œä½¿å…¶æ›´é«˜æ•ˆåœ°è¿è¡Œã€‚ä»æœ¬è´¨ä¸Šè®²ï¼Œæˆ‘ä»¬æ­£åœ¨æ¶‰è¶³æ•°æ®åº“ç®¡ç†å‘˜çš„è§’è‰²ï¼Œè¿™å°†æ˜¯ä½ ä»¬ç¬¬ä¸‰æ¬¡ä½œä¸šçš„é‡ç‚¹ã€‚æ„Ÿè°¢å„ä½çš„å…³æ³¨ï¼Œæˆ‘æœŸå¾…ç€ä¸‹ä¸€å ‚è¯¾çš„åˆ°æ¥ã€‚
So, to wrap up, we've now learned how query plans are enumerated and subsequently costed, completing our discussion on the role of the query optimizer. We've thoroughly covered what it does and how it accomplishes its tasks. This knowledge will be particularly useful in your professional life, especially when you encounter problematic queries that run too long or behave unpredictably. In your labs, we will teach you how to examine the query plan, perform some cost analysis, and consider alternative approaches to identify and solve issues. You may find that suggesting a different index could dramatically enhance the query's performance, making it run much more efficiently. Essentially, we're dabbling in the role of a database administrator, and this will be the focus of your third assignment. Thank you for your attention, and I look forward to our next session.
```


$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$
