<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403204519226.png" alt="image-20240403204519226" style="zoom: 33%;" /> 

```
å¤§å®¶å¥½ï¼Œæ¬¢è¿å›åˆ°æ•°æ®åº“ç³»ç»Ÿç¬¬ 11 è®²ã€‚ä»Šå¤©ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨æŸ¥è¯¢å¤„ç†è¿™ä¸€ä¸»é¢˜ã€‚æˆ‘æƒ³ç»™å¤§å®¶æä¸ªé†’ï¼šæœ¬è®²åº§å¯èƒ½æ˜¯æœ¬ç³»åˆ—ä¸­æœ€å…·æŒ‘æˆ˜æ€§çš„ä¸€è®²ã€‚æˆ‘ä»¬å°†ä»‹ç»å„ç§æ–°æœ¯è¯­å’Œä¸€äº›å…¬å¼ã€‚ä¸è¿‡ï¼Œæˆ‘é¼“åŠ±å¤§å®¶ä¿æŒæŠ•å…¥å’Œè€å¿ƒã€‚åœ¨æ¥ä¸‹æ¥çš„å‡ è®²ä¸­ï¼Œæˆ‘ä»¬å°†ç»§ç»­ç»ƒä¹ å’Œè§£è¯»è¿™äº›æ¦‚å¿µï¼Œç¡®ä¿ä½ èƒ½å½»åº•ç†Ÿæ‚‰å®ƒä»¬ã€‚
Hello, everyone, and welcome back to our 11th lecture on database systems. Today, we'll delve deeper into the topic of query processing. I want to give you a heads-up: this lecture might be the most challenging one in the series. We're going to introduce a variety of new terms and some formulas. However, I encourage you to stay engaged and patient. Over the next few lectures, we'll continue to practice and unpack these concepts to ensure that you become thoroughly acquainted with them.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403204228406.png" alt="image-20240403204228406" style="zoom: 33%;" /> 

```
åœ¨ä¸Šä¸€è¯¾ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†æ•°æ®åº“ç³»ç»Ÿçš„ç»„æˆéƒ¨åˆ†ï¼Œå¹¶å¼ºè°ƒæŸ¥è¯¢å¤„ç†æ¨¡å—æ˜¯ä¸€ä¸ªå…³é”®è¦ç´ ã€‚è¯¥æ¨¡å—åœ¨å†³å®šå¦‚ä½•ä»¥æœ€æœ‰æ•ˆçš„æ–¹å¼æ‰§è¡Œæˆ‘ä»¬è¾“å…¥çš„æŸ¥è¯¢æ–¹é¢èµ·ç€å…³é”®ä½œç”¨ã€‚æˆ‘ä»¬å°†åœ¨æ•´ä¸ªè¯¾ç¨‹ä¸­æ¢è®¨è¯¥æ¨¡å—ä¸­çš„å¤šä¸ªç»„ä»¶ã€‚ä¸è¿‡ï¼Œä»Šå¤©æˆ‘ä»¬çš„é‡ç‚¹å°†æ”¾åœ¨æ‰§è¡Œå™¨ä¸Šã€‚æ‰§è¡Œå™¨æ˜¯å†³å®šå¦‚ä½•æ‰§è¡ŒæŸ¥è¯¢çš„æ¯ä¸ªæ­¥éª¤çš„å…³é”®ç»„ä»¶ã€‚
In our last session, we discussed the components of database systems, highlighting the query processing module as a crucial element. This module plays a pivotal role in determining how a query we input is executed in the most efficient manner possible. We'll be exploring several components within this module throughout our course. Today, however, our focus will be on the executor. The executor is a critical component that dictates how each individual step of a query is executed.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403204704994.png" alt="image-20240403204704994" style="zoom:33%;" /> 

```
é¦–å…ˆï¼Œæˆ‘å°†ç®€è¦ä»‹ç»æˆ‘ä»¬è¾“å…¥æŸ¥è¯¢æ—¶å‘ç”Ÿçš„æƒ…å†µã€‚è¿™ä¸ªè¿‡ç¨‹åŒ…æ‹¬å°†æˆ‘ä»¬çš„æŸ¥è¯¢è½¬åŒ–ä¸ºæŸ¥è¯¢è®¡åˆ’ã€‚æŸ¥è¯¢è®¡åˆ’å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªè“å›¾æˆ–ä¸€ç³»åˆ—æ­¥éª¤ï¼Œæ¦‚è¿°äº†å¦‚ä½•æ£€ç´¢æˆ‘ä»¬çš„æ•°æ®ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†ä»ç®—æ³•çš„è§’åº¦æ·±å…¥æ¢è®¨å¦‚ä½•æ‰§è¡Œè¯¥è®¡åˆ’çš„æ¯ä¸€æ­¥ã€‚åœ¨ä»Šå¤©çš„è®²åº§ä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹è®¨è®ºé€‰æ‹©å’Œé¢„æµ‹ã€‚åœ¨ä¸‹ä¸€è¯¾ä¸­ï¼Œæˆ‘ä»¬å°†ä¸“é—¨è®¨è®ºè¿æ¥ã€‚å¦‚æœä½ å‘ç°è‡ªå·±éœ€è¦å¤ä¹ æˆ–é¢å¤–çš„æ”¯æŒï¼Œæˆ‘å»ºè®®ä½ å‚è€ƒæ•™ç§‘ä¹¦ä¸­çš„ç¬¬ 12 ç« å’Œç¬¬ 14 ç« ã€‚
First, I'll provide a brief overview of what happens when we input a query. This process involves transforming our query into something known as a query plan. A query plan can be thought of as a blueprint or a sequence of steps that outlines how to retrieve our data. We'll then delve into how each step of this plan can be executed from an algorithmic standpoint. For today's lecture, we will concentrate on selections and projections. In our next session, we will focus exclusively on joins. Should you find yourself needing a refresher or additional support, I encourage you to refer back to chapters 12 and 14 in our textbook.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403221722170.png" alt="image-20240403221722170" style="zoom:33%;" /> 

â€¢Some database operations are EXPENSIVE

â€¢DBMSs can greatly improve performance by being â€˜smartâ€™
â€“ e.g., can speed up 1,000,000x over naÃ¯ve approach

â€¢ Main weapons are:

1. clever implementation techniques for operators
2. exploiting â€˜equivalenciesâ€™of relational operators
3. using cost models to choose among alternatives

```
äº†è§£æŸ¥è¯¢å¤„ç†å’Œæ‰§è¡Œè‡³å…³é‡è¦ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºæ•°æ®åº“æ“ä½œæœ¬èº«æˆæœ¬é«˜æ˜‚ã€‚è™½ç„¶è®¿é—®å•ä¸ªå…ƒç»„çš„æˆæœ¬å¯èƒ½å¾ˆä½ï¼Œä½†æ•°æ®åº“äº¤äº’é€šå¸¸æ¶‰åŠå¤§é‡æ•°æ®ã€‚ä¸€ä¸ªç¨å¾®æ¬¡ä¼˜çš„æ“ä½œï¼Œå¦‚æœæ‰©å±•åˆ°æˆç™¾ä¸Šåƒæ¡è®°å½•ï¼Œå°±ä¼šå¯¼è‡´æ•ˆç‡å¤§å¹…ä¸‹é™ã€‚é‡è¦çš„æ˜¯è¦è®°ä½ï¼Œåœ¨åºå¤§çš„æ•°æ®é›†ä¸Šè¿›è¡Œæ“ä½œçš„æ€»æˆæœ¬å¯èƒ½éå¸¸å¯è§‚ã€‚å› æ­¤ï¼Œå³ä½¿æ˜¯çœ‹ä¼¼å¾®å°çš„ä½æ•ˆç‡ï¼Œä¹Ÿå¯èƒ½å¯¼è‡´æŸ¥è¯¢éœ€è¦æ•°å¤©æ‰èƒ½å®Œæˆã€‚è¦ç¡®ä¿æ€§èƒ½ä¸ä»…æ˜¯è¶³å¤Ÿçš„ï¼Œè€Œä¸”æ˜¯æœ€ä½³çš„ï¼Œä¼˜åŒ–æ¯é¡¹æ“ä½œè‡³å…³é‡è¦ã€‚è¿™ç§ä¼˜åŒ–çš„å½±å“æ˜¯å·¨å¤§çš„ï¼Œé€šå¸¸èƒ½å°†æ€§èƒ½æé«˜ 100 ä¸‡å€æˆ–æ›´å¤šã€‚è¿™ç§æå‡å¯èƒ½æ„å‘³ç€å°†æŸ¥è¯¢æ‰§è¡Œæ—¶é—´ä»æ•°å¤©ç¼©çŸ­åˆ°å‡ æ¯«ç§’ï¼Œè¿™æ— ç–‘æ˜¯æˆ‘ä»¬çš„ç›®æ ‡ã€‚æ¯•ç«Ÿï¼Œæ²¡æœ‰äººæ„¿æ„ä¸ºæŸ¥è¯¢ç»“æœç­‰å¾…å¾ˆé•¿æ—¶é—´ã€‚
Understanding query processing and execution is crucial, primarily because database operations are inherently costly. While accessing a single tuple might be inexpensive, database interactions typically involve large volumes of data. A marginally suboptimal operation, when scaled to hundreds or thousands of records, can lead to a significant decrease in efficiency. It's vital to remember that the aggregate cost of operations over vast datasets can be substantial. Therefore, even a seemingly minor inefficiency can result in queries that take days to complete. Optimization of every operation is paramount to ensure that the performance is not just adequate but optimal. The impact of this optimization is dramaticâ€”often improving performance by a factor of a million or more. This enhancement could mean reducing query execution time from days to mere milliseconds, which is, undeniably, our objective. After all, no one wants to wait an age for a query to yield results.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403221722170.png" alt="image-20240403221722170" style="zoom:33%;" /> 

â€¢Some database operations are EXPENSIVE

â€¢DBMSs can greatly improve performance by being â€˜smartâ€™
â€“ e.g., can speed up 1,000,000x over naÃ¯ve approach

â€¢ Main weapons are:

1. clever implementation techniques for operators
2. exploiting â€˜equivalenciesâ€™of relational operators
3. using cost models to choose among alternatives

```
é‚£ä¹ˆï¼Œæ•°æ®åº“æ˜¯å¦‚ä½•å®ç°å¦‚æ­¤é«˜æ°´å¹³çš„æŸ¥è¯¢ä¼˜åŒ–çš„å‘¢ï¼Ÿæœ‰å‡ ç§å…³é”® "æ­¦å™¨ "å¯ä¾›ä½¿ç”¨ã€‚é¦–å…ˆæ˜¯åº”ç”¨å„ç§å…³ç³»ä»£æ•°è¿ç®—ç¬¦çš„å¤æ‚å®ç°æŠ€æœ¯ã€‚æˆ‘ä»¬å°†æ¢è®¨è¿™äº›æŠ€æœ¯ï¼Œå¹¶äº†è§£å…³ç³»ä»£æ•°å¦‚ä½•æ”¯æŒæˆ‘ä»¬è¯†åˆ«ç­‰ä»·ä½†æ›´é«˜æ•ˆçš„æŸ¥è¯¢è®¡åˆ’ã€‚
So, how do databases achieve such levels of query optimization? There are a few key 'weapons' at their disposal. The first is the application of sophisticated implementation techniques for various relational algebra operators. We'll explore these techniques and see how relational algebra underpins our ability to identify equivalent, yet more efficient, query plans.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403221722170.png" alt="image-20240403221722170" style="zoom:33%;" /> 

**1ï¸âƒ£**Some database operations are EXPENSIVE

**2ï¸âƒ£**DBMSs can greatly improve performance by being â€˜smartâ€™

- e.g., can speed up 1,000,000x over naÃ¯ve approach

**3ï¸âƒ£**Main weapons are:

1. clever implementation techniques for operators
2. exploiting â€˜equivalenciesâ€™of relational operators
3. using cost models to choose among alternatives

```
æœ€åï¼Œæ•°æ®åº“åˆ©ç”¨æˆæœ¬æ¨¡å‹åœ¨æ— æ•°å¯èƒ½çš„æŸ¥è¯¢è®¡åˆ’ä¸­è¿›è¡Œå¯¼èˆªã€‚é€šè¿‡è¯„ä¼°ä¸åŒæ–¹æ³•çš„æˆæœ¬ï¼Œæ•°æ®åº“å¯ä»¥é€‰æ‹©æœ€æœ‰æ•ˆçš„æ‰§è¡Œè·¯å¾„ã€‚è¿™ä¸ä»…ä»…æ˜¯è¦æ‰¾åˆ°å¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œè€Œæ˜¯è¦åœ¨èµ„æºåˆ©ç”¨ç‡å’Œæ—¶é—´æ•ˆç‡æ–¹é¢æ‰¾åˆ°æœ€ä½³æ–¹æ¡ˆã€‚æ‰€é€‰æ‹©çš„è®¡åˆ’å¾€å¾€èƒ½ä»¥æœ€ä½çš„æˆæœ¬è·å¾—æœ€ä½³çš„æ€§èƒ½ï¼Œä»è€Œå°†åŸæœ¬æ¯ç‡¥å†—é•¿çš„æµç¨‹è½¬å˜ä¸ºå¿«é€Ÿç®€åŒ–çš„æ“ä½œã€‚
Finally, databases utilize cost models to navigate through the myriad of possible query plans. By assessing the cost of different approaches, databases can select the most efficient execution path. This is not just about finding a workable solution; it's about finding the best possible one in terms of resource utilization and time efficiency. The selected plan is often the one that promises the best performance at the lowest cost, turning what could be an exhaustive and lengthy process into a swift and streamlined operation.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403204838309.png" alt="image-20240403204838309" style="zoom: 33%;" /> 

```
å½“æˆ‘ä»¬åœ¨å¹³å°ä¸Šè¾“å…¥å‘½ä»¤æ—¶ï¼ŒSQL æŸ¥è¯¢ä¹‹æ—…çš„ç¬¬ä¸€é˜¶æ®µå°±å¼€å§‹äº†ï¼Œå®ƒå°†ç›´æ¥è¿›å…¥æŸ¥è¯¢è§£æå™¨çš„æ€€æŠ±ã€‚ä¸å…¶ä»–è¿›è¡Œè¯­æ³•æ£€æŸ¥çš„ç¼–ç¨‹è¯­è¨€ä¸€æ ·ï¼ŒæŸ¥è¯¢è§£æå™¨ä¹Ÿä¼šä»”ç»†æ£€æŸ¥æˆ‘ä»¬çš„æŸ¥è¯¢ï¼Œä»¥ç¡®è®¤å…¶æ­£ç¡®æ€§ã€‚å¦‚æœå‡ºç°ä»»ä½•é”™åˆ«å­—æˆ–æ··ä¹±ï¼Œæ¯”å¦‚ "SELECT FROM WHERE "åºåˆ—å‡ºé”™ï¼Œè§£æå™¨å°±ä¼šç«‹å³æŠ¥å‘Šè¯­æ³•é”™è¯¯ã€‚æ­¤å¤–ï¼Œè¿™ä¸€é˜¶æ®µå¹¶ä¸å±€é™äºå•çº¯çš„è§£æï¼›æ•°æ®åº“å¼•æ“è¿˜å¯èƒ½é‡å†™æˆ‘ä»¬çš„æŸ¥è¯¢ä»¥æé«˜æ€§èƒ½ã€‚è¿˜è®°å¾—æˆ‘ä»¬è®¨è®ºè¿‡çš„ "IN "å’Œ "NOT IN "å­å¥ä»¥åŠè¿æ¥çš„æ›¿ä»£ç”¨æ³•å—ï¼Ÿè™½ç„¶æ‚¨å¯èƒ½ä¼šä½¿ç”¨ "IN "æ¥ç¼–å†™æŸ¥è¯¢ï¼Œä½†è§£æå™¨å¯ä»¥æ™ºèƒ½åœ°ä½¿ç”¨è¿æ¥æ¥é‡å†™æŸ¥è¯¢ï¼Œä»è€Œæé«˜æ‰§è¡Œæ•ˆç‡ã€‚
The first stage in our SQL query's journey begins when we input our command on the platform, leading it directly into the arms of the query parser. Much like other programming languages that conduct syntax checks, the query parser scrutinizes our query to confirm its correctness. Should there be any typos or disorderâ€”like getting the sequence 'SELECT FROM WHERE' wrongâ€”the parser will promptly report a syntax error. Furthermore, this stage isn't limited to mere parsing; the database engine may also rewrite our queries to enhance performance. Recall our discussion about 'IN' and 'NOT IN' clauses, and the alternative use of joins? Although you might write a query using 'IN', the parser could intelligently rewrite it using a join for more efficient execution.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403204838309.png" alt="image-20240403204838309" style="zoom: 33%;" /> 

```
ç»è¿‡è§£æå’Œå¯èƒ½çš„é‡å†™åï¼Œä¿®æ”¹åçš„æŸ¥è¯¢å°†è¿›å…¥æŸ¥è¯¢ä¼˜åŒ–å™¨--åŸºæœ¬ä¸Šå°±æ˜¯æ•°æ®åº“çš„å¤§è„‘ï¼Œåœ¨è¿™é‡Œåšå‡ºå…³é”®å†³ç­–ã€‚ä¼˜åŒ–å™¨çš„ä»»åŠ¡æ˜¯æ£€æŸ¥æ‰§è¡ŒæŸ¥è¯¢çš„æ‰€æœ‰å¯è¡Œè®¡åˆ’ï¼Œå°†æŸ¥è¯¢è®¡åˆ’è§†ä¸ºä¸€ç³»åˆ—æ­¥éª¤ï¼Œæˆ–è€…è¯´æ˜¯å…³ç³»è¿ç®—ç¬¦çš„å †å ï¼Œä»¥å½¢æˆä¸€ä¸ªå®Œæ•´çš„ç­–ç•¥ã€‚æˆ‘ä»¬å°†åœ¨æ¥ä¸‹æ¥çš„è®²åº§ä¸­æ·±å…¥æ¢è®¨è¿™äº›è®¡åˆ’çš„å…·ä½“ç»†èŠ‚ã€‚ä¼˜åŒ–å™¨ä½¿ç”¨å…³ç³»ä»£æ•°åŸç†æ¥è¯„ä¼°å¤‡é€‰è®¡åˆ’ï¼Œå¹¶é€šè¿‡æˆæœ¬ä¼°ç®—æ¥ç¡®å®šæœ€ç»æµçš„æ–¹æ³•ã€‚ä¼˜åŒ–å™¨é€‰å‡ºæœ€ä¼˜è®¡åˆ’åï¼Œå°±ä¼šå°†å…¶è½¬å…¥æœ€åçš„æ‰§è¡Œé˜¶æ®µã€‚
Subsequent to parsing and potential rewriting, the modified query advances to the query optimizerâ€”essentially the database's brain, where crucial decisions are made. The optimizer's task is to examine all feasible plans for executing the query, envisioning a query plan as a series of steps or, if you will, a stack of relational operators assembled to form a complete strategy. We will delve deeper into the specifics of these plans in upcoming lectures. The optimizer uses the principles of relational algebra to evaluate alternative plans and employs cost estimates to determine the most economical approach. Once the optimizer elects the optimal plan, it forwards it to the final phase of execution.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403204838309.png" alt="image-20240403204838309" style="zoom: 33%;" /> 

```
é€‰å®šçš„æŸ¥è¯¢è®¡åˆ’å°†äº¤ç»™æ‰€è°“çš„æŸ¥è¯¢è®¡åˆ’è¯„ä¼°å™¨æˆ–æ‰§è¡Œå™¨--æˆ‘å°†äº¤æ›¿ä½¿ç”¨è¿™äº›æœ¯è¯­ï¼Œä»¥ä¾¿æ‚¨ç†Ÿæ‚‰ä¸åŒæ–‡æœ¬ä¸­çš„ä¸åŒæœ¯è¯­ã€‚è¿™ä¸ªæ‰§è¡Œå™¨è´Ÿè´£æ‰§è¡Œè®¡åˆ’ï¼Œä»¥æœ€æœ‰æ•ˆçš„æ–¹å¼æ‰§è¡Œæ¯ä¸ªæ­¥éª¤å’Œå…³ç³»è¿ç®—ç¬¦--æ— è®ºæ˜¯é€‰æ‹©ã€æŠ•å½±è¿˜æ˜¯è¿æ¥ã€‚åœ¨ä»Šå¤©çš„è®²åº§ä¸­ï¼Œæˆ‘ä»¬å°†ä»æ‰§è¡Œçš„è§’åº¦é˜æ˜ "æœ€ä½³æ–¹å¼ "çš„å«ä¹‰ï¼Œç¡®ä¿æˆ‘ä»¬ç†è§£æ•°æ®åº“å¦‚ä½•å°†æˆ‘ä»¬çš„æŸ¥è¯¢è½¬åŒ–ä¸ºæ—¢è¿…é€Ÿåˆç²¾ç¡®çš„æ“ä½œã€‚
The chosen query plan is handed off to what's known as the query plan evaluator, or executorâ€”terms I'll use interchangeably to acquaint you with the varying nomenclature found across different texts. This executor is responsible for ğŸš—rying out the plan, executing each step and relational operatorâ€”be it selection, projection, or joinâ€”in the most effective manner. In today's lecture, we'll clarify what "the best possible way" entails from an execution standpoint, ensuring that we understand how a database translates our queries into actions that are both swift and precise.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403222424752.png" alt="image-20240403222424752" style="zoom: 33%;" /> 

**1ï¸âƒ£**We will consider how to implement:

1. Selection(Ïƒ) Selects a subset of rows from relation

2. Projection(Ï€) Deletes unwanted columns from relation

3. Join($\bowtie$â€‹) Allows us to combine two relations

**2ï¸âƒ£**Operators can be then be composed creating query plans

```
åœ¨æœ¬è¯¾ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹è®¨è®ºå¦‚ä½•é«˜æ•ˆåœ°å®ç°ä¸‰ç§åŸºæœ¬æ•°æ®åº“æ“ä½œï¼šé€‰æ‹©ã€æŠ•å½±å’Œè¿æ¥ã€‚è™½ç„¶è¿˜æœ‰å…¶ä»–æ“ä½œï¼ˆå¦‚èšåˆï¼‰ï¼Œä½†æˆ‘ä»¬é›†ä¸­è®¨è®ºçš„è¿™ä¸‰ä¸ªæ“ä½œä¸ä»…æ˜¯æœ€å¸¸è§çš„ï¼Œä¹Ÿæ˜¯æœ€è€—è´¹èµ„æºçš„ã€‚åœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œä¼˜åŒ–å¯ä»¥äº§ç”Ÿé‡å¤§å½±å“ï¼Œå› æ­¤ä¹Ÿå¸å¼•äº†å¤§é‡å…³æ³¨ã€‚æˆ‘ä»¬å°†æ¢è®¨è¿™äº›æ“ä½œç¬¦çš„æ‰§è¡Œç­–ç•¥ã€‚è¯·è®°ä½ï¼Œè¿™äº›æ˜¯æˆ‘ä»¬å°†ä»¥å„ç§æ–¹å¼ç»„åˆèµ·æ¥æ„å»ºç»¼åˆæŸ¥è¯¢è®¡åˆ’çš„æ„ä»¶ã€‚
In this course, we're going to focus on the efficient implementation of three fundamental database operations: selection, projection, and join. While there are additional operators, like aggregation, the three we are concentrating on are not only the most common, but also the most resource-intensive. These are the areas where optimization can have a significant impact, and consequently, they attract considerable attention. We will explore the execution strategies for these operators. Remember, these are the building blocks that we will combine in various ways to construct a comprehensive query plan.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403205202228.png" alt="image-20240403205202228" style="zoom:33%;" /> 

```
å¯¹äºé‚£äº›å¸Œæœ›æ·±å…¥ç ”ç©¶é€‰æ‹©çš„äººæ¥è¯´ï¼Œæˆ‘ä»¬çš„æ•°æ®åº“ç®¡ç†æ•™ç§‘ä¹¦ç¬¬ 14 ç« æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ„æºã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œè¿™æœ¬æ•™ç§‘ä¹¦å¹¶ä¸æ˜¯æœ¬è¯¾ç¨‹çš„å¿…ä¿®è¯¾ã€‚å®ƒæ‰€æ¶µç›–çš„ä¸»é¢˜æ¯”æˆ‘ä»¬å½“å‰å­¦ä¹ æ‰€éœ€çš„å†…å®¹æ›´åŠ è¯¦ç»†ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå¦‚æœæ‚¨çš„å¥½å¥‡å¿ƒè¢«æ¿€å‘å‡ºæ¥ï¼Œæˆ–è€…æ‚¨å¯¹è¿™äº›ä¸»é¢˜æœ‰æµ“åšçš„å…´è¶£å¹¶å¸Œæœ›åŠ æ·±ç†è§£ï¼Œæˆ‘å¼ºçƒˆå»ºè®®æ‚¨å‚è€ƒè¿™äº›ç« èŠ‚ã€‚å®ƒä»¬èƒ½è®©ä½ æ›´é€å½»åœ°ç†è§£æˆ‘ä»¬æ­£åœ¨è®¨è®ºçš„ææ–™ã€‚
For those of you looking to delve deeper into selections, Chapter 14 of our database management textbook is an excellent resource. It's worth mentioning, though, that the textbook isn't compulsory for this course. It does cover topics in greater detail than what we'll need for our current studies. Nonetheless, if your curiosity is piqued or if you have a keen interest in these subjects and wish to broaden your understanding, I highly encourage you to consult these chapters. They can provide you with a more thorough comprehension of the material we're discussing.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403222903025.png" alt="image-20240403222903025" style="zoom:33%;" /> 

Sailors (sid: integer, sname: string, rating: integer, age: real)
Reserves (sid: integer, bid: integer, day: dates, rname: string)

**1ï¸âƒ£**Sailors (S):

1. Each tuple is 50 bytes long, 80 tuples per page, 500 pages
2. N = NPages(S) = 500, $p_S$â€‹=NTuplesPerPage(S) = 80
3. NTuples(S) = 500*80 = 40000

**2ï¸âƒ£**Reserves (R):

1. Each tuple is 40 bytes long, 100 tuples per page, 1000 pages
2. M= NPages(R) = 1000, $p_R$â€‹=NTuplesPerPage(R) =100
3. NTuples(R) = 100000

```
åœ¨æœ¬è®²åº§ä¸­ï¼Œæˆ‘ä»¬å°†é‡æ¸©æˆ‘ä»¬ç†Ÿæ‚‰çš„ "æ°´æ‰‹ "å’Œ "å‚¨å¤‡ "ç¤ºä¾‹ã€‚ä¸ºäº†å¼ºåŒ–æˆ‘ä»¬çš„ç¤ºä¾‹ï¼Œè®©æˆ‘ä»¬è€ƒè™‘ä¸€äº›å…·ä½“ç»†èŠ‚ï¼š"æ°´æ‰‹ "è¡¨æ¯é¡µåŒ…å« 80 ä¸ªå›¾å…ƒï¼Œå…± 500 é¡µã€‚è¿™ç›¸å½“äºåœ¨ "æ°´æ‰‹ "å…³ç³»ä¸­åŒ…å«äº† 40,000 ä¸ªå›¾å…ƒã€‚åŒæ—¶ï¼Œ"å‚¨å¤‡ "è¡¨æ¯é¡µåŒ…å« 100 ä¸ªå›¾å…ƒï¼Œåˆ†å¸ƒåœ¨ 1,000 é¡µä¸­ï¼Œæ€»è®¡ 100,000 ä¸ªå›¾å…ƒã€‚åœ¨æˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢æ•°æ®åº“ç¤ºä¾‹æ—¶ï¼Œè¿™äº›æ•°å­—å°†éå¸¸æœ‰ç”¨ã€‚
During this lecture, we'll revisit our familiar examples involving 'sailors' and 'reserves'. To enhance our example, let's consider some specific details: the 'sailors' table contains 80 tuples per page, across a total of 500 pages. This amounts to 40,000 tuples within the 'sailors' relation. Meanwhile, the 'reserves' table holds 100 tuples per page, spread over 1,000 pages, which sums up to 100,000 tuples in total. These figures will be instrumental as we explore our database examples further.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403225646438.png" alt="image-20240403225646438" style="zoom:33%;" /> 

**1ï¸âƒ£**Of the form: $\sigma{_\text{R.attr op value}}(R)$

**2ï¸âƒ£**E.g. 

```sql
SELECT * FROM Reserves R WHERE R.BID > 20;
```

**3ï¸âƒ£**The best way to perform a selection depends on:

1. available indexes/access paths
2. expected size of the result (number of tuples and/or number of pages)

```
å…³ç³»ä»£æ•°ä¸­çš„ç®€å•é€‰æ‹©ç”¨å¸Œè…Šç¬¦å· Sigma (Ïƒ) è¡¨ç¤ºï¼Œåé¢æ˜¯åº”ç”¨äºè¡¨çš„æ¡ä»¶ã€‚åœ¨ SQL ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ SELECT è¯­å¥æ¥é˜è¿°è¿™ä¸€æ¦‚å¿µï¼Œè¯¥è¯­å¥ç”¨äºæŒ‡å®šè¡¨ä¸­çš„åˆ—ï¼ŒåŒæ—¶ä½¿ç”¨ WHERE å­å¥æ¥æ ¹æ®ç‰¹å®šæ¡ä»¶è¿‡æ»¤ç»“æœã€‚æ‰§è¡Œé€‰æ‹©æ“ä½œçš„æ•ˆç‡å–å†³äºå¯ç”¨çš„ç´¢å¼•æˆ–è®¿é—®è·¯å¾„--å®ƒä»¬å¯ä»¥æ˜¯å †æ–‡ä»¶ã€æ’åºæ–‡ä»¶æˆ–å„ç§ç´¢å¼•ç»“æ„ï¼Œå¦‚èšç°‡ã€éèšç°‡ã€æ•£åˆ—æˆ–åŸºäºæ ‘çš„ç´¢å¼•ã€‚
Simple selections in relational algebra are denoted by the Greek symbol Sigma (Ïƒ), followed by a condition applied to a table. In SQL, we articulate this concept using the SELECT statement to specify columns from a table, accompanied by a WHERE clause to filter the results based on a certain condition. The efficiency of executing a selection operation hinges on the available indexes or access pathsâ€”these could be heap files, sorted files, or various index structures like clustered, non-clustered, hash, or tree-based indexes.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403225646438.png" alt="image-20240403225646438" style="zoom:33%;" /> 

**1ï¸âƒ£**Of the form: $\sigma{_\text{R.attr op value}}(R)$

**2ï¸âƒ£**E.g. 

```sql
SELECT * FROM Reserves R WHERE R.BID > 20;
```

**3ï¸âƒ£**The best way to perform a selection depends on:

1. available indexes/access paths
2. expected size of the result (number of tuples and/or number of pages)

```
æ‰§è¡Œé€‰æ‹©çš„æœ€ä½³ç­–ç•¥ä¸ä»…å–å†³äºå¯ç”¨çš„ç´¢å¼•ï¼Œè¿˜å–å†³äºç»“æœçš„é¢„æœŸå¤§å°ï¼Œå³æŸ¥è¯¢å°†è¿”å›çš„å…ƒç»„æ•°é‡ã€‚è¿™ç§é¢„æœŸå¤§å°ä¼šå½±å“æ“ä½œçš„æˆæœ¬ï¼Œå› ä¸ºè®¿é—®å•ä¸ªå›¾å…ƒä¸è®¿é—®æ•°ç™¾æˆ–æ•°ç™¾ä¸‡ä¸ªå›¾å…ƒæ‰€äº§ç”Ÿçš„è´¹ç”¨å¤§ä¸ç›¸åŒã€‚è¿™å°±æ˜¯ä¸€ä¸ªç±»ä¼¼äºé¸¡å’Œè›‹çš„ä¸¤éš¾é—®é¢˜ï¼šç†æƒ³æƒ…å†µä¸‹ï¼Œå¦‚æœæˆ‘ä»¬äº‹å…ˆçŸ¥é“ç»“æœçš„å¤§å°ï¼Œå°±å¯ä»¥é€‰æ‹©æœ€æœ‰æ•ˆçš„æ‰§è¡Œæ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›ä¿¡æ¯åªæœ‰åœ¨æŸ¥è¯¢è¿è¡Œåæ‰ä¼šæ˜¾ç¤ºå‡ºæ¥ï¼Œå¯¹äºä¼˜åŒ–æ¥è¯´ä¸ºæ—¶å·²æ™šã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ•°æ®åº“ä½¿ç”¨ä¼°ç®—æŠ€æœ¯æ¥è¿‘ä¼¼ä¼°è®¡ç»“æœå¤§å°å’Œæ¯ä¸ªæ“ä½œçš„ç›¸å…³æˆæœ¬ã€‚åœ¨æœ¬è®²åº§å’Œæ¥ä¸‹æ¥çš„è®²åº§ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨è¿™äº›ä¼°ç®—å¦‚ä½•æŒ‡å¯¼é€‰æ‹©æŸ¥è¯¢çš„æœ€ä½³æ‰§è¡Œç­–ç•¥ã€‚
The optimal strategy for executing a selection is not only determined by the indexes available but also by the expected size of the resultâ€”essentially, the number of tuples the query will return. This expected size affects the operation's cost, as accessing a single tuple versus hundreds or millions will incur vastly different expenses. Herein lies a dilemma akin to the chicken and egg scenario: ideally, if we knew the result size in advance, we could choose the most efficient execution method. However, this information is only revealed after the query runs, which is too late for optimization. To tackle this, databases use estimation techniques to approximate the result size and the associated cost for each operation. Over the course of this lecture and the following ones, we'll explore how these estimations guide the selection of the best execution strategy for a query.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403230955369.png" alt="image-20240403230955369" style="zoom:33%;" /> 

**1ï¸âƒ£**Size of result approximated as: 
$$
\text{size of relation * Î (reductionfactors)}
$$
**2ï¸âƒ£**Reduction factor is usually called **selectivity**. It estimates what portion of the relation will qualify for the given predicate, i.e. satisfy the given condition.

1. This is estimated by the optimizer (will be taught next week)
2. E.g. 30% of records qualify, or 5% of records qualify

```
è®©æˆ‘ä»¬æ·±å…¥æ¢è®¨ä¸€ä¸‹å¦‚ä½•ä¼°ç®—å•ä¸ªè¡¨ä¸Šé€‰æ‹©æ“ä½œçš„ç»“æœå¤§å°ã€‚æˆ‘ä»¬å¯ä»¥ç”¨æ•´ä¸ªå…³ç³»çš„å¤§å°ä¹˜ä»¥æ‰€è°“çš„è¿˜åŸå› å­çš„ä¹˜ç§¯æ¥è¿‘ä¼¼ä¼°ç®—ç»“æœå¤§å°ã€‚ç¼©å‡å› å­ä¹Ÿç§°ä¸ºé€‰æ‹©æ€§å› å­ï¼Œç”¨äºä¼°ç®—ç¬¦åˆç»™å®šæ¡ä»¶çš„è¡¨çš„æ¯”ä¾‹ã€‚è¿™å°±ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªåˆç†çš„è¿‘ä¼¼å€¼ï¼Œå³æœ‰å¤šå°‘å›¾å…ƒå°†ç¬¦åˆæ¡ä»¶å¹¶åŒ…å«åœ¨æœ€ç»ˆç»“æœé›†ä¸­ã€‚
Let's delve into how we can estimate the result size of a selection operation on a single table. The result size can be approximated by taking the size of the entire relation and multiplying it by the product of what we call reduction factors. Reduction factors, also known as selectivity factors, estimate the fraction of the table that meets a given condition. This gives us a reasonable approximation of how many tuples will qualify and be included in the final result set.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403225646438.png" alt="image-20240403225646438" style="zoom:33%;" /> 

**1ï¸âƒ£**Of the form: $\sigma{_\text{R.attr op value}}(R)$

**2ï¸âƒ£**E.g. 

```sql
SELECT * FROM Reserves R WHERE R.BID > 20;
```

**3ï¸âƒ£**The best way to perform a selection depends on:

1. available indexes/access paths
2. expected size of the result (number of tuples and/or number of pages)

```
åœ¨æˆ‘ä»¬çš„ WHERE å­å¥ä¸­--å®ƒä»£è¡¨ SQL ä¸­çš„ä¸€ä¸ªé€‰æ‹©--æ•°æ®åº“å°†ä¼°ç®—æ¯ä¸ªè°“è¯çš„ç¼©å‡å› å­ã€‚ä»æ ¹æœ¬ä¸Šè¯´ï¼Œè¿™å°±æ˜¯é¢„æµ‹å“ªéƒ¨åˆ†æ•°æ®å°†æ»¡è¶³æŒ‡å®šæ¡ä»¶ï¼Œä»è€Œé€šè¿‡æ“ä½œè¾“å‡ºã€‚è¿™ç§ä¼°è®¡æ˜¯äº†è§£æŸ¥è¯¢å°†è¿”å›å¤šå°‘å›¾å…ƒçš„æ ¸å¿ƒã€‚
In the context of our WHERE clauseâ€”which represents a selection in SQLâ€”the database will estimate the reduction factor for each predicate present. This is fundamentally about predicting what portion of the data will satisfy the specified conditions and thus be output by the operation. This estimation is at the core of understanding how many tuples will be returned by the query.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403230955369.png" alt="image-20240403230955369" style="zoom:33%;" /> 

**1ï¸âƒ£**Size of result approximated as: 
$$
\text{size of relation * Î (reductionfactors)}
$$
**2ï¸âƒ£**Reduction factor is usually called **selectivity**. It estimates what portion of the relation will qualify for the given predicate, i.e. satisfy the given condition.

1. This is estimated by the optimizer (will be taught next week)
2. E.g. 30% of records qualify, or 5% of records qualify

```
æˆ‘åœ¨è¿™é‡Œè§£é‡Šä¸€ä¸‹ä¼°ç®—ç¼©å‡ç³»æ•°çš„è¿‡ç¨‹ï¼Œä¸€æ—¦æŒæ¡äº†çªé—¨ï¼Œè¿™ä¸ªè¿‡ç¨‹å°±ä¼šéå¸¸ç›´è§‚ã€‚ä»Šå¤©ï¼Œæˆ‘å°†å¼•å¯¼ä½ å»ºç«‹è¿™ç§ç›´è§‰ï¼Œä»¥ä¾¿ä½ èƒ½è‡ªå¦‚åœ°ä½¿ç”¨è¿™äº›æ¦‚å¿µå’Œå…¬å¼ã€‚è®©æˆ‘ä»¬ä»¥ "æ°´æ‰‹ "è¡¨ä¸ºä¾‹ï¼ŒæŸ¥æ‰¾æ’åä¸º 8 çš„æ°´æ‰‹ã€‚å¦‚æœæ’åå±æ€§å¯ä»¥å– 1 åˆ° 10 ä¹‹é—´çš„ä»»ä½•å€¼ï¼Œé‚£ä¹ˆå°±æœ‰ 10 ä¸ªå¯èƒ½çš„æ’åå€¼ã€‚ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬ä¸“é—¨æœç´¢æ’åä¸º 8 çš„æ°´æ‰‹ï¼Œé‚£ä¹ˆè¿™ä¸ªè°“è¯çš„ç¼©å‡å› å­å°±æ˜¯ 1/10ï¼Œå³ 0.1ï¼Œå› ä¸ºåœ¨ 10 ä¸ªå¯èƒ½çš„æ’åå€¼ä¸­ï¼Œæˆ‘ä»¬åªå¯¹å…¶ä¸­ä¸€ä¸ªæ„Ÿå…´è¶£ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬ä¼°è®¡è¯¥é€‰æ‹©å°†è¿”å›ååˆ†ä¹‹ä¸€çš„æ•°æ®ã€‚éšç€ç ”ç©¶çš„æ·±å…¥ï¼Œæˆ‘ä»¬å°†å¯¹å…¶ä»–è°“è¯åº”ç”¨ç±»ä¼¼çš„é€»è¾‘ã€‚
I'm here to explain the process of estimating reduction factors, which is quite intuitive once you get the hang of it. I'll guide you through developing this intuition today so that you'll be comfortable using these concepts and formulas going forward. Let's take an example from our 'sailors' table where we're looking for sailors with a ranking of eight. If the ranking attribute can take on any value between one and ten, there are ten possible values for ranking. Now, if we're searching specifically for a ranking of eight, the reduction factor for this predicate would be 1/10, or 0.1, because out of the ten possible ranking values, we're only interested in one. This means we estimate that the selection will return a tenth of the data. We'll apply similar logic to other predicates as we progress in our studies.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403232037091.png" alt="image-20240403232037091" style="zoom:33%;" /> 

**1ï¸âƒ£**With no index, unsorted:

1. Must scan the whole relation, i.e. perform Heap Scan
2. **Cost = Number of Pages of Relation, i.e. NPages(R)**
3. **Example**: Reserves cost(R)= 1000 IO (1000 pages)

```
åœ¨æ•°æ®å­˜å‚¨åœ¨å †ä¸­ä¸”æ²¡æœ‰ç´¢å¼•çš„æƒ…å†µä¸‹ï¼Œé€‰æ‹©æ“ä½œå¿…é¡»æ‰«ææ•´ä¸ªå…³ç³»ï¼Œä¹Ÿç§°ä¸ºå †æ‰«æã€‚è¿™æ˜¯å› ä¸ºï¼Œåœ¨æ²¡æœ‰ç´¢å¼•å’Œæ•°æ®æœªæ’åºçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¿…é¡»éå†æ¯ä¸€é¡µï¼Œä»¥ç¡®ä¿æ²¡æœ‰æ½œåœ¨çš„åŒ¹é…å…³ç³»è¢«å¿½ç•¥ã€‚è¿™ç§å †æ‰«æçš„æˆæœ¬ç›¸å½“äºå…³ç³»ä¸­çš„é¡µæ•°ï¼Œå› ä¸ºæ¯ä¸€é¡µéƒ½éœ€è¦æ£€æŸ¥ã€‚
In situations where our data is stored in a heap and we don't have an index, a selection operation necessitates scanning the entire relation, also known as a heap scan. This is because, without an index and with data that is unsorted, we must traverse every page to ensure no potential match is overlooked. The cost of such a heap scan equates to the number of pages in the relation since every page needs to be inspected.
```

**2ï¸âƒ£**With no index, but file is sorted:

1. cost **= binary search cost + number of pages containing results**
2. **Cost = log2(NPages(R))** + (==RF==*NPages(R))
3. **Example**: Reserves cost(R)= 10 I/O + (==RF==*NPages(R))

```
åœ¨æ²¡æœ‰ç´¢å¼•çš„æƒ…å†µä¸‹å¤„ç†æ’åºæ•°æ®æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨æ•°æ®çš„æ’åºç‰¹æ€§æ¥æ‰§è¡ŒäºŒè¿›åˆ¶æœç´¢ã€‚è¿™ç§æ–¹æ³•å¤§å¤§ç¼©å°äº†æœç´¢ç©ºé—´ï¼Œä½¿æˆ‘ä»¬åªéœ€å…³æ³¨æœ‰å¯èƒ½æ‰¾åˆ°åˆæ ¼è®°å½•çš„éƒ¨åˆ†ã€‚è¿™é‡Œçš„æˆæœ¬åŒ…æ‹¬å®šä½åˆå§‹åˆæ ¼è®°å½•çš„äºŒè¿›åˆ¶æœç´¢ï¼Œä»¥åŠåŒ…å«åç»­ç»“æœçš„é¡µæ•°ã€‚æ‰¾åˆ°ç¬¬ä¸€ä¸ªç¬¦åˆæ¡ä»¶çš„å…ƒç»„åï¼Œæˆ‘ä»¬å°±å¯ä»¥é«˜æ•ˆåœ°éå†å·²æ’åºçš„æ•°æ®ï¼Œæ”¶é›†å…¶ä½™ç»“æœã€‚
When dealing with sorted data in the absence of an index, we can leverage the sorted nature of the data to perform a binary search. This approach significantly narrows down the search space, allowing us to focus only on the part where a qualifying record is likely to be found. The cost here includes the binary search to locate the initial qualifying record plus the number of pages that contain the subsequent results. After finding the first qualifying tuple, we can efficiently traverse through the sorted data to gather the rest of the results.
```

**3ï¸âƒ£**With an index on selection attribute

1. Use index to find qualifying data entries,
2. Then retrieve corresponding data records
3. Discussed nextâ€¦.

```
åœ¨é€‰æ‹©å±æ€§ä¸Šå»ºç«‹ç´¢å¼•ä¼šæ”¹å˜æ¸¸æˆè§„åˆ™ã€‚æˆ‘ä»¬ä½¿ç”¨ç´¢å¼•å°†ç¬¦åˆæ¡ä»¶çš„æ•°æ®é¡¹å½’é›¶ï¼Œç„¶åä»è¡¨ä¸­è·å–ç›¸åº”çš„æ•°æ®è®°å½•ã€‚ç´¢å¼•é€‰æ‹©æä¾›äº†ä¸€ç§æ›´æœ‰é’ˆå¯¹æ€§çš„æ–¹æ³•ï¼Œå‡å°‘äº†æˆ‘ä»¬éœ€è¦è®¿é—®çš„é¡µé¢æ•°é‡ã€‚æˆ‘ä»¬å°†åœ¨æ¥ä¸‹æ¥çš„è®²åº§ä¸­è¿›ä¸€æ­¥æ¢è®¨è¿™ç§æƒ…å†µã€‚
Having an index on a selection attribute changes the game. We use the index to zero in on the qualifying data entries and then fetch the corresponding data records from the table. The indexed selection offers a more targeted approach, reducing the number of pages we need to access. We'll explore this scenario further in the upcoming sections of our lecture.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403232702772.png" alt="image-20240403232702772" style="zoom: 50%;" /> 

```
è®©æˆ‘ä»¬æ¥çœ‹çœ‹è¿™æ ·ä¸€ç§æƒ…å†µï¼šæˆ‘ä»¬æœ‰ä¸€ä¸ªæ’åºè¿‡çš„æ•°æ®é›†ï¼Œå…±æœ‰ 8 é¡µï¼Œè®°å½•æŒ‰ç‰¹å®šå±æ€§ï¼ˆæ¯”å¦‚å¹´é¾„ï¼‰æ’åºã€‚å¦‚æœæˆ‘ä»¬è¦æœç´¢å¹´é¾„å¤§äº 20 å²çš„è®°å½•ï¼Œæˆ‘ä»¬é¦–å…ˆè¦è¿›è¡ŒäºŒè¿›åˆ¶æœç´¢ï¼Œæ‰¾åˆ°åŒ…å«ç¬¦åˆæ¡ä»¶è®°å½•çš„ç¬¬ä¸€é¡µã€‚å‡è®¾æœç´¢ç»“æœæ˜¾ç¤ºï¼Œç¬¦åˆæ¡ä»¶çš„è®°å½•æ˜¯ä»æ ‡æœ‰ "20 "çš„é¡µé¢å¼€å§‹çš„ã€‚æˆ‘ä»¬å°±å¯ä»¥ç¡®å®šè¿™éƒ¨åˆ†æ“ä½œçš„æˆæœ¬ä¸ºé¡µæ•°çš„å¯¹æ•°åŸº 2ï¼Œå¯¹äº 8 é¡µæ¥è¯´å°±æ˜¯ log_2(8)ã€‚
Let's examine a scenario where we have a sorted dataset across eight pages, with records ordered by a particular attributeâ€”say, age. If we're searching for records where the age is greater than 20, we'd start with a binary search to locate the first page containing the qualifying records. Suppose the search indicates that records meeting our criteria begin on a page labeled with a '20'. We then determine the cost of this part of the operation as the logarithm base 2 of the number of pages, which for eight pages would be log_2(8).
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403232702772.png" alt="image-20240403232702772" style="zoom: 50%;" /> 

```
ä¸€æ—¦æˆ‘ä»¬æ‰¾åˆ°äº†æ‰€éœ€è®°å½•å¼€å§‹çš„é¡µé¢ï¼Œæˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨ç¼©å‡å› å­æ¥ä¼°ç®—é¢„è®¡èƒ½æ£€ç´¢åˆ°çš„æ•°æ®é‡ã€‚å¦‚æœç¼©å‡å› å­è¡¨æ˜ 25% çš„æ•°æ®ï¼ˆæˆ– 0.25 çš„å› å­ï¼‰å°†æ»¡è¶³æ¡ä»¶ï¼Œæˆ‘ä»¬å°±å¯ä»¥å°†å…¶ä¹˜ä»¥æ€»é¡µæ•°ï¼Œä»è€Œå¾—å‡ºæœ‰å¤šå°‘é¡µå°†åŒ…å«ç»“æœã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œ0.25 ä¹˜ä»¥ 8 é¡µç­‰äº 2 é¡µç»“æœã€‚ä¹‹æ‰€ä»¥èƒ½å‡†ç¡®é¢„æµ‹ï¼Œæ˜¯å› ä¸ºæ•°æ®å·²ç»æ’åºï¼›ä¸‹é¢ 25% çš„è®°å½•å°†æ˜¯æˆ‘ä»¬æƒ³è¦çš„ã€‚ç”±äºæ’åºé¡ºåºçš„åŸå› ï¼Œå€¼ "20 "ä¸å¯èƒ½å‡ºç°åœ¨è¿™ä¸€æ®µä¹‹å‰æˆ–ä¹‹åã€‚æ•°æ®åº“ä½¿ç”¨æˆæœ¬å…¬å¼æ¥ä¼°ç®—æ‰§è¡Œæ­¤ç±»é€‰æ‹©æ“ä½œæ‰€éœ€çš„å·¥ä½œé‡ï¼Œå°±æ˜¯åŸºäºè¿™ä¸€åŸåˆ™ã€‚
Once we've found the page where our desired records begin, we'll use the reduction factor to estimate the portion of data we expect to retrieve. If our reduction factor suggests that 25% of the dataâ€”or a factor of 0.25â€”will meet the condition, we multiply this by the total number of pages to find out how many pages will contain the results. In our case, 0.25 times eight pages equals two pages of results. This is possible to predict with accuracy because the data is sorted; the following 25% of records will be the ones we want. There's no chance of the value '20' appearing before or after this segment because of the sort order. This principle underlies the cost formula that databases use to estimate the effort required to perform such a selection operation.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

 <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403232037091.png" alt="image-20240403232037091" style="zoom:33%;" />

**1ï¸âƒ£**With no index, unsorted:

1. Must scan the whole relation, i.e. perform Heap Scan
2. **Cost = Number of Pages of Relation, i.e. NPages(R)**
3. **Example**: Reserves cost(R)= 1000 IO (1000 pages)

**2ï¸âƒ£**With no index, but file is sorted:

1. cost **= binary search cost + number of pages containing results**
2. **Cost = log2(NPages(R))** + (==RF==*NPages(R))
3. **Example**: Reserves cost(R)= 10 I/O + (==RF==*NPages(R))

**3ï¸âƒ£**With an index on selection attribute

1. Use index to find qualifying data entries,
2. Then retrieve corresponding data records
3. Discussed nextâ€¦.

```
å½“æˆ‘ä»¬æœ‰äº†ç´¢å¼•ï¼Œæµç¨‹å°±ä¼šå‘ç”Ÿå¾ˆå¤§çš„å˜åŒ–ã€‚æˆ‘ä»¬åˆ©ç”¨ç´¢å¼•è¿…é€Ÿæ‰¾å‡ºç¬¦åˆæ ‡å‡†çš„ç¬¬ä¸€æ¡è®°å½•ã€‚é€šè¿‡ç´¢å¼•ç¡®å®šç¬¦åˆæ¡ä»¶çš„è®°å½•åï¼Œæˆ‘ä»¬å†æ£€ç´¢å †ä¸­å­˜å‚¨çš„ç›¸åº”æ•°æ®ã€‚è¿™ä¸€æ­¥éª¤ä¸æˆ‘ä»¬ä¹‹å‰å…³äºèšç±»å’Œéèšç±»ç´¢å¼•çš„è®¨è®ºå¦‚å‡ºä¸€è¾™ï¼Œå®ƒä»¬åœ¨å†³å®šæˆ‘ä»¬å¦‚ä½•å¿«é€Ÿé«˜æ•ˆåœ°è®¿é—®æ‰€éœ€æ•°æ®æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚
When we have indexes available, the process changes significantly. We use the index to swiftly pinpoint the first record that meets our criteria. After identifying the qualifying records through the index, we then retrieve the corresponding data stored in the heap. This step echoes our previous discussion on clustered and non-clustered indices, which play a pivotal role in determining how quickly and efficiently we can access the desired data.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403233144289.png" alt="image-20240403233144289" style="zoom:33%;" /> 

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403233208963.png" alt="image-20240403233208963" style="zoom:50%;" /> 

```
ä»¥ä¸‹æ˜¯å¯¹æœç´¢å¦‚ä½•ä½¿ç”¨ç´¢å¼•è¿›è¡Œæ“ä½œçš„å¿«é€Ÿå›é¡¾ï¼Œå¯ä»¥ä½œä¸ºå›é¡¾ã€‚ æˆ‘ä»¬ä»ç´¢å¼•ç»“æ„çš„æ ¹å¼€å§‹æœç´¢ï¼Œç„¶åå‘ä¸‹éå†åˆ°å¶å­ï¼Œåœ¨å¶å­ä¸­æ‰¾åˆ°æ„Ÿå…´è¶£çš„æ•°æ®æ¡ç›®ã€‚ ä¸€æ—¦æ‰¾åˆ°ï¼Œè¿™äº›æ¡ç›®å°±ä¼šå¼•å¯¼æˆ‘ä»¬æ‰¾åˆ°æ•°æ®é¡µä¸Šå­˜å‚¨çš„å®é™…æ•°æ®ã€‚ ç´¢å¼•ä¸»è¦æœ‰ä¸¤ç§ç±»å‹ï¼šèšé›†ç´¢å¼•å’Œéèšé›†ç´¢å¼•ã€‚ åœ¨èšé›†ç´¢å¼•ä¸­ï¼Œæ•°æ®æ¡ç›®çš„é¡ºåºä¸æ•°æ®è®°å½•çš„ç‰©ç†é¡ºåºç›¸å¯¹åº”â€”â€”ä¸¤è€…çš„æ’åºæ–¹å¼ç±»ä¼¼ã€‚ ç›¸åï¼Œå¯¹äºéèšé›†ç´¢å¼•ï¼Œè™½ç„¶ç´¢å¼•ä¸­çš„æ•°æ®æ¡ç›®å·²æ’åºï¼Œä½†æ­¤é¡ºåºä¸å­˜å‚¨ä¸­æ•°æ®è®°å½•çš„ç‰©ç†é¡ºåºä¸åŒ¹é…ã€‚
Hereâ€™s a quick review of how searches operate with indexes, which might serve as a refresher. We initiate the search from the root of the index structure and traverse down to the leaves, where we find the data entries of interest. Once located, these entries lead us to the actual data stored on the data pages. There are two main types of indices: clustered and non-clustered. In a clustered index, the order of the data entries corresponds with the physical order of the data recordsâ€”both are sorted similarly. Conversely, with non-clustered indices, while the data entries in the index are sorted, this order does not match the physical order of the data records in storage.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403233800621.png" alt="image-20240403233800621" style="zoom:33%;" /> 

**1ï¸âƒ£**Cost depends on the number of qualifying tuples

**2ï¸âƒ£**Clustering is important when calculating the total cost

**3ï¸âƒ£**Steps to perform

1. Find qualifying data entries:
   - Go through the index: height typically small, 2-4 I/O in case of B+tree, 1.2 I/Oin case of hash index (negligibleif many records retrieved)
   - Once data entries are reached, go through data entries one by one and lookup corresponding data records (in the data file)
2. Retrieve data records (in the data file)

**4ï¸âƒ£**Cost:

1. Clusteredindex: **Cost = (NPages(I) +** ==NPages==**(R))*RF**
2. Unclusteredindex: **Cost = (NPages(I) +** ==NTuples==**(R))*RF**

```
ä½¿ç”¨ç´¢å¼•çš„æŸ¥è¯¢æˆæœ¬å–å†³äºç´¢å¼•æ˜¯å¦èšé›†ã€‚ æœç´¢è¿‡ç¨‹å¯ä»¥è¢«è®¤ä¸ºåŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µã€‚ æœ€åˆï¼Œæˆ‘ä»¬ä»æ ¹åˆ°å¶éå†ç´¢å¼•æ ‘æ¥å®šä½æ•°æ®æ¡ç›®â€”â€”è¿™äº›æ˜¯æŒ‡å‘åŒ…å«æŸ¥è¯¢ç»“æœçš„å…ƒç»„çš„æŒ‡é’ˆã€‚ ç„¶åï¼Œæˆ‘ä»¬ä»æ•°æ®æ–‡ä»¶ä¸­è·å–ç›¸åº”çš„è®°å½•ã€‚
The cost of a query utilizing an index depends on whether the index is clustered or not. The search process can be thought of as comprising two main phases. Initially, we traverse the index tree from the root to the leaves to locate the data entriesâ€”these are the pointers directing us to the tuples containing our query results. Then, we fetch the corresponding records from the data file.

å¯¹äºèšé›†ç´¢å¼•ï¼Œé€šè¿‡è€ƒè™‘ç´¢å¼•ä¸­çš„é¡µæ•°ï¼ˆç”¨â€œIâ€è¡¨ç¤ºï¼‰åŠ ä¸Šå…³ç³»ä¸­çš„é¡µæ•°ï¼ˆç”¨â€œRâ€è¡¨ç¤ºï¼‰æ¥ä¼°è®¡æˆæœ¬ï¼Œç„¶åå°†æ€»æ•°ä¹˜ä»¥å‡å°‘é‡ å› ç´ ã€‚ ç›¸åï¼Œåœ¨å¤„ç†éèšé›†ç´¢å¼•æ—¶ï¼Œæˆæœ¬æ˜¯é€šè¿‡ç´¢å¼•ä¸­çš„é¡µæ•°åŠ ä¸Šå…³ç³»ä¸­çš„å…ƒç»„æ•°æ¥è®¡ç®—çš„ - æ¯ä¸ªä¹˜ä»¥ç¼©å‡å› å­ä»¥åæ˜ é€‰æ‹©æ€§æœç´¢ã€‚
For a clustered index, the cost is estimated by considering the number of pages in the index (denoted by 'I') plus the number of pages in the relation (denoted by 'R'), with the total then being multiplied by the reduction factor. In contrast, when dealing with a non-clustered index, the cost is calculated by taking the number of pages in the index plus the number of tuples in the relationâ€”each multiplied by the reduction factor to reflect the selective search.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403233144289.png" alt="image-20240403233144289" style="zoom:33%;" /> 

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403233208963.png" alt="image-20240403233208963" style="zoom:50%;" />  

```
å¯¹äºå›¾å·¦ä¾§æè¿°çš„åœºæ™¯ï¼Œä½¿ç”¨èšé›†ç´¢å¼•ï¼Œæœç´¢æ“ä½œçš„æˆæœ¬é€šè¿‡ç´¢å¼•é¡µçš„æ•°é‡æ¥ä¼°è®¡ï¼Œå¹¶é€šè¿‡ç¼©å‡å› å­è¿›è¡Œè°ƒæ•´ã€‚ è¿™è¯´æ˜äº†æ•°æ®æ˜¯æœ‰åºçš„ï¼šä¸€æ—¦æ‰¾åˆ°ç¬¬ä¸€ä¸ªç›¸å…³è®°å½•ï¼Œæˆ‘ä»¬å°±å¯ä»¥æ¨æ–­åç»­è®°å½•æ˜¯ç›¸é‚»çš„å¹¶ä¸”å±äºæˆ‘ä»¬çš„æœç´¢ã€‚ åŒæ ·çš„åŸåˆ™ä¹Ÿé€‚ç”¨äºæ•°æ®è®°å½•æœ¬èº«ï¼Œå› ä¸ºå®ƒä»¬æ˜¯ä¸ç´¢å¼•ä¸€èµ·æ’åºçš„ã€‚ å¦‚æœæˆ‘ä»¬ä»¥æœç´¢ 3000 åå­¦ç”Ÿä¸ºä¾‹ï¼Œæ¯é¡µåŒ…å« 1000 æ¡è®°å½•ï¼Œé‚£ä¹ˆæˆ‘ä»¬åªéœ€è¦æ£€æŸ¥ç´¢å¼•ä¸­çš„ä¸‰é¡µå’Œæ•°æ®æ–‡ä»¶ä¸­çš„ä¸‰é¡µã€‚ æˆ‘ä»¬çš„æœç´¢éå¸¸é«˜æ•ˆï¼Œå› ä¸ºæˆ‘ä»¬ç¡®ä¿¡æ²¡æœ‰ä»»ä½•ç»“æœä¼šè¶…å‡ºè¿™ä¸ªè¿ç»­èŒƒå›´ã€‚
For the scenario depicted on the left side of the diagram, with a clustered index, the cost of a search operation is estimated by the number of index pages, adjusted by the reduction factor. This accounts for the fact that the data is ordered: once the first relevant record is located, we can infer that the subsequent records are adjacent and pertain to our search. The same principle applies to the data records themselves since they are sorted in tandem with the index. If we take the example where we are searching for 3000 students and each page holds a thousand records, we'd only need to examine three pages in the index and three pages in the data file. Our search is efficient because we're certain that no results will lie outside this contiguous range.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403233144289.png" alt="image-20240403233144289" style="zoom:33%;" /> 

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403233208963.png" alt="image-20240403233208963" style="zoom:50%;" />  

```
ç›¸åï¼Œå¯¹äºéèšé›†ç´¢å¼•ï¼Œå¦‚å³ä¾§æ‰€ç¤ºï¼Œè™½ç„¶ç´¢å¼•æœ¬èº«å·²æ’åºï¼Œä½†æ•°æ®è®°å½•å¹¶æœªæ’åºã€‚ æˆ‘ä»¬ä»ç„¶å¯¹ç´¢å¼•é¡µçš„æ•°é‡åº”ç”¨ç¼©å‡å› å­ï¼Œä½†å¯¹äºæ¯ä¸ªç¬¦åˆæ¡ä»¶çš„å…ƒç»„ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šé¢ä¸´é¢å¤–çš„ I/O æ“ä½œã€‚ è¿™æ˜¯å› ä¸ºä¸è¿™äº›ç´¢å¼•æ¡ç›®å¯¹åº”çš„è®°å½•å¯èƒ½åˆ†æ•£åœ¨å„ä¸ªé¡µé¢ä¸Šã€‚ å›æƒ³ä¸€ä¸‹æˆ‘ä»¬ä¸Šä¸€è¯¾ä¸­çš„ç¤ºä¾‹ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ 3000 åå­¦ç”Ÿï¼Œå¹¶ä¸”æ¯ä¸ªå­¦ç”Ÿéƒ½å¯èƒ½å­˜å‚¨åœ¨ä¸åŒçš„é¡µé¢ä¸Šï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯èƒ½éœ€è¦æ‰§è¡Œ 3000 ä¸ªå•ç‹¬çš„ I/O æ“ä½œæ¥è®¿é—®æ‰€æœ‰å¿…è¦çš„è®°å½•ã€‚
In contrast, with a non-clustered index, as shown on the right side, while the index itself is sorted, the data records are not. We still apply the reduction factor to the number of index pages, but for each qualifying tuple, we may face an additional I/O operation. This is due to the fact that the records corresponding to these index entries could be scattered across various pages. Recalling the example from our previous session, if we have 3000 students and each one is potentially stored on a different page, we may need to perform 3000 separate I/O operations to access all the necessary records.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403234426948.png" alt="image-20240403234426948" style="zoom:33%;" /> 

**1ï¸âƒ£**Example: Letâ€™s say that 10% of Reserves tuples qualify, and letâ€™s say that index occupies 50 pages

**2ï¸âƒ£**RF = 10% = 0.1, NPages(I) = 50, NPages(R) = 1000, NTuplesPerPage(R) = 100

**3ï¸âƒ£**Cost:

1. Clusteredindex:
   - Cost = (NPages(I) + NPages(R))*RF
   - Cost = (50+ 1000)*0.1 = ==105 (I/O)== (Cheapest access path)
2. Unclusteredindex:
   - Cost = (NPages(I) + NTuples(R))*RF
   - Cost = (50+ 100000)*0.1 = ==10005==(I/O)
3. Heap Scan:
   - Cost = NPages(R) = 1000 (I/O)

``` 
ä»¥æˆ‘ä»¬çš„â€œreservesâ€è¡¨ä¸ºä¾‹ï¼Œæˆ‘ä»¬å°†è®¡ç®—ä¸èšé›†ç´¢å¼•å’Œéèšé›†ç´¢å¼•ä»¥åŠå †æ‰«æç›¸å…³çš„æˆæœ¬ã€‚ å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå¯¼è‡´ç¼©å‡ç³»æ•°ä¸º 10% çš„è°“è¯ï¼Œå¹¶ä¸”æˆ‘ä»¬çš„ç´¢å¼•è·¨è¶Š 50 é¡µã€‚ å¯¹äºèšé›†ç´¢å¼•ï¼Œæˆæœ¬æ¶‰åŠç´¢å¼•ä¸­çš„é¡µæ•°ä¸å…³ç³»ä¸­çš„é¡µæ•°ä¹‹å’Œï¼Œç„¶åä¹˜ä»¥ç¼©å‡å› å­ã€‚ æ­¤è®¡ç®—äº§ç”Ÿ 105 ä¸ª I/O æ“ä½œã€‚ å¯¹äºéèšé›†ç´¢å¼•ï¼Œæˆæœ¬æ˜¯é€šè¿‡å°†ç´¢å¼•ä¸­çš„é¡µæ•°ä¸å…³ç³»ä¸­çš„å…ƒç»„æ€»æ•°ç›¸åŠ ï¼Œå†ä¹˜ä»¥ç¼©å‡å› å­æ¥ç¡®å®šçš„ï¼Œç»“æœæ˜¯ 10,005 æ¬¡ I/O æ“ä½œã€‚
Using our 'reserves' table as an example, we'll calculate the costs associated with both a clustered and an unclustered index, as well as a heap scan. Assume we have a predicate that results in a reduction factor of 10%, and our index spans 50 pages. For a clustered index, the cost involves the sum of the number of pages in the index and the number of pages in the relation, which is then multiplied by the reduction factor. This calculation yields 105 I/O operations. When it comes to an unclustered index, the cost is determined by adding the number of pages in the index to the total number of tuples in the relation, again multiplied by the reduction factor, resulting in 10,005 I/O operations.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403234426948.png" alt="image-20240403234426948" style="zoom:33%;" /> 

**1ï¸âƒ£**Example: Letâ€™s say that 10% of Reserves tuples qualify, and letâ€™s say that index occupies 50 pages

**2ï¸âƒ£**RF = 10% = 0.1, NPages(I) = 50, NPages(R) = 1000, NTuplesPerPage(R) = 100

**3ï¸âƒ£**Cost:

1. Clusteredindex:
   - Cost = (NPages(I) + NPages(R))*RF
   - Cost = (50+ 1000)*0.1 = ==105 (I/O)== (Cheapest access path)
2. Unclusteredindex:
   - Cost = (NPages(I) + NTuples(R))*RF
   - Cost = (50+ 100000)*0.1 = ==10005==(I/O)
3. Heap Scan:
   - Cost = NPages(R) = 1000 (I/O)

```
æ— è®ºæˆ‘ä»¬æ˜¯å¦æœ‰ç´¢å¼•ï¼Œå †æ‰«æå§‹ç»ˆæ˜¯ä¸€ä¸ªå¯ç”¨çš„é€‰é¡¹ï¼Œå…¶ä¸­é€šè¿‡æ‰«æå­˜å‚¨åœ¨ç£ç›˜ä¸Šçš„æ‰€æœ‰é¡µé¢æ¥è®¿é—®æ•°æ®ã€‚ è¿™ç§æ–¹æ³•æ€»æ˜¯æœ‰ä¸€å®šçš„æˆæœ¬ï¼Œæˆ‘ä»¬å°†ä¸ç´¢å¼•è®¿é—®çš„æˆæœ¬è¿›è¡Œæ¯”è¾ƒï¼Œä»¥ç¡®å®šæ›´ç»æµçš„é€‰æ‹©ã€‚ åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¼˜åŒ–å™¨å°†é€‰æ‹©èšé›†ç´¢å¼•æ¥æ‰§è¡ŒæŸ¥è¯¢ï¼Œå› ä¸ºå®ƒåœ¨ 105 ä¸ª I/O æ“ä½œæ—¶æˆæœ¬æœ€ä½ï¼Œå› æ­¤æ˜¯æœ€ä¾¿å®œä¸”æœ€æœ‰æ•ˆçš„è®¿é—®è·¯å¾„ã€‚
Regardless of whether we have indexes or not, a heap scan is always an available option, where data is accessed by sweeping through all pages stored on disk. This approach will always have a certain cost, against which we compare the cost of indexed accesses to determine the more economical choice. In this instance, the optimizer would select the clustered index for executing the query since it has the lowest cost at 105 I/O operations, thus being the cheapest and most efficient access path.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403234842604.png" alt="image-20240403234842604" style="zoom:33%;" /> 

**1ï¸âƒ£**Typically queries have multiple predicates (conditions)

**2ï¸âƒ£****Example**: day<8/9/94 AND rname=â€˜Paulâ€™AND bid=5 AND sid=3

**3ï¸âƒ£**B-tree index matches(a combination of) predicates that involve only attributes in a **prefix of the search key**

1. Index on <a, b, c> matches predicates on: (a,b,c), (a,b) and(a)
2. Index on <a, b, c> matchesa=5 AND b=3, but will notused to answer b=3
3. This implies that only reduction factors of predicates that are part of the prefix will be used to determine the cost (they are called matching predicates (or primary conjuncts))

```
åœ¨å¤„ç†æŸ¥è¯¢é€‰æ‹©æ¡ä»¶æ—¶ï¼Œå¿…é¡»é€šè¿‡è¯„ä¼°åŒ¹é…çš„è°“è¯æ•°é‡æ¥è¯„ä¼°ç´¢å¼•å¦‚ä½•ä¼˜åŒ–æŸ¥è¯¢ã€‚ æŸ¥è¯¢é€šå¸¸å¸¦æœ‰å¤šä¸ªæ¡ä»¶ã€‚ ä¾‹å¦‚ï¼ŒæŸ¥è¯¢å¯ä»¥åŒ…å«æœ‰å…³æ—¥æœŸã€åç§°ã€èˆ¹ ID å’Œæ°´æ‰‹ ID çš„è°“è¯ã€‚ å½“ç´¢å¼•åŠ é€Ÿæ¶‰åŠä¸æœç´¢é”®å‰ç¼€ä¸­çš„å±æ€§åŒ¹é…çš„è°“è¯çš„æŸ¥è¯¢æ—¶ï¼Œç´¢å¼•ç‰¹åˆ«æœ‰ç”¨ã€‚ å›æƒ³ä¸€ä¸‹ï¼Œæœç´¢é”®æ˜¯æ„å»ºç´¢å¼•çš„ä¸€ç»„å±æ€§ã€‚
When dealing with query selection conditions, itâ€™s essential to assess how an index can optimize the query by evaluating the number of predicates that match. Queries often come with multiple conditions. For example, a query could include predicates on the day, name, boat ID, and sailor ID. An index is particularly useful when it accelerates queries involving predicates that match the attributes in the prefix of the search key. Recall that a search key is a set of attributes on which an index is built.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403234842604.png" alt="image-20240403234842604" style="zoom:33%;" /> 

**1ï¸âƒ£**Typically queries have multiple predicates (conditions)

**2ï¸âƒ£****Example**: day<8/9/94 AND rname=â€˜Paulâ€™AND bid=5 AND sid=3

**3ï¸âƒ£**B-tree index matches(a combination of) predicates that involve only attributes in a **prefix of the search key**

1. Index on <a, b, c> matches predicates on: (a,b,c), (a,b) and(a)
2. Index on <a, b, c> matchesa=5 AND b=3, but will notused to answer b=3
3. This implies that only reduction factors of predicates that are part of the prefix will be used to determine the cost (they are called matching predicates (or primary conjuncts))

```
å¦‚æœåœ¨å±æ€§ Aã€B å’Œ C ä¸Šåˆ›å»ºç´¢å¼•ï¼Œåˆ™å®ƒå¯ä»¥æœ‰æ•ˆåœ°æ”¯æŒå¯¹ Aã€Bã€C æˆ–åŒ…å« A çš„ä»»æ„ç»„åˆè¿›è¡Œè°“è¯çš„æŸ¥è¯¢ï¼Œå› ä¸º A æ˜¯å‰ç¼€çš„å¼€å¤´ã€‚ ä½†æ˜¯ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ<a, b, c> ä¸Šçš„ç´¢å¼•ä¸ä¼šåŠ å¿«ä»…æŸ¥æ‰¾æ¡ä»¶ B = 3 çš„æŸ¥è¯¢ï¼Œå› ä¸ºå•ç‹¬çš„ B å¹¶ä¸æ„æˆå‰ç¼€ã€‚ ä½†å¦‚æœæ¡ä»¶åŒ…æ‹¬A = 5 AND B = 3ï¼Œåˆ™è¯¥å¯¹å½¢æˆå‰ç¼€å¹¶ä¸”å¯ä»¥ä½¿ç”¨ç´¢å¼•ã€‚ è¿™äº›åŒ¹é…è°“è¯çš„ç¼©å‡å› å­ï¼ˆä¹Ÿç§°ä¸ºä¸»è¿æ¥ï¼‰å°†æœ‰åŠ©äºå‡å°‘å’Œç¡®å®šæŸ¥è¯¢æˆæœ¬ã€‚ è¿™ç§ä¸»è¦è¿æ¥è¯çš„æ¦‚å¿µå¯èƒ½ä¼šå‡ºç°åœ¨æ–‡çŒ®æˆ–åœ¨çº¿èµ„æºä¸­çš„å„ç§æœ¯è¯­ä¸‹ï¼Œå°½ç®¡å®ƒå¯èƒ½éå¸¸æŠ½è±¡ã€‚
If an index is created on attributes A, B, and C, it can efficiently support queries with predicates on A, B, C, or any combination of these where A is included, since A is the start of the prefix. However, it's crucial to note that an index on <a, b, c> would not expedite a query seeking only condition B = 3, as B alone does not constitute a prefix. But if the conditions include A = 5 AND B = 3, this pair forms a prefix and the index can be used. The reduction factors for these matching predicates, also known as primary conjuncts, will be instrumental in reducing and determining the cost of the query. This concept of primary conjuncts may appear under various terms in literature or online resources, even though it can be quite abstract.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403210937748.png" alt="image-20240403210937748" style="zoom: 33%;" /> 

```
è€ƒè™‘æˆ‘ä»¬æœ‰ä¸€ä¸ªå»ºç«‹åœ¨ A å’Œ B åˆ—ä¸Šçš„ç´¢å¼•ï¼Œè¿™æ„å‘³ç€åœ¨ A çš„æ¯ç»„ä¸­ï¼Œæ•°æ®é¦–å…ˆæŒ‰ A æ’åºï¼Œç„¶åæŒ‰ B æ’åºã€‚å› æ­¤ï¼Œå¦‚æœ A ä¸º 1ï¼Œåˆ™ B çš„èŒƒå›´å¯ä»¥ä» 1 åˆ° 7ã€2 åˆ° 8ï¼Œå¹¶ä¸” ä¾æ­¤ç±»æ¨ï¼Œéµå¾ªç´¢å¼•æŒ‡å®šçš„é¡ºåºã€‚ è¯¥ç´¢å¼•å…è®¸æˆ‘ä»¬é€šè¿‡ç›´æ¥è·³è½¬åˆ° A ä¸º 3 çš„æ¡ç›®æ¥é«˜æ•ˆåœ°æ‰§è¡Œå¸¦æœ‰ A=3 ç­‰è°“è¯çš„æŸ¥è¯¢ã€‚è¯¥ç´¢å¼•å¯¹äº A=1 AND B>2 ç­‰å¤åˆæ¡ä»¶ä¹Ÿå¾ˆæœ‰ç”¨ã€‚ é€šè¿‡æŸ¥æ‰¾ A ä¸º 1 çš„è®°å½•ï¼Œæˆ‘ä»¬å¯ä»¥æ‰«ææ’åºåçš„ B å€¼ï¼Œä»¥å¿«é€Ÿæ‰¾åˆ°å¤§äº 2 çš„è®°å½•ã€‚
Consider we have an index built on columns A and B, meaning the data is ordered firstly by A and then by B within each group of A. So if A is 1, then B could range from 1 to 7, 2 to 8, and so on, following the order dictated by the index. This index allows us to efficiently execute a query with a predicate such as A=3 by directly jumping to the entries where A is 3. The index is also beneficial for a compound condition like A=1 AND B>2. By finding the records where A is 1, we can then scan through the sorted B values to quickly locate those greater than 2.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403210937748.png" alt="image-20240403210937748" style="zoom: 33%;" /> 

```
ä½†æ˜¯ï¼Œç´¢å¼•å¯¹äºä»…æŒ‡å®š B=1 çš„æŸ¥è¯¢æ²¡æœ‰å¸®åŠ©ï¼Œå› ä¸ºæ•°æ®ä¸ä»…ä»…æŒ‰ B æ’åºã€‚å¦‚æœæˆ‘ä»¬è¦åœ¨ç´¢å¼•ä¸­æŸ¥æ‰¾ B ä¸º 1 çš„ä½ç½®ï¼Œæˆ‘ä»¬ä¸ä»…ä¼šæ‰¾åˆ°å®ƒ ä½äºå·²æ’åº B å€¼çš„å¼€å¤´ï¼Œä½†å¯èƒ½åˆ†æ•£åœ¨å„å¤„ï¼Œå› ä¸ºä¸»æ’åºé”® A å¯ä»¥å…·æœ‰è®¸å¤šå…³è”çš„ B å€¼ã€‚
However, the index doesn't assist with a query that only specifies B=1, because the data isn't sorted solely by B. If we were to look for where B is 1 within the index, weâ€™d find it not only at the beginning of the sorted B values but potentially scattered throughout, since A, the primary sort key, can have many associated B values.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403210937748.png" alt="image-20240403210937748" style="zoom: 33%;" /> ```

```
åœ¨è®¡ç®—è®¿é—®æ•°æ®çš„æˆæœ¬æ—¶ï¼Œå½“æ¡ä»¶ä»…é€‚ç”¨äº A æ—¶ï¼Œå°†ä½¿ç”¨ A çš„ç¼©å‡å› å­ã€‚å¦‚æœæ¡ä»¶åŒæ—¶åŒ…å« A å’Œ Bï¼Œåˆ™å°†è€ƒè™‘å®ƒä»¬çš„ç¼©å‡å› å­ã€‚ ä½†å¯¹äºä»…æ¶‰åŠ Bï¼ˆä¸æ˜¯å‰ç¼€ï¼‰çš„æ¡ä»¶ï¼Œç´¢å¼•å¯¹é™ä½æˆæœ¬æ²¡æœ‰è´¡çŒ®ã€‚ åœ¨æŸ¥è¯¢åŒ…å«æ¡ä»¶A=1ã€B>3ã€C=7çš„åœºæ™¯ä¸‹ï¼Œç´¢å¼•è¾…åŠ©å®šä½Aã€Bæ¡ä»¶ã€‚ å¯¹äºä¸åŒ…å«åœ¨ç´¢å¼•ä¸­çš„ Cï¼Œç¨åå°†æ•°æ®æå–åˆ°å†…å­˜ä¸­ä»¥ä¾›è¿›ä¸€æ­¥å¤„ç†æ—¶æ‰§è¡Œæ£€æŸ¥ã€‚ å› æ­¤ï¼Œæˆæœ¬ä¸­è€ƒè™‘äº†Aå’ŒBçš„æŠ˜å‡ç³»æ•°ï¼Œä½†ä¸è€ƒè™‘Cã€‚
In calculating the cost of accessing data, the reduction factor for A is used when the condition is solely on A. If the condition includes both A and B, both their reduction factors are considered. But for a condition solely on B, which isn't a prefix, the index doesnâ€™t contribute to cost reduction. In a scenario where the query includes conditions A=1, B>3, and C=7, the index assists in locating A and B conditions. For C, which is not included in the index, the check is performed later when the data is fetched into memory for further processing. Hence, the reduction factors of A and B are considered in the cost, but not C.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403235426384.png" alt="image-20240403235426384" style="zoom:33%;" /> 

**1ï¸âƒ£**Find the **cheapest access path**

- An index or file scan with the **least estimated page I/O**

**2ï¸âƒ£**Retrieve tuples using it

- **Predicates that match** this index reduce the number of tuples retrieved (and impact the cost)

**3ï¸âƒ£**Apply the predicates that **donâ€™t match** the index (if any) later on

1. These predicates are used to disğŸš—d some retrieved tuples, but do not affect number of tuples/pages fetched (nor the total cost)
2. In this case selection over other predicates is said to be done â€œon-the-flyâ€

```
å½“ä½¿ç”¨å¤šä¸ªè°“è¯æ‰§è¡Œé€‰æ‹©æ—¶ï¼Œç›®æ ‡æ˜¯ç¡®å®šæœ€å…·æˆæœ¬æ•ˆç›Šçš„è®¿é—®è·¯å¾„ã€‚ è¿™å¯ä»¥æ˜¯ç´¢å¼•æˆ–å®Œæ•´æ–‡ä»¶æ‰«æï¼Œå¹¶ä¸”æ‰€é€‰æ–¹æ³•åº”è¯¥å…·æœ‰æœ€ä½ä¼°è®¡çš„é¡µé¢è®¿é—®æ¬¡æ•°ï¼Œè¿™ç›¸å½“äºæ£€ç´¢å…ƒç»„æ‰€éœ€çš„ I/O æ“ä½œã€‚ æ­£å¦‚æˆ‘æ‰€è§£é‡Šçš„ï¼Œä¸ç´¢å¼•å¯¹é½çš„è°“è¯å¯ä»¥æ˜¾ç€å‡å°‘æˆ‘ä»¬éœ€è¦è·å–çš„å…ƒç»„æ•°é‡ã€‚
When executing a selection with multiple predicates, the goal is to determine the most cost-effective access path. This could be an index or a full file scan, and the chosen method should have the lowest estimated number of page accesses, which equates to I/O operations needed to retrieve the tuples. As I've explained, predicates that align with the index can significantly reduce the number of tuples we need to fetch.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403235426384.png" alt="image-20240403235426384" style="zoom:33%;" /> 

**1ï¸âƒ£**Find the **cheapest access path**

- An index or file scan with the **least estimated page I/O**

**2ï¸âƒ£**Retrieve tuples using it

- **Predicates that match** this index reduce the number of tuples retrieved (and impact the cost)

**3ï¸âƒ£**Apply the predicates that **donâ€™t match** the index (if any) later on

1. These predicates are used to disğŸš—d some retrieved tuples, but do not affect number of tuples/pages fetched (nor the total cost)
2. In this case selection over other predicates is said to be done â€œon-the-flyâ€

```
åœ¨æˆ‘ä»¬é€šè¿‡ç´¢å¼•æ£€ç´¢æ•°æ®å¹¶å°†å…¶åŠ è½½åˆ°å†…å­˜ä¸­ï¼ˆæƒ³è±¡ä¸€ä¸‹å°†æ•°æ®ä»ç£ç›˜ç§»åŠ¨åˆ°å†…å­˜è¿›è¡Œå¤„ç†çš„è¿‡ç¨‹ï¼‰ä¹‹åï¼Œæˆ‘ä»¬å°±å¯ä»¥åº”ç”¨ä¸ç´¢å¼•ä¸åŒ¹é…çš„ä»»ä½•è°“è¯ã€‚ ä¾‹å¦‚ï¼Œåœ¨æˆ‘ä»¬çš„åœºæ™¯ä¸­ï¼ŒC ä¸Šçš„è°“è¯å°†åœ¨è¿™ä¸ªé˜¶æ®µè¿›è¡Œè¯„ä¼°ã€‚ æ­¤æ­¥éª¤ç§°ä¸ºâ€œå³æ—¶â€å¤„ç†ï¼Œå› ä¸ºå®ƒå‘ç”Ÿåœ¨å†…å­˜ä¸­ï¼Œå› æ­¤ä¸ä¼šå¢åŠ  I/O æ“ä½œæˆæœ¬ã€‚ è¯·è®°ä½ï¼Œæˆ‘ä»¬ä¸»è¦æ ¹æ® I/O æ“ä½œæ¥é‡åŒ–æˆæœ¬ã€‚
After we retrieve the data via the index and load it into memoryâ€”picture the process of moving data from disk to memory for processingâ€”we can then apply any predicates that don't match the index. For instance, in our scenario, the predicate on C would be assessed at this stage. This step is referred to as 'on-the-fly' processing because it occurs in memory and thus does not add to the I/O operation cost. Remember, we quantify cost primarily in terms of I/O operations.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240404000039172.png" alt="image-20240404000039172" style="zoom:33%;" /> 

**1ï¸âƒ£**Example: day < 8/9/94 AND bid=5 AND sid=3

**2ï¸âƒ£**A B+ tree index on day can be used;

1. RF = RF(day)
2. Then, bid=5and sid=3 must be checked for each retrieved tuple on the fly

**3ï¸âƒ£**Similarly, a hash index on <bid, sid> could be used;

1. $\Pi{}RF$â€‹= RF(bid)*RF(sid)
2. Then, day<8/9/94must be checkedon the fly

**4ï¸âƒ£**How about a B+treeon <rname,day>? (Y/N)

**5ï¸âƒ£**How about a B+treeon <day, rname>? (Y/N)

**6ï¸âƒ£**How about a Hash index on <day, rname>? (Y/N)

```
è®©æˆ‘ä»¬çœ‹ä¸€äº›ä¾‹å­ï¼Œäº†è§£å¦‚ä½•åœ¨é¢å¯¹å¤šä¸ªè°“è¯æ—¶è®¡ç®—æœ€ä¾¿å®œçš„è®¿é—®è·¯å¾„ã€‚ å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå…³äºâ€œdayâ€å±æ€§çš„ B æ ‘ç´¢å¼•ã€‚ åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ç¡®å®šç´¢å¼•çš„ç¼©å‡å› å­æ¥è®¡ç®—æˆæœ¬ã€‚ ç¼©å‡ç³»æ•°å°†çº³å…¥æˆ‘ä»¬çš„æŒ‡æ•°æˆæœ¬å…¬å¼ä¸­ã€‚ â€œèˆ¹ IDâ€å’Œâ€œæ°´æ‰‹ IDâ€çš„è°“è¯å°†åœ¨ç¨åçš„åŠ¨æ€å¤„ç†è¿‡ç¨‹ä¸­åº”ç”¨ï¼Œå› æ­¤å®ƒä»¬ä¸ä¼šå½±å“åˆå§‹æˆæœ¬ã€‚
Letâ€™s look at some examples of how to calculate the cheapest access path when faced with multiple predicates. Suppose we have a B-tree index on the â€˜dayâ€™ attribute. In this case, we can determine the index's reduction factor to calculate the cost. The reduction factor will be incorporated into our cost formula for the index. The predicates for â€˜boat IDâ€™ and â€˜sailor IDâ€™ will be applied afterward, during the on-the-fly processing, so they won't contribute to the initial cost.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240404000039172.png" alt="image-20240404000039172" style="zoom:33%;" /> 

**1ï¸âƒ£**Example: day < 8/9/94 AND bid=5 AND sid=3

**2ï¸âƒ£**A B+ tree index on day can be used;

1. RF = RF(day)
2. Then, bid=5and sid=3 must be checked for each retrieved tuple on the fly

**3ï¸âƒ£**Similarly, a hash index on <bid, sid> could be used;

1. $\Pi{}RF$= RF(bid)*RF(sid)
2. Then, day<8/9/94must be checkedon the fly

**4ï¸âƒ£**How about a B+treeon <rname,day>? (Y/N)

**5ï¸âƒ£**How about a B+treeon <day, rname>? (Y/N)

**6ï¸âƒ£**How about a Hash index on <day, rname>? (Y/N)

```
å¦ä¸€æ–¹é¢ï¼Œå¦‚æœæˆ‘ä»¬åœ¨â€œèˆ¹â€å’Œâ€œæ°´æ‰‹ IDâ€ä¸Šæœ‰å“ˆå¸Œç´¢å¼•ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨â€œèˆ¹ IDâ€å’Œâ€œæ°´æ‰‹ IDâ€çš„ç¼©å‡å› å­æ¥é™ä½æˆæœ¬ã€‚ ä½†æ˜¯ï¼Œå°†åŠ¨æ€æ£€æŸ¥â€œdayâ€è°“è¯ï¼Œå› ä¸ºå®ƒä¸å“ˆå¸Œç´¢å¼•çš„æ¡ä»¶ä¸åŒ¹é…ã€‚ è¦å˜å¾—ç†Ÿç»ƒï¼Œå¿…é¡»ç»ƒä¹ ç¡®å®šå“ªäº›è°“è¯å¯ç”¨äºé™ä½ç´¢å¼•æ“ä½œçš„æˆæœ¬ã€‚
On the other hand, if we have a hash index on â€˜boatâ€™ and â€˜sailor IDâ€™, we'll use the reduction factors for both â€˜boat IDâ€™ and â€˜sailor IDâ€™ to decrease the cost. However, the predicate on â€˜dayâ€™ will be checked on the fly because it doesnâ€™t match the hash index's criteria. To become proficient, it's essential to practice identifying which predicates can be used to reduce the cost of index operations.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240404000039172.png" alt="image-20240404000039172" style="zoom:33%;" /> 

**1ï¸âƒ£**Example: day < 8/9/94 AND bid=5 AND sid=3

**2ï¸âƒ£**A B+ tree index on day can be used;

1. RF = RF(day)
2. Then, bid=5and sid=3 must be checked for each retrieved tuple on the fly

**3ï¸âƒ£**Similarly, a hash index on <bid, sid> could be used;

1. $\Pi{}RF$= RF(bid)*RF(sid)
2. Then, day<8/9/94must be checkedon the fly

**4ï¸âƒ£**How about a B+treeon <rname,day>? (Y/N)

**5ï¸âƒ£**How about a B+treeon <day, rname>? (Y/N)

**6ï¸âƒ£**How about a Hash index on <day, rname>? (Y/N)

```
å°†å…¶è§†ä¸ºç»ƒä¹ ï¼šå†³å®šæ˜¯å¦å¯ä»¥åˆ©ç”¨è¿™äº›ç´¢å¼•æ¥åŠ å¿«æŸ¥è¯¢æ‰§è¡Œä»¥åŠåº”ç”¨å“ªäº›ç¼©å‡å› å­ã€‚ å…·ä½“æ¥è¯´ï¼Œä»”ç»†æŸ¥çœ‹â€œdayâ€å’Œâ€œnameâ€çš„å“ˆå¸Œç´¢å¼•ã€‚ å°½ç®¡è¿™äº›æ˜¯ä¸»è¦è¿æ¥çš„ä¸€éƒ¨åˆ†ï¼Œä½†å“ˆå¸Œç´¢å¼•å®é™…ä¸Šå¹¶ä¸èƒ½åŠ é€Ÿå¯¹â€œå¤©â€ç­‰èŒƒå›´æ¡ä»¶çš„åˆ†æï¼Œå› ä¸ºå¦‚å‰æ‰€è¿°ï¼Œå“ˆå¸Œç´¢å¼•ä»…å¯¹äºç›¸ç­‰æ¡ä»¶è€Œè¨€æ˜¯æœ€ä½³çš„ã€‚
Consider this as a practice exercise: decide whether these indexes can be leveraged to expedite query execution and which reduction factors would apply. Specifically, take a close look at the hash index on â€˜dayâ€™ and â€˜nameâ€™. Even though these are part of the primary conjuncts, a hash index doesnâ€™t actually accelerate the analysis for a range condition like â€˜dayâ€™ because, as discussed previously, hash indexes are optimal for equality conditions only.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403211155291.png" alt="image-20240403211155291" style="zoom:33%;" /> 

```
è®©æˆ‘ä»¬æŠŠæ³¨æ„åŠ›è½¬å‘æŠ•å½±çš„æ“ä½œã€‚ åœ¨æˆ‘ä»¬æ·±å…¥ç ”ç©¶è¿™ä¸ªæ¦‚å¿µä¹‹å‰ï¼Œæˆ‘æƒ³å…ˆåšä¸€ä¸ªæ€è€ƒç»ƒä¹ ã€‚ æ‚¨å¯èƒ½è¿˜è®°å¾—ï¼ŒæŠ•å½±æ˜¯ä»å…³ç³»ä¸­é€‰æ‹©ç›¸å…³å±æ€§å­é›†å¹¶æ ¹æ®å…³ç³»ä»£æ•°æ¶ˆé™¤ç»“æœä¸­ä»»ä½•é‡å¤é¡¹çš„è¿‡ç¨‹ã€‚ è€ƒè™‘ä¸€ä¸‹æˆ‘ä»¬å¦‚ä½•å®Œæˆè¿™é¡¹ä»»åŠ¡ï¼šå¯ä»¥åº”ç”¨å“ªäº›æŠ€æœ¯æ¥ä»å€¼æ•°ç»„ä¸­åˆ é™¤é‡å¤é¡¹ï¼Ÿ æˆ‘é¼“åŠ±æ‚¨æš‚åœç‰‡åˆ»å¹¶é›†æ€å¹¿ç›Šã€‚ æ€è€ƒå¯èƒ½çš„æ–¹æ³•å¹¶æå‡ºä¸€äº›æƒ³æ³•ã€‚ è¿™æ˜¯åº”ç”¨æ‚¨æ‰€å­¦åˆ°çš„çŸ¥è¯†å¹¶å¢å¼ºæ‚¨å¯¹æ•°æ®åº“æ“ä½œçš„ç†è§£çš„ç»ä½³æœºä¼šã€‚
Let's turn our attention to the operation of projection. I'd like to present a thought exercise before we delve deeper into the concept. Projection, as you may recall, is the process of selecting a subset of relevant attributes from a relation and, in accordance with relational algebra, eliminating any duplicates in the result. Consider how we might accomplish this task: what techniques can be applied to remove duplicates from an array of values? I encourage you to pause for a moment and brainstorm. Think about possible methods and come back with some ideas. This is an excellent opportunity to apply what you've learned and to enhance your understanding of database operations.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240404000618858.png" alt="image-20240404000618858" style="zoom:33%;" /> 

**1ï¸âƒ£**Issue with projection is removing duplicates

```sql
SELECT DISTINCT R.sid, R.bid FROM Reserves R
```

**2ï¸âƒ£**Projection can be done based on hashingor sorting

```
è®©æˆ‘ä»¬æ·±å…¥ç ”ç©¶ä¸€ä¸‹è¯†åˆ«æ•°ç»„ä¸­é‡å¤å€¼çš„ç­–ç•¥ã€‚ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•æ˜¯å¯¹æ•°ç»„è¿›è¡Œæ’åºã€‚é€šè¿‡æ’åºï¼Œæˆ‘ä»¬å¯ä»¥ç¡®ä¿ä»»ä½•é‡å¤å€¼éƒ½è¢«é‚»è¿‘æ”¾ç½®ï¼Œä»è€Œç®€åŒ–æ£€æµ‹è¿‡ç¨‹ã€‚å¦å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨å“ˆå¸Œå‡½æ•°ï¼Œè¿™è®©äººæƒ³èµ·å“ˆå¸Œè¡¨å’Œç´¢å¼•èƒŒåçš„æœºåˆ¶ã€‚è¿™ç§æ–¹æ³•å¯ä»¥ç¡®ä¿ç›¸åŒçš„å€¼æˆ–é‡å¤æ•°æ®è¢«åˆ†é…åˆ°åŒä¸€ä¸ªæ•°æ®æ¡¶ä¸­ï¼Œä»è€Œè®©æˆ‘ä»¬å¯ä»¥ä¸“æ³¨äºç‰¹å®šçš„å­é›†ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ•°æ®é›†æ¥è¿›è¡Œé‡å¤æ•°æ®éªŒè¯ã€‚è¿™ç§æŠ€æœ¯å¤§å¤§ç¼©å°äº†æˆ‘ä»¬çš„æœç´¢èŒƒå›´ï¼Œæ˜¯æ•°æ®åº“ç®¡ç†ä¸­å¸¸ç”¨çš„ä¸€ç§é«˜æ•ˆæ–¹æ³•ï¼Œå¯é€šè¿‡å“ˆå¸Œæˆ–æ’åºæ¶ˆé™¤é‡å¤æ•°æ®ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå½“æˆ‘ä»¬å¸Œæœ›åœ¨ MySQL ä¸­åˆ é™¤é‡å¤æ•°æ®æ—¶ï¼Œå¿…é¡»æ˜ç¡®ä½¿ç”¨DISTINCTå­å¥ï¼Œå› ä¸º MySQL ä¸ä¼šè‡ªåŠ¨ä¸ºæˆ‘ä»¬åˆ é™¤é‡å¤æ•°æ®ã€‚
Let's delve into the strategies for identifying duplicates within an array of values. One effective method is to sort the array. By sorting, we ensure that any duplicates will be positioned adjacently, simplifying the detection process. Alternatively, we can employ a hash function, reminiscent of the mechanisms behind hash tables and indexes. This approach ensures that identical values, or duplicates, are allocated to the same bucket, allowing us to focus on a specific subset rather than the entire dataset for duplicate verification. This technique significantly reduces the scope of our search, making it an efficient method commonly utilized in database management to eliminate duplicates through either hashing or sorting. It's important to note that when we wish to remove duplicates in MySQL, for instance, we must explicitly use the DISTINCT clause, as MySQL does not automatically remove duplicates for us.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240404210223720.png" alt="image-20240404210223720" style="zoom: 33%;" /> 

**1ï¸âƒ£**Basic approach is to use sorting

1. Scan R, extract only the neededattributes

2. Sort the result set (typically using external merge sort)

3. Remove adjacentduplicates

   <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240404210401602.png" alt="image-20240404210401602" style="zoom:50%;" /> 

```
è¦æ‰§è¡ŒåŸºäºæ’åºçš„æŠ•å½±æ“ä½œï¼Œæˆ‘ä»¬é¦–å…ˆè¦å¯¹å…³ç³»è¿›è¡Œæ‰«æï¼Œåˆ†ç¦»å‡ºæ„Ÿå…´è¶£çš„å±æ€§ï¼Œè¿™æ˜¯æŠ•å½±çš„æœ¬è´¨ã€‚åœ¨åˆ†ç¦»å‡ºè¿™äº›å±æ€§åï¼Œæˆ‘ä»¬çš„ä¸‹ä¸€æ­¥å°±æ˜¯å¯¹å®ƒä»¬è¿›è¡Œæ’åºï¼Œé‡ç‚¹æ˜¯æ£€æµ‹ä»»ä½•ç›¸é‚»çš„é‡å¤æ•°æ®ã€‚ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬æ ¹æ®å¹´é¾„å’Œè–ªæ°´è¿™ä¸¤ä¸ªå±æ€§å¯¹æ•°æ®è¿›è¡Œæ’åºã€‚æ•°æ®æ’åºå®Œæˆåï¼Œæˆ‘ä»¬çš„ä»»åŠ¡å°±æ˜¯æ£€æŸ¥ç›¸é‚»æ¡ç›®æ˜¯å¦å­˜åœ¨é‡å¤ã€‚è¿™ä¸ªè¿‡ç¨‹è™½ç„¶ç®€å•æ˜äº†ï¼Œä½†å´èƒ½ç¡®ä¿å¦‚æœå­˜åœ¨é‡å¤é¡¹ï¼Œå®ƒä»¬å°†ç´§ç´§ç›¸é‚»ã€‚
To perform a projection operation based on sorting, we first undertake a scan of the relation to isolate the attributes of interest, which is the essence of projection. After isolating these attributes, our next step is to sort them, focusing specifically on detecting any adjacent duplicates. For instance, consider a scenario where we are sorting data based on two attributes: age and salary. Once the data is sorted, our task is to examine adjacent entries for duplicates. This process, though straightforward, ensures that if duplicates are present, they will be immediately adjacent.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240404210223720.png" alt="image-20240404210223720" style="zoom: 33%;" /> 

**1ï¸âƒ£**Basic approach is to use sorting

1. Scan R, extract only the neededattributes

2. Sort the result set (typically using external merge sort)

3. Remove adjacentduplicates

   <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240404210401602.png" alt="image-20240404210401602" style="zoom:50%;" /> 

```sq
ç„¶è€Œï¼Œæ•°æ®é‡çš„å·¨å¤§å¸¦æ¥äº†ä¸¥å³»çš„æŒ‘æˆ˜ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œæ•°æ®å­˜æ”¾åœ¨ç£ç›˜é¡µé¢ä¸Šï¼Œè€Œå¯ç”¨äºå¤„ç†çš„å†…å­˜å´éå¸¸æœ‰é™ã€‚ç”±äºæ‰€æœ‰å¤„ç†éƒ½æ˜¯åœ¨å†…å­˜ä¸­è¿›è¡Œçš„ï¼Œå› æ­¤å…³é”®é—®é¢˜æ˜¯å½“æ•°æ®è¶…è¿‡å¯ç”¨å†…å­˜å®¹é‡æ—¶å¦‚ä½•è¿›è¡Œæ’åºã€‚è¿™æ˜¯æ•°æ®åº“éœ€è¦è§£å†³çš„ä¸€ä¸ªå¸¸è§é—®é¢˜ã€‚However, a critical challenge arises due to the volume of data. Typically, data resides on disk pages, whereas the memory available for processing is significantly more limited. Since all processing occurs in memory, the pivotal question is how to perform sorting when the data exceeds available memory capacity. This is a common problem that databases need to address.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240404210223720.png" alt="image-20240404210223720" style="zoom: 33%;" /> 

**1ï¸âƒ£**Basic approach is to use sorting

1. Scan R, extract only the neededattributes

2. Sort the result set (typically using external merge sort)

3. Remove adjacentduplicates

   <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240404210401602.png" alt="image-20240404210401602" style="zoom:50%;" /> 

```
ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ•°æ®åº“é‡‡ç”¨äº†ä¸€ç§ç§°ä¸ºå¤–éƒ¨æ’åºçš„æŠ€æœ¯ï¼Œå®ƒæ˜¯å¤„ç†è¶…å‡ºå†…å­˜é™åˆ¶çš„å¤§å‹æ•°æ®é›†çš„ä¸€ç§å·§å¦™è€Œå®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚å¯¹äºé‚£äº›å¯¹ç®—æ³•æƒ…æœ‰ç‹¬é’Ÿçš„äººæ¥è¯´ï¼Œå¤–éƒ¨æ’åºèƒŒåçš„åŸç†ç›¸å½“è€äººå¯»å‘³ï¼Œå€¼å¾—ç®€å•è§£é‡Šä¸€ä¸‹ï¼Œä»¥æŒæ¡å…¶åŸºæœ¬åŸç†ã€‚
To manage this, databases employ a technique known as external sorting, which is an ingenious and practical solution for handling large datasets that exceed memory constraints. For those with an affinity for algorithms, the rationale behind external sorting is quite intriguing and merits a brief explanation to grasp its basic principles.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403213709995.png" alt="image-20240403213709995" style="zoom:33%;" /> 

```
åœ¨å®é™…æ“ä½œä¸­ï¼Œå½“å¤„ç†å­˜å‚¨åœ¨ç£ç›˜ä¸Šæ— æ•°æ•°æ®é¡µä¸­çš„å¤§å‹æ•°æ®é›†æ—¶ï¼Œæˆ‘ä»¬ä¼šé‡åˆ°ä¸€ä¸ªå¾ˆå¤§çš„é™åˆ¶ï¼šè®¡ç®—æœºç³»ç»Ÿä¸­çš„ RAM å®¹é‡æœ‰é™ï¼Œè€Œæ‰€æœ‰å¤„ç†è¿‡ç¨‹éƒ½æ˜¯åœ¨ RAM ä¸­è¿›è¡Œçš„ã€‚ä¸ºäº†å¯åŠ¨æ’åºï¼Œæˆ‘ä»¬å¿…é¡»å°†æ•°æ®ä»ç£ç›˜ä¼ è¾“åˆ°å†…å­˜ä¸­ã€‚ç„¶è€Œï¼Œå½“æ•°æ®é›†è¶…è¿‡ RAM çš„å®¹é‡æ—¶ï¼Œè¿™å°±å¸¦æ¥äº†æŒ‘æˆ˜ï¼Œåœ¨è¿™ä¸ªç®€åŒ–çš„ä¾‹å­ä¸­ï¼ŒRAM ä¸€æ¬¡åªèƒ½å®¹çº³ä¸‰é¡µæ•°æ®ã€‚
In practical terms, when dealing with large datasets stored across numerous data pages on a disk, we encounter a significant constraint: the limited size of RAM in our computer systems where all processing occurs. To initiate sorting, we must transfer the data from the disk into memory. However, this poses a challenge when the dataset exceeds the capacity of RAM, which in this simplified example, can only accommodate three pages of data at a time.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403213709995.png" alt="image-20240403213709995" style="zoom:33%;" /> 

```
è§£å†³è¿™ä¸ªé—®é¢˜çš„åŠæ³•æ˜¯å°†è¾ƒå¤§çš„æ•°æ®é›†åˆ†å‰²æˆé€‚åˆå†…å­˜çš„è¾ƒå°éƒ¨åˆ†ã€‚æˆ‘ä»¬é¦–å…ˆå°†å‰ä¸‰é¡µåŠ è½½åˆ° RAM ä¸­ï¼Œå¯¹å…¶è¿›è¡Œåˆ†ç±»ï¼Œç„¶åå°†å…¶è¿”å›ç£ç›˜ï¼Œä¸ºä¸‹ä¸€ç»„é¡µé¢é‡Šæ”¾å†…å­˜ã€‚é‡å¤è¿™ä¸€æ­¥éª¤ï¼Œä¸€æ¬¡ä¸€æ‰¹ï¼Œç›´åˆ°æˆ‘ä»¬éå†æ•´ä¸ªæ•°æ®é›†ã€‚åœ¨å¤–éƒ¨æ’åºçš„æƒ…å†µä¸‹ï¼Œè¿™äº›éå†è¢«ç§°ä¸º "è¿è¡Œ"ï¼Œæ¯æ¬¡è¿è¡Œéƒ½ä¼šäº§ç”ŸåŸå§‹æ•°æ®é›†çš„éƒ¨åˆ†æ’åºæ®µã€‚
The solution to this problem is to segment the larger dataset into smaller portions that fit into memory. We begin by loading the first three pages into RAM, sorting them, and then returning them to the disk to free up memory for the next set of pages. This step is repeated, one batch at a time, until we have traversed the entire dataset. Each of these passes, known in the context of external sorting as a 'run', results in partially sorted segments of the original dataset.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403213709995.png" alt="image-20240403213709995" style="zoom:33%;" /> 

```
åœ¨éšåçš„é˜¶æ®µï¼Œæˆ‘ä»¬ä»æ¯ä¸ªæ’åºæ®µä¸­æå–ç¬¬ä¸€é¡µï¼Œå¹¶å°†å…¶åˆå¹¶ã€‚é€šè¿‡å¯¹è¿™äº›åˆå¹¶çš„é¡µé¢è¿›è¡Œæ’åºå¹¶å°†å…¶è¿”å›ç£ç›˜ï¼Œæˆ‘ä»¬å¯ä»¥ç¡®ä¿åœ¨è¿™ä¸ªå­é›†ä¸­æ’åºå‡ºå°½å¯èƒ½å°çš„å€¼ã€‚è¿™ä¸€è¿­ä»£è¿‡ç¨‹æ¶‰åŠå¯¹æ•°æ®çš„å¤šæ¬¡ä¼ é€’æˆ– "è¿è¡Œ"ï¼Œæœ€ç»ˆä¼šå¾—åˆ°ä¸€ä¸ªå®Œå…¨æ’åºçš„æ•°æ®é›†ã€‚é€šè¿‡è¿™ç§è¢«ç§°ä¸º "æ’åº-åˆå¹¶ "æ“ä½œçš„æŠ€æœ¯ï¼Œæˆ‘ä»¬å°†æ’åºä»»åŠ¡åˆ†è§£ä¸ºæ˜“äºç®¡ç†çš„éƒ¨åˆ†ï¼Œä»è€Œå·§å¦™åœ°è§„é¿äº†å†…å­˜é™åˆ¶ã€‚
In the subsequent phase, we take the first page from each sorted segment and merge them. By sorting these combined pages and returning them to the disk, we ensure that we have the smallest possible values sorted within this subset. This iterative process, which involves multiple passes or 'runs' over the data, eventually leads to a fully sorted dataset. Through this technique, known as the sort-merge operation, we cleverly circumvent the memory limitation by breaking down the sorting task into manageable parts.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240404211110066.png" alt="image-20240404211110066" style="zoom:33%;" /> 

****1ï¸âƒ£**If data does not fit in memory do several passes**

**2ï¸âƒ£**Sort runs: Make each B pages sorted (called runs)

**3ï¸âƒ£**Merge runs: Make multiple passes to merge runs

1. Pass 2: Produce runs of length B(B-1) pages  ==We will let you know==
2. Pass 3: Produce runs of length B(B-1)$^2$â€‹ pages ==how many passes there are==
3. ......................
4. Pass P: Produce runs of length B(B-1)$^P$â€‹pages

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240404211310984.png" alt="image-20240404211310984" style="zoom:50%;" /> 

```
å¤–éƒ¨åˆå¹¶æ’åºæ˜¯ä¸€ç§å¼ºå¤§çš„ç®—æ³•ï¼Œè®¾è®¡ç”¨äºå¯¹è¶…å‡ºç³»ç»Ÿå†…å­˜å®¹é‡çš„æ•°æ®è¿›è¡Œæ’åºã€‚å½“å†…å­˜æ— æ³•å®¹çº³æ•°æ®æ—¶ï¼Œæˆ‘ä»¬ä¼šå¯¹å…¶è¿›è¡Œå¤šæ¬¡å¤„ç†ã€‚æœ€åˆï¼Œæˆ‘ä»¬åˆ›å»ºçš„æ’åºæ®µè¢«ç§°ä¸º "è¿è¡Œ"ã€‚ç„¶åæœ‰æ¡ä¸ç´Šåœ°åˆå¹¶è¿™äº›è¿è¡Œæ®µï¼Œæ¯æ¬¡ä»æ¯ä¸ªæ®µä¸­åˆå¹¶ä¸€é¡µï¼Œä»¥è·å¾—å®Œå…¨æ’åºçš„æ•°æ®é›†ã€‚è¿™ä¸ªè¿‡ç¨‹æ¶‰åŠå¤šè½®åˆå¹¶ï¼Œæ•°æ®æ’åºæ‰€éœ€çš„ç²¾ç¡®æ¬¡æ•°å–å†³äºå¤æ‚çš„æ•°å­¦åŸç†ï¼Œå…¶ä¸­è€ƒè™‘åˆ°æ•°æ®é›†çš„å¤§å°å’Œå¯ç”¨å†…å­˜ã€‚
The external merge sort is a robust algorithm designed for sorting data that exceeds the capacity of the system's memory. When data cannot be accommodated in RAM, we perform multiple passes over it. Initially, we create sorted segments known as 'runs'. These runs are then methodically merged, one page at a time, from each segment to achieve a fully sorted dataset. The process involves several rounds of merging, and the precise number of passes required to sort the data depends on complex mathematical principles, which take into account the size of the dataset and the available memory.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240404211110066.png" alt="image-20240404211110066" style="zoom:33%;" /> 

****1ï¸âƒ£**If data does not fit in memory do several passes**

**2ï¸âƒ£**Sort runs: Make each B pages sorted (called runs)

**3ï¸âƒ£**Merge runs: Make multiple passes to merge runs

1. Pass 2: Produce runs of length B(B-1) pages  ==We will let you know==
2. Pass 3: Produce runs of length B(B-1)$^2$â€‹ pages ==how many passes there are==
3. ......................
4. Pass P: Produce runs of length B(B-1)$^P$â€‹pages

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240404211310984.png" alt="image-20240404211310984" style="zoom:50%;" /> 

```
åœ¨æœ¬è¯¾ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä¸ºæ‚¨æä¾›å¿…è¦åˆæ ¼æ¬¡æ•°èƒŒåçš„å¤æ‚æ•°å­¦çŸ¥è¯†ï¼Œä»è€Œç®€åŒ–è¿™ä¸€è¿‡ç¨‹ã€‚è¿™æ„å‘³ç€æ‚¨åªéœ€å°†ç»™å®šçš„ä¿¡æ¯åº”ç”¨åˆ°è§„å®šçš„å…¬å¼ä¸­å³å¯ã€‚å¯¹äºé‚£äº›å¯¹ç®—æ³•ç»†èŠ‚æœ‰æµ“åšå…´è¶£çš„äººï¼Œæ•™ç§‘ä¹¦ç¬¬ 13 ç« å°†å¯¹å…¶è¿›è¡Œè¯¦å°½è®¨è®ºã€‚ä¸è¿‡ï¼Œæœ¬è¯¾ç¨‹å¹¶ä¸è¦æ±‚æ‚¨æ·±å…¥äº†è§£å¤–éƒ¨åˆå¹¶æ’åºçš„æœºåˆ¶ã€‚æˆ‘ä»¬çš„é‡ç‚¹å°†æ”¾åœ¨ç®—æ³•çš„æˆæœ¬è®¡ç®—æ–¹é¢ï¼Œè¿™å¯¹æ•°æ®åº“è‡³å…³é‡è¦ã€‚ç¨åï¼Œæˆ‘å°†æ¼”ç¤ºè®¡ç®—è¯¥ç®—æ³•è¿è¡Œæˆæœ¬æ‰€éœ€çš„å…·ä½“å…¬å¼ã€‚
For the purposes of this course, the intricate mathematics behind the number of necessary passes will be provided to you, simplifying the process. This means you'll only have to apply the given information into the prescribed formula. For those who have a keen interest in the algorithmic details, they are thoroughly discussed in Chapter 13 of the textbook. However, a deep understanding of the mechanics of external merge sort is not required for this course. Our focus will be on the costing aspect of the algorithm, which is crucial for databases. Shortly, I will demonstrate the specific formula that you will need to use for calculating the cost of running this algorithm.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

![image-20240403214301313](https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403214301313.png)

```
# buffer pages in memory B = 4, each page 2 records, sorting on a single attribute (just showing the attribute value)
```

```
To grasp the external merge sort, let's consider a small-scale example where our memory can hold four pages, and each page includes two records. We're sorting based on a single attribute, represented here by one attribute value. Despite having a multitude of pages in the input file, our limitation is the four pages that memory can simultaneously accommodate. We start by sorting the first four pages, creating a series of 'sorted runs'. We then continue this process until all the pages are sorted into these runs. In the subsequent phase, we take one page from each run and sort them in memory. Since the runs are already sorted, we know the smallest value is at the forefront, ensuring we're working with positive values. This process repeats: pulling data from the disk, sorting it in memory, and outputting the result, effectively sorting the entire data set through these iterative memory-disk exchanges.
ä¸ºäº†ç†è§£å¤–éƒ¨åˆå¹¶æ’åºï¼Œè®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªå°è§„æ¨¡çš„ä¾‹å­ï¼šæˆ‘ä»¬çš„å†…å­˜å¯ä»¥å®¹çº³å››ä¸ªé¡µé¢ï¼Œæ¯ä¸ªé¡µé¢åŒ…æ‹¬ä¸¤æ¡è®°å½•ã€‚æˆ‘ä»¬æ ¹æ®å•ä¸ªå±æ€§è¿›è¡Œæ’åºï¼Œè¿™é‡Œç”¨ä¸€ä¸ªå±æ€§å€¼æ¥è¡¨ç¤ºã€‚å°½ç®¡è¾“å…¥æ–‡ä»¶ä¸­æœ‰è®¸å¤šé¡µé¢ï¼Œä½†æˆ‘ä»¬çš„é™åˆ¶æ˜¯å†…å­˜åªèƒ½åŒæ—¶å®¹çº³å››ä¸ªé¡µé¢ã€‚æˆ‘ä»¬é¦–å…ˆå¯¹å‰å››é¡µè¿›è¡Œæ’åºï¼Œåˆ›å»ºä¸€ç³»åˆ— "æ’åºè¿è¡Œ"ã€‚ç„¶åï¼Œæˆ‘ä»¬ç»§ç»­è¿™ä¸ªè¿‡ç¨‹ï¼Œç›´åˆ°æ‰€æœ‰é¡µé¢éƒ½è¢«æ’åºåˆ°è¿™äº›è¿è¡Œä¸­ã€‚åœ¨éšåçš„é˜¶æ®µï¼Œæˆ‘ä»¬ä»æ¯ä¸ªè¿è¡Œä¸­æŠ½å–ä¸€é¡µï¼Œå¹¶åœ¨å†…å­˜ä¸­è¿›è¡Œæ’åºã€‚ç”±äºå·²ç»å¯¹è¿è¡Œè¿›è¡Œäº†æ’åºï¼Œæˆ‘ä»¬çŸ¥é“æœ€å°å€¼ä½äºæœ€å‰é¢ï¼Œä»è€Œç¡®ä¿æˆ‘ä»¬å¤„ç†çš„æ˜¯æ­£å€¼ã€‚è¿™ä¸ªè¿‡ç¨‹ä¸æ–­é‡å¤ï¼šä»ç£ç›˜ä¸­æå–æ•°æ®ï¼Œåœ¨å†…å­˜ä¸­æ’åºï¼Œç„¶åè¾“å‡ºç»“æœï¼Œé€šè¿‡è¿™äº›å†…å­˜-ç£ç›˜çš„è¿­ä»£äº¤æ¢ï¼Œæœ‰æ•ˆåœ°å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œæ’åºã€‚
```

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403214417654.png" alt="image-20240403214417654" style="zoom: 39%;" /> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403214501611.png" alt="image-20240403214501611" style="zoom: 39%;" />  

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403214624220.png" alt="image-20240403214624220" style="zoom:39%;" /> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403214645394.png" alt="image-20240403214645394" style="zoom:39%;" /> 

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403214719266.png" alt="image-20240403214719266" style="zoom:39%;" /> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403214840734.png" alt="image-20240403214840734" style="zoom:39%;" /> 

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403214925145.png" alt="image-20240403214925145" style="zoom: 39%;" /> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403214944541.png" alt="image-20240403214944541" style="zoom:39%;" /> 

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403215019391.png" alt="image-20240403215019391" style="zoom:39%;" /> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403215036463.png" alt="image-20240403215036463" style="zoom:39%;" /> 

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403215056351.png" alt="image-20240403215056351" style="zoom:39%;" /> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240403215115209.png" alt="image-20240403215115209" style="zoom:39%;" /> 

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$
<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240404211857988.png" alt="image-20240404211857988" style="zoom:33%;" /> 

**1ï¸âƒ£**Sorting with external sort:

1. Scan R, extract only the needed attributes
2. Sort the result set using EXTERNAL SORT
3. Remove adjacent duplicates

**2ï¸âƒ£**Cost = 

```sql
ReadTable+              -- Read the entire table and keep only projected attributes
WriteProjectedPages+    -- Write pages with projected attributes to disk
SortingCost+            -- Sort pages with projected attributes with external sort
ReadProjectedPages      -- Read sorted projected pages to disğŸš—d adjacent duplicates
```

****3ï¸âƒ£**WriteProjectedPages**= NPages(R)* PF

****4ï¸âƒ£**PF: Projection Factor** says how much are we projecting, ratio with respect to all attributes (e.g. keeping Â¼ of attributes, or 10% of all attributes)

**5ï¸âƒ£**$\text{SortingCost= 2*NumPasses*ReadProjectedPages}$â€‹

The 2 represents: Every time we read and write

 ```\
 è°ˆåˆ°ä½¿ç”¨å¤–éƒ¨åˆå¹¶æ’åºçš„æŠ•å½±æ“ä½œï¼Œæˆ‘ä»¬çš„é‡ç‚¹è½¬ç§»åˆ°äº†æˆæœ¬è¯„ä¼°ä¸Šã€‚æ‰§è¡ŒæŠ•å½±çš„æˆæœ¬ä¸»è¦ç”±ä¸‰ä¸ªæ­¥éª¤å†³å®šã€‚é¦–å…ˆï¼Œè¯»å–æ•°æ®å¹¶æå–æŠ•å½±çš„ç›¸å…³å±æ€§ã€‚æå–åï¼Œå¯¹å±æ€§è¿›è¡Œæ’åºã€‚æ’åºå®Œæˆåçš„æœ€åä¸€æ­¥æ˜¯åˆ é™¤ç›¸é‚»çš„é‡å¤æ•°æ®ã€‚è¿™äº›æ­¥éª¤å…±åŒå®šä¹‰äº†é€šè¿‡å¤–éƒ¨åˆå¹¶æ’åºè¯„ä¼°æŠ•å½±æˆæœ¬æ‰€æ¶‰åŠçš„ä»»åŠ¡ã€‚
 Turning to the projection operation using external merge sort, our focus shifts to cost evaluation. The cost of performing a projection is delineated by three primary steps. Initially, data is read and the relevant attributes for the projection are extracted. Following this extraction, the attributes are sorted. The final step, once sorting is complete, involves the removal of adjacent duplicates. These steps collectively define the tasks involved in assessing the cost of a projection via external merge sort.
 ```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

 <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240404211857988.png" alt="image-20240404211857988" style="zoom:33%;" />

**1ï¸âƒ£**Sorting with external sort:

1. Scan R, extract only the needed attributes
2. Sort the result set using EXTERNAL SORT
3. Remove adjacent duplicates

**2ï¸âƒ£**Cost = 

```sql
ReadTable+              -- Read the entire table and keep only projected attributes
WriteProjectedPages+    -- Write pages with projected attributes to disk
SortingCost+            -- Sort pages with projected attributes with external sort
ReadProjectedPages      -- Read sorted projected pages to disğŸš—d adjacent duplicates
```

****3ï¸âƒ£**WriteProjectedPages**= NPages(R)* PF

****4ï¸âƒ£**PF: Projection Factor** says how much are we projecting, ratio with respect to all attributes (e.g. keeping Â¼ of attributes, or 10% of all attributes)

**5ï¸âƒ£**$\text{SortingCost= 2*NumPasses*ReadProjectedPages}$â€‹

The 2 represents: Every time we read and write

```
ä½¿ç”¨å¤–éƒ¨åˆå¹¶æ’åºçš„æŠ•å½±æˆæœ¬åŒ…æ‹¬å°†æ•´ä¸ªè¡¨ä»ç£ç›˜è¯»å–åˆ°å†…å­˜ï¼Œç„¶åå°†æŠ•å½±å±æ€§å†™å›ç£ç›˜çš„æ“ä½œã€‚ç„¶åå†æ¬¡è¯»å–æŠ•å½±é¡µé¢è¿›è¡Œæ’åºï¼Œè¿™å°±éœ€è¦æˆ‘ä»¬è€ƒè™‘å¤–éƒ¨æ’åºçš„æˆæœ¬ã€‚
The cost of a projection using external merge sort includes the operations of reading the entire table from disk to memory and then writing the projected attributes back to disk. This is followed by reading the projected pages again for sorting, which requires us to account for the cost of the external sort.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

 <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240404211857988.png" alt="image-20240404211857988" style="zoom:33%;" />

**1ï¸âƒ£**Sorting with external sort:

1. Scan R, extract only the needed attributes
2. Sort the result set using EXTERNAL SORT
3. Remove adjacent duplicates

**2ï¸âƒ£**Cost = 

```sql
ReadTable+              -- Read the entire table and keep only projected attributes
WriteProjectedPages+    -- Write pages with projected attributes to disk
SortingCost+            -- Sort pages with projected attributes with external sort
ReadProjectedPages      -- Read sorted projected pages to disğŸš—d adjacent duplicates
```

****3ï¸âƒ£**WriteProjectedPages**= NPages(R)* PF

****4ï¸âƒ£**PF: Projection Factor** says how much are we projecting, ratio with respect to all attributes (e.g. keeping Â¼ of attributes, or 10% of all attributes)

**5ï¸âƒ£**$\text{SortingCost= 2*NumPasses*ReadProjectedPages}$â€‹

The 2 represents: Every time we read and write

```
æŠ•å½±å› å­åœ¨å°†æŠ•å½±é¡µé¢å†™å›ç£ç›˜æ—¶å‘æŒ¥ä½œç”¨ã€‚å®ƒåæ˜ äº†æŠ•å½±åä¿ç•™çš„åˆ—çš„æ¯”ä¾‹ï¼Œç±»ä¼¼äºç¼©å‡å› å­ã€‚ä¾‹å¦‚ï¼Œ25% çš„æŠ•å½±ç³»æ•°è¡¨ç¤ºæˆ‘ä»¬ä¿ç•™äº†å››åˆ†ä¹‹ä¸€çš„åˆ—ã€‚æ’åºçš„æˆæœ¬ç”±å¤–éƒ¨æ’åºçš„æˆæœ¬å†³å®šï¼Œå¤–éƒ¨æ’åºçš„æˆæœ¬æ˜¯æ’åºæ¬¡æ•°ä¹˜ä»¥æŠ•å½±é¡µæ•°çš„ä¸¤å€ï¼Œä½“ç°äº†æ¯æ¬¡æ’åºæ‰€éœ€çš„è¯»å†™æ“ä½œã€‚
The projection factor comes into play when writing projected pages back to disk. It reflects the proportion of columns retained post-projection, similar to a reduction factor. For instance, a 25% projection factor indicates that we're keeping a quarter of the columns. The cost of sorting is encapsulated by the cost of the external sort, which is two times the number of passes multiplied by the number of projected pages, embodying the read and write operations necessary for each pass.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240404213412688.png" alt="image-20240404213412688" style="zoom:33%;" /> 

**1ï¸âƒ£**Example: Letâ€™s say that we project Â¼ of all attributes, and letâ€™s say that we have 20 pages in memory

**2ï¸âƒ£**PF = 1/4 = 0.25, NPages(R) = 1000

**3ï¸âƒ£**With 20 memory pages we can sort in 2 passes

**4ï¸âƒ£**Cost =

```
ReadTable+
WriteProjectedPages+
SortingCost+
ReadProjectedPages
= 1000 + 0.25 * 1000 + 2*2*250 + 250 = 2500 (I/O)
```

```
æœ¬ä¾‹æ¼”ç¤ºäº†å¦‚ä½•åº”ç”¨å…¬å¼è®¡ç®—ä¸¤æ¬¡æ’åºæ“ä½œçš„æˆæœ¬ï¼Œé‡ç‚¹æ˜¯æ•°æ®æŠ•å½±ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªåœ¨å†…å­˜ä¸­å­˜å‚¨äº† 1000 é¡µçš„è¡¨ã€‚ä¸ºäº†ä¼˜åŒ–æ’åºï¼Œæˆ‘ä»¬æŠ•å½±äº†å››åˆ†ä¹‹ä¸€çš„å±æ€§ï¼Œä»è€Œæœ‰æ•ˆåœ°ç¼©å°äº†è¡¨çš„å¤§å°ã€‚è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹ï¼šæ€»æˆæœ¬åŒ…æ‹¬è¯»å–æ•´ä¸ªè¡¨çš„æˆæœ¬ï¼ŒåŠ ä¸Šå†™å…¥æŠ•å½±é¡µçš„æˆæœ¬ï¼ŒåŠ ä¸Šæ’åºæˆæœ¬ï¼ŒåŠ ä¸Šè¯»å–æŠ•å½±é¡µä»¥æ¶ˆé™¤é‡å¤çš„æˆæœ¬ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥å…¬å¼å¯ç»†åˆ†ä¸ºè¯»è¡¨æˆæœ¬ï¼ˆ1000 é¡µï¼‰ã€å†™å…¥æŠ•å½±é¡µçš„æˆæœ¬ï¼ˆ0.25 * 1000 é¡µï¼‰ä»¥åŠä¸¤æ¬¡é€šè¿‡æ¬¡æ•°ä¹˜ä»¥è¯»å–æŠ•å½±é¡µçš„æˆæœ¬ã€‚è¿™ä¸€ç­–ç•¥åªæŠ•å½±äº†å››åˆ†ä¹‹ä¸€çš„å±æ€§ï¼Œä»è€Œå¤§å¤§å‡å°‘äº†æ•°æ®é›†ï¼Œç®€åŒ–äº†æ’åºè¿‡ç¨‹ï¼Œä¾¿äºæœ€åä¸€æ­¥çš„é‡å¤æ¶ˆé™¤ã€‚
This example demonstrates the application of a formula to calculate the cost of a two-pass sort operation with a focus on data projection. Assume we have a table stored across 1000 pages in memory. To optimize the sort, we project a quarter of the attributes, effectively reducing the table's size. The calculation proceeds as follows: the total cost comprises the cost of reading the entire table, plus the cost of writing the projected pages, plus the sorting cost, plus the cost of reading the projected pages to eliminate duplicates. Specifically, the formula breaks down into the cost of reading the table (1000 pages), adding the cost of writing the projected pages (0.25 * 1000 pages), and twice the number of passes multiplied by the cost of reading the projected pages. This strategy significantly reduces the dataset by projecting only a quarter of the attributes, thereby streamlining the sorting process and facilitating the final step of duplicate elimination.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240404213849558.png" alt="image-20240404213849558" style="zoom:33%;" /> 

**1ï¸âƒ£**Hashing-based projection

1. Scan R, extract only the neededattributes
2. Hash data into buckets
   - Apply hash function h1to choose one of B output buffers
3. Remove adjacentduplicates from a bucket
   - 2 tuples from different partitions guaranteed to be distinct

```
é™¤äº†æ’åºï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨æ•£åˆ—æ³•è¿›è¡ŒæŠ•å½±ï¼Œå…¶åŸç†ä¸æ­¤ç±»ä¼¼ã€‚è¿™ç§æ–¹æ³•ä¸æ˜¯è¿›è¡Œæ’åºï¼Œè€Œæ˜¯å°†æ•°æ®åˆ†éš”æˆå¤šä¸ªæ•°æ®æ¡¶ï¼Œç„¶åå¯¹è¿™äº›æ•°æ®æ¡¶è¿›è¡Œæ‰«æï¼Œæ‰¾å‡ºå¹¶åˆ é™¤é‡å¤æ•°æ®ã€‚è¿™ä¸€è¿‡ç¨‹åœ¨æˆåŠŸè¯†åˆ«å’Œæ¶ˆé™¤é‡å¤æ•°æ®åç»“æŸã€‚
In addition to sorting, we can apply projection using hashing, which operates on a similar principle. Rather than sorting, the method involves segregating the data into buckets and then scanning these buckets to locate and remove duplicates. This process concludes upon the successful identification and elimination of duplicates.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240404214239521.png" alt="image-20240404214239521" style="zoom:33%;" /> 

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240404214253121.png" alt="image-20240404214253121" style="zoom:67%;" /> 

```
è¿™ç§åŸºäºæ•£åˆ—çš„æŠ•å½±æ³•éœ€è¦ä¸€æ¬¡ä¸€é¡µåœ°å¤„ç†ä¸€ä¸ªå¤§è¡¨ã€‚æ¯ä¸€é¡µçš„æ•°æ®éƒ½é€šè¿‡æ•£åˆ—å‡½æ•°å®šå‘åˆ°ç›¸åº”çš„æ•°æ®æ¡¶ã€‚éšåï¼Œå¯¹è¿™äº›æ•°æ®æ¡¶è¿›è¡Œä¼ é€’ï¼Œæ¶ˆé™¤é‡å¤æ•°æ®ã€‚ä¸è¿‡ï¼Œè¿™ä¸€è¿‡ç¨‹ä¹Ÿå¹¶éæ²¡æœ‰æŒ‘æˆ˜ï¼Œè¿™ä¸æ’åºè¿‡ç¨‹ä¸­é‡åˆ°çš„æŒ‘æˆ˜å¦‚å‡ºä¸€è¾™ã€‚å…·ä½“æ¥è¯´ï¼Œå½“è¾“å…¥å¤§å°è¶…è¿‡å†…å­˜å®¹é‡æ—¶ï¼Œæ•°æ®æ¡¶ä¹Ÿæ— æ³•è£…å…¥å¯ç”¨å†…å­˜ï¼Œè¿™å°±é€ æˆäº†å¾ˆå¤§çš„éšœç¢ã€‚
This projection, based on hashing, involves processing a large table one page at a time. Each page's data is directed to its corresponding bucket through a hashing function. Subsequently, a pass over these buckets eliminates duplicates. However, the process is not without its challenges, mirroring those encountered in sorting. Specifically, when the input size exceeds memory capacity, the buckets also cannot fit into the available memory, presenting a significant obstacle.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240404214458947.png" alt="image-20240404214458947" style="zoom:33%;" /> 

**1ï¸âƒ£**Partitiondata into B partitions with h1 hash function

**2ï¸âƒ£**Load each partition, hash it with another hash function (h2) and eliminate duplicates

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240404214527586.png" alt="image-20240404214527586" style="zoom:50%;" /> 

```
å½“å­˜å‚¨æ¡¶è¶…è¿‡å†…å­˜å®¹é‡æ—¶ï¼Œå¤–éƒ¨æ•£åˆ—åŠŸèƒ½å°±ä¼šå‘æŒ¥ä½œç”¨ï¼Œå…è®¸å¤šæ¬¡æ•£åˆ—ã€‚èµ·åˆï¼Œè¯¥è¿‡ç¨‹åæ˜ äº†æ ‡å‡†æ–¹æ³•ï¼šè¯»å–æ•°æ®ï¼Œç„¶ååº”ç”¨æ•£åˆ—å‡½æ•°å°†æ•°æ®åˆ†é…åˆ°ç›¸åº”çš„åˆ†åŒºä¸­ã€‚è€ƒè™‘åˆ°å­˜å‚¨æ¡¶æ— æ³•å®Œå…¨é©»ç•™åœ¨å†…å­˜ä¸­çš„é™åˆ¶ï¼Œå°†æ•°æ®å¸è½½åˆ°ç£ç›˜æˆä¸ºä¸€ç§å¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œä»è€Œä¿ƒè¿›åˆ†åŒºçš„åˆ›å»ºã€‚è¿™é‡Œçš„å‡è®¾æ˜¯ï¼Œå†…å­˜åªèƒ½å®¹çº³æ¯ä¸ªæ•°æ®æ¡¶çš„ä¸€ä¸ªé¡µé¢ï¼Œè¿™å°±è¶³å¤Ÿäº†ã€‚åˆ†åŒºå®Œæˆåï¼Œä¸‹ä¸€æ­¥æ˜¯å•ç‹¬é‡æ–°åŠ è½½æ¯ä¸ªåˆ†åŒºï¼Œå¹¶åº”ç”¨æ–°çš„å“ˆå¸Œå‡½æ•°å°†æ•°æ®åˆ†å‘åˆ°æ•°æ®æ¡¶ä¸­ï¼Œç„¶åè¿›è¡Œé‡å¤æ£€æŸ¥ã€‚
When buckets exceed memory capacity, external hashing comes into play, allowing for hashing across multiple passes. Initially, the process mirrors the standard approach: data is read, and a hash function is applied to allocate data into corresponding partitions. Given the constraint that a bucket cannot fully reside in memory, offloading to disk becomes a viable solution, facilitating the creation of partitions. The assumption here is that memory can only accommodate one page per bucket, which suffices. After partitioning, the next step involves reloading each partition individually and applying a new hash function to distribute data into buckets, where duplicate checks are then performed.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240404214654716.png" alt="image-20240404214654716" style="zoom:50%;" /> 

**1ï¸âƒ£**Partitioning phase:

1. Read R using one input buffer
2. For each tuple:
   - DisğŸš—d unwanted fields
   - Apply hash function h1to choose one of B-1 output buffers
3. Result is B-1 partitions (of tuples with no unwanted fields)
   - 2 tuples from different partitions guaranteed to be distinct

**2ï¸âƒ£**Duplicate elimination phase:

1. For each partition
   - Read it and build an in-memory hash table
     - using hash function h2(<> h1) on all fields
   - while disğŸš—ding duplicates
2. If partition does not fit in memory
   - Apply hash-based projection algorithm recursively to this partition (we will not do thisâ€¦)

```
æ•°æ®å¤„ç†è¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªä¸»è¦æ­¥éª¤ï¼šé¦–å…ˆæ˜¯åˆ†åŒºï¼Œå°†æ•°æ®åˆ†æ•£åˆ°å„ä¸ªåˆ†åŒºå¹¶å­˜å‚¨åˆ°ç£ç›˜ä¸Šã€‚éšåæ˜¯æ¶ˆé™¤é‡å¤é˜¶æ®µï¼Œåœ¨è¿™ä¸€é˜¶æ®µä¸­ï¼Œæ¯æ¬¡å¤„ç†ä¸€ä¸ªåˆ†åŒºã€‚é‡‡ç”¨æ–°çš„æ•£åˆ—å‡½æ•°åœ¨å†…å­˜ä¸­åˆ›å»ºæ–°çš„æ•°æ®æ¡¶ï¼Œä¾¿äºæ£€æŸ¥æ¯ä¸ªç›¸åº”æ•°æ®æ¡¶ä¸­çš„é‡å¤æ•°æ®ã€‚ä½¿ç”¨æ–°çš„å“ˆå¸Œå‡½æ•°è€Œä¸æ˜¯é‡å¤ä½¿ç”¨åŸæ¥çš„å“ˆå¸Œå‡½æ•°æ˜¯æœ‰æˆ˜ç•¥æ„ä¹‰çš„ã€‚ä½¿ç”¨ç›¸åŒçš„å“ˆå¸Œå‡½æ•°ä¼šå¯¼è‡´æ•´ä¸ªåˆ†åŒºè¢«é‡æ–°å®šä½åˆ°ä¸€ä¸ªå†…å­˜æ¡¶ä¸­ï¼Œè¿™æ˜¯ä¸€ç§ä½æ•ˆä¸”æ˜‚è´µçš„æ“ä½œã€‚æˆ‘ä»¬çš„ç›®çš„æ˜¯è¿›ä¸€æ­¥åˆ†å‰²æ•°æ®ï¼Œå› æ­¤åœ¨æ­¤é˜¶æ®µå¼•å…¥äº†æ–°çš„æ•£åˆ—å‡½æ•°ã€‚
The data handling process is divided into two primary steps: initially, partitioning, where data is distributed into partitions and stored on disk. Following this, the duplicate elimination phase occurs, during which one partition is processed at a time. A new hash function is applied to create new buckets in memory, facilitating the inspection of each corresponding bucket for duplicates. The rationale behind employing a new hash function rather than reusing the original one is strategic. Utilizing the same hash function would result in the entire partition being relocated to a single memory bucket, an inefficient and costly operation. The aim is to segment the data further, hence the introduction of a new hash function at this stage.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240404215130208.png" alt="image-20240404215130208" style="zoom:33%;" /> 

```
Cost =       
      ReadTable+
      WriteProjectedPages+
      ReadProjectedPages
      
Our example:
Cost = 
      ReadTable+
      WriteProjectedPages+
      ReadProjectedPages
     = 1000 + 0.25 * 1000 + 250 = 1500 (I/O)
      
```

```
å¯¹äºé‚£äº›å¯¹ç®—æ³•æ„Ÿå…´è¶£çš„äººæ¥è¯´ï¼Œæˆ‘å·²ç»æ¼”ç¤ºäº†å¤–éƒ¨æ•£åˆ—ï¼Œä½†äº†è§£å…¶å†…éƒ¨å·¥ä½œåŸç†å¯¹æˆ‘ä»¬çš„ç›®çš„æ¥è¯´å¹¶ä¸é‡è¦ï¼›è¯·æ”¾å¿ƒï¼Œæˆ‘ä»¬ä¸ä¼šå¯¹æ­¤è¿›è¡Œå®¡é—®ã€‚ä¸è¿‡ï¼Œæœ€é‡è¦çš„æ˜¯æŒæ¡ä¸è¿™ä¸€æ“ä½œç›¸å…³çš„æˆæœ¬--åŸºäºæ•£åˆ—çš„æ¨ç®—--è¿™é‡Œå°†æ¦‚è¿°è¿™ä¸€æˆæœ¬ã€‚
For those intrigued by the algorithmic aspect, I've demonstrated external hashing, but it's not essential to understand its inner workings for our purposes; rest assured, there will be no interrogation on this. What is crucial, however, is grasping the cost associated with this operationâ€”the projection based on hashingâ€”and that cost is outlined here.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

 <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240404215130208.png" alt="image-20240404215130208" style="zoom:33%;" /> 

```sql
Cost =       
      ReadTable+            -- Read the entire table and project attributes
      WriteProjectedPages+  -- Write projected pages into corresponding partitions
      ReadProjectedPages    -- Read partitions one by one, create another hash table and disğŸš—d duplicates within a bucket
      
Our example:
Cost = 
      ReadTable+
      WriteProjectedPages+
      ReadProjectedPages
     = 1000 + 0.25 * 1000 + 250 = 1500 (I/O)
      
```

```
æˆæœ¬åŒ…æ‹¬è¯»å–æ•´ä¸ªè¡¨ä»¥é¢„æµ‹å±æ€§ï¼Œç„¶åå°†è¿™äº›é¢„æµ‹é¡µå†™å…¥ç›¸åº”çš„åˆ†åŒºã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¯»å–æ‰€æœ‰æ•°æ®ä»¥åˆ›å»ºåˆ†åŒºã€‚ä¹‹åï¼Œæˆ‘ä»¬é€ä¸ªå¤„ç†åˆ†åŒºï¼Œç”Ÿæˆå¦ä¸€ä¸ªå“ˆå¸Œè¡¨ï¼Œå¹¶æ¶ˆé™¤æ¯ä¸ªæ¡¶ä¸­çš„é‡å¤æ•°æ®ã€‚å…·ä½“æ¥è¯´ï¼Œæˆæœ¬ç»†åˆ†å¦‚ä¸‹ï¼šè¯»å–è¡¨çš„æˆæœ¬ä¸º 1000ã€‚å†™å…¥æŠ•å½±é¡µï¼ˆå  1000 çš„ 25%ï¼‰äº§ç”Ÿ 250 ä¸ªæŠ•å½±é¡µã€‚è¿™ä¸ªè¿‡ç¨‹ä¼šå»ºç«‹åˆ†åŒºï¼Œç„¶åè¯»å–è¿™ 250 ä¸ªé¢„è®¡é¡µé¢æ„æˆåç»­æˆæœ¬ï¼Œæœ€ç»ˆæ€»æˆæœ¬ä¸º 1500ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå°½ç®¡æ‰§è¡Œäº†å¤§é‡çš„é€»è¾‘æ“ä½œï¼ˆåˆ†åŒºå’Œæ•°æ®æ“ä½œï¼‰ï¼Œä½†åªæœ‰è¯»å–å’Œå†™å…¥çš„ç›´æ¥æ“ä½œå¯¹è®¡ç®—æˆæœ¬æœ‰è´¡çŒ®ã€‚
The cost involves reading the entire table to project attributes and then writing these projected pages into corresponding partitions. Initially, we read all the data to create partitions. Following this, we process partitions one by one, generate another hash table, and eliminate duplicates within each bucket. Specifically, the cost breakdown is as follows: reading the table incurs a cost of 1000. Writing projected pages, which amounts to 25% of 1000, results in 250 projected pages. This process establishes partitions, and then reading these 250 projected pages constitutes the subsequent cost, culminating in a total cost of 1500. It's important to note, despite the extensive logical operations performedâ€”partitioning and data manipulationâ€”only the direct actions of reading and writing contribute to the calculated cost.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240404214458947.png" alt="image-20240404214458947" style="zoom:33%;" /> 

**1ï¸âƒ£**Partitiondata into B partitions with h1 hash function

**2ï¸âƒ£**Load each partition, hash it with another hash function (h2) and eliminate duplicates

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240404214527586.png" alt="image-20240404214527586" style="zoom:50%;" /> 

```
å¥½çš„,ä»å­—é¢ä¸Šçœ‹ï¼Œæˆ‘ä»¬è¦æŠŠå¤šå°‘é¡µå†…å®¹ä»è¿™é‡Œå¸¦å…¥æˆ‘çš„è®°å¿†ä¸­ï¼Œå†ä»è®°å¿†ä¸­æ”¾å›è¿™é‡Œã€‚
All right. What contributes is literally how many pages we will bring from this into me memory and put it back from memory into this. 
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240404215130208.png" alt="image-20240404215130208" style="zoom:33%;" /> 

```
Cost =       
      ReadTable+
      WriteProjectedPages+
      ReadProjectedPages
      
Our example:
Cost = 
      ReadTable+
      WriteProjectedPages+
      ReadProjectedPages
     = 1000 + 0.25 * 1000 + 250 = 1500 (I/O)
      
```

```
è¿™ä¸€ç‚¹åœ¨è¿™äº›ç›¸å½“ç®€å•çš„å…¬å¼ä¸­å¾—åˆ°äº†çœŸæ­£çš„ä½“ç°ï¼Œåœ¨æŸ¥è¯¢å¤„ç†æ¨¡å—çš„å…¶ä½™éƒ¨åˆ†ä¸­ï¼Œè¿™äº›å…¬å¼ä¹Ÿå°†ä¿æŒç®€å•ã€‚å¥½çš„ã€‚
And this is really reflected in those fairly simple formulas and the formulas are going to remain as simple for the rest of this query processing module. All right.
```

$$
\textcolor{red}{\Huge{-------}\colorbox{yellow}{åˆ†å‰²çº¿}\Huge{-------}}
$$

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240404220037290.png" alt="image-20240404220037290" style="zoom: 33%;" /> 

**1ï¸âƒ£**Understand the logic behind relational operators

**2ï¸âƒ£**Learn alternatives for selections and projections (for now)

- Be able to calculate the cost of alternatives

**3ï¸âƒ£**Important for Assignment 3 as well

```
ä»Šå¤©è®²åº§çš„ä¸»æ—¨æ˜¯è®©å¤§å®¶æŒæ¡è¯„ä¼°å„ç§è®¡åˆ’å’Œè¿è¥å•†æˆæœ¬çš„å¿…è¦æŠ€èƒ½ã€‚è™½ç„¶æˆ‘ä»¬æ¢è®¨äº†ä¸€ç³»åˆ—ä¸»é¢˜ï¼Œä½†æˆ‘ä»¬çš„ä¸»è¦é‡ç‚¹æ˜¯äº†è§£å¦‚ä½•è¯„ä¼°é¢„æµ‹å’Œé€‰æ‹©è¿‡ç¨‹çš„æ•ˆç‡ã€‚åœ¨æ¥ä¸‹æ¥çš„è¯¾ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨åŠ ç›Ÿçš„å¤æ‚æ€§ï¼Œéšåå­¦ä¹ è®¡ç®—æ•´ä¸ªè®¡åˆ’çš„æˆæœ¬ï¼Œä»¥ç¡®å®šæœ€æœ‰æ•ˆçš„ç­–ç•¥ã€‚è¿™ä¸€ç»ƒä¹ æ¨¡æ‹Ÿäº†ä¼˜åŒ–äººå‘˜çš„è§’è‰²ï¼Œåæ˜ äº†æ•°æ®åº“ç®¡ç†å‘˜åœ¨ç°å®ä¸–ç•Œä¸­è´Ÿè´£æé«˜æ€§èƒ½å’Œç®€åŒ–åˆ†æçš„èŒè´£ã€‚è¿™ä¹Ÿå°†æ˜¯ä½ ä»¬ç¬¬ä¸‰æ¬¡ä½œä¸šçš„ä¸»é¢˜ã€‚æ„Ÿè°¢æ‚¨çš„å…³æ³¨ï¼Œæˆ‘æœŸå¾…ç€æˆ‘ä»¬çš„ä¸‹ä¸€å ‚è¯¾ã€‚
The essence of today's lecture was to equip you with the skills necessary for evaluating the cost of various plans and operators. While we've explored a range of topics, our primary focus has been on understanding how to assess the efficiency of projection and selection processes. In our upcoming sessions, we will delve into the complexities of joins and subsequently learn to calculate the costs of entire plans to identify the most effective strategies. This exercise simulates the role of an optimizer, mirroring the real-world responsibilities of a database administrator tasked with enhancing performance and streamlining analysis. This will also be the theme of your third assignment. Thank you for your attention, and I look forward to our next session.
```





